{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAMIDSpiyalong/Introduction-to-Machine-Learning-for-Energy/blob/main/Lecture_4b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers From Scratch"
      ],
      "metadata": {
        "id": "MenE2varZEXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab will build a transformer from scratch for translation task, which is the orginal function for the encoder decoder structure. We use tensorflow dataset."
      ],
      "metadata": {
        "id": "aTSadDXiPQLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objectives"
      ],
      "metadata": {
        "id": "pAifCvd_Tth9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Understand and build a dot product self attention block.\n",
        "2. Using the attention block to build a transformer with multiple heads.\n",
        "3. Using the prepared dataset, train a transformer to auto-translate."
      ],
      "metadata": {
        "id": "gg_2bsitTwAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "vWyjB-YNwTG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll build a transformer from scratch, layer-by-layer. We'll start with the **Multi-Head Self-Attention** layer since that's the most involved bit. Once we have that working, the rest of the model will look familiar if you've been following the course so far."
      ],
      "metadata": {
        "id": "mDkTVv3KMJX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "LqX04fFXBdxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaled Dot Product Self-Attention"
      ],
      "metadata": {
        "id": "-XnKHnlYyijq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Inside each attention head is a **Scaled Dot Product Self-Attention** operation as we covered in the slides. Given *queries*, *keys*, and *values*, the operation returns a new \"mix\" of the values.\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T)}{\\sqrt{d_k}})V$$\n",
        "\n",
        "The following function implements this and also takes a mask to account for padding and for masking future tokens for decoding (i.e. **look-ahead mask**). We'll cover masking later in the notebook."
      ],
      "metadata": {
        "id": "3NAf9HP7RsQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores)\n",
        "  return tf.matmul(weights, value), weights"
      ],
      "metadata": {
        "id": "7hpO6cGEN7HK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose our *queries*, *keys*, and *values* are each a length of 3 with a dimension of 4."
      ],
      "metadata": {
        "id": "lC_HhsreXh3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 3\n",
        "embed_dim = 4\n",
        "\n",
        "queries = np.random.rand(seq_len, embed_dim)\n",
        "keys = np.random.rand(seq_len, embed_dim)\n",
        "values = np.random.rand(seq_len, embed_dim)\n",
        "\n",
        "print(\"Queries:\\n\", queries)"
      ],
      "metadata": {
        "id": "WB2cDybgX5LZ",
        "outputId": "3355e529-e17c-45d2-fd36-1aa873a96382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries:\n",
            " [[0.90769595 0.85981611 0.07292586 0.15409233]\n",
            " [0.85617015 0.36758837 0.63618922 0.5813505 ]\n",
            " [0.91135708 0.81539009 0.19281194 0.57139135]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This would be the self-attention output and weights."
      ],
      "metadata": {
        "id": "QuNdMuz5vb1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output, attn_weights = scaled_dot_product_attention(queries, keys, values)\n",
        "\n",
        "print(\"Output\\n\", output, \"\\n\")\n",
        "print(\"Weights\\n\", attn_weights)"
      ],
      "metadata": {
        "id": "pxKj56hNX5UO",
        "outputId": "11b5b823-d48e-4a6f-e704-f2c1ae006395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output\n",
            " tf.Tensor(\n",
            "[[0.42496407 0.5024725  0.57338387 0.62166166]\n",
            " [0.41672707 0.49846074 0.5729456  0.62078524]\n",
            " [0.42951483 0.50796884 0.57826084 0.6218555 ]], shape=(3, 4), dtype=float32) \n",
            "\n",
            "Weights\n",
            " tf.Tensor(\n",
            "[[0.27782482 0.3765212  0.34565398]\n",
            " [0.2895227  0.3526816  0.3577957 ]\n",
            " [0.2678568  0.38508782 0.34705544]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating queries, keys, and values for multiple heads."
      ],
      "metadata": {
        "id": "O8NLm6qaN7DE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a way to calculate self-attention, let's actually generate the input *queries*, *keys*, and *values* for multiple heads."
      ],
      "metadata": {
        "id": "wBm9jbpSN6-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YLiJy9OzfMu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easier to understand things this way and we can certainly code it this way as well. But we can also \"simulate\" different heads with a single query matrix, single key matrix, and single value matrix.\n",
        "<br><br>\n",
        "We'll do both. First we'll create *query*, *key*, and *value* vectors using separate weights per head.\n",
        "<br><br>\n",
        "In the slides, we used an example of 12 dimensional embeddings processed by  three attentions heads, and we'll do the same here."
      ],
      "metadata": {
        "id": "3tKPwmi3fbys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 3\n",
        "embed_dim = 12\n",
        "num_heads = 3\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "print(f\"Dimension of each head: {head_dim}\")"
      ],
      "metadata": {
        "id": "rJLyGtqbX3uW",
        "outputId": "d4790448-b830-484a-cab7-e16f97a61591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of each head: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using separate weight matrices per head**"
      ],
      "metadata": {
        "id": "JDl37YzAf7bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose these are our input embeddings. Here we have a batch of 1 containing a sequence of length 3, with each element being a 12-dimensional embedding."
      ],
      "metadata": {
        "id": "xQ_KoJq3fv-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
        "print(\"Input shape: \", x.shape, \"\\n\")\n",
        "print(\"Input:\\n\", x)"
      ],
      "metadata": {
        "id": "7NcX3KBrX3uW",
        "outputId": "986de86b-3aeb-49d6-dec5-eb2318953e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  (1, 3, 12) \n",
            "\n",
            "Input:\n",
            " [[[0.9 0.1 0.1 0.3 0.5 0.6 0.4 0.1 0.2 0.1 0.5 0.8]\n",
            "  [0.9 0.1 0.  0.7 0.4 0.7 0.4 0.4 0.2 0.7 0.  0.8]\n",
            "  [1.  0.6 0.2 0.6 0.4 0.2 0.3 0.3 0.9 0.8 0.2 0.2]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll declare three sets of *query* weights (one for each head), three sets of *key* weights, and three sets of *value* weights. Remember each weight matrix should have a dimension of $\\text{d}\\ \\text{x}\\ \\text{d/h}$."
      ],
      "metadata": {
        "id": "uvJicbp6f7pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The query weights for each head.\n",
        "wq0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The key weights for each head.\n",
        "wk0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The value weights for each head.\n",
        "wv0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv2 = np.random.rand(embed_dim, head_dim).round(1)"
      ],
      "metadata": {
        "id": "8zdg7rqrX3uX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The three sets of query weights (one for each head):\")\n",
        "print(\"wq0:\\n\", wq0)\n",
        "print(\"wq1:\\n\", wq1)\n",
        "print(\"wq2:\\n\", wq1)"
      ],
      "metadata": {
        "id": "QzMRHZooX3uX",
        "outputId": "62d7c512-5aab-4324-e305-dba263534b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three sets of query weights (one for each head):\n",
            "wq0:\n",
            " [[0.2 0.2 0.  0.4]\n",
            " [0.1 0.8 1.  0.4]\n",
            " [0.  0.  0.1 0.2]\n",
            " [0.8 0.4 0.1 0.2]\n",
            " [0.3 0.6 0.9 0.2]\n",
            " [0.5 1.  0.3 1. ]\n",
            " [0.6 0.7 0.3 0.4]\n",
            " [0.4 0.1 0.9 0.5]\n",
            " [0.2 0.9 0.1 0.3]\n",
            " [0.6 0.3 0.5 0.9]\n",
            " [0.9 0.7 0.5 0.9]\n",
            " [0.4 0.8 0.  0.3]]\n",
            "wq1:\n",
            " [[1.  0.1 0.2 0.6]\n",
            " [0.9 0.3 0.1 0.3]\n",
            " [0.5 0.3 0.6 0.1]\n",
            " [0.2 0.7 0.5 0.4]\n",
            " [0.3 0.  0.3 0.8]\n",
            " [0.3 0.9 0.4 0.9]\n",
            " [0.4 0.6 0.9 0.2]\n",
            " [0.9 0.5 0.7 0.3]\n",
            " [0.  0.8 0.9 0.1]\n",
            " [0.9 0.1 0.9 0.3]\n",
            " [0.2 0.3 0.2 0.9]\n",
            " [0.5 0.5 0.1 0.8]]\n",
            "wq2:\n",
            " [[1.  0.1 0.2 0.6]\n",
            " [0.9 0.3 0.1 0.3]\n",
            " [0.5 0.3 0.6 0.1]\n",
            " [0.2 0.7 0.5 0.4]\n",
            " [0.3 0.  0.3 0.8]\n",
            " [0.3 0.9 0.4 0.9]\n",
            " [0.4 0.6 0.9 0.2]\n",
            " [0.9 0.5 0.7 0.3]\n",
            " [0.  0.8 0.9 0.1]\n",
            " [0.9 0.1 0.9 0.3]\n",
            " [0.2 0.3 0.2 0.9]\n",
            " [0.5 0.5 0.1 0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll generate our *queries*, *keys*, and *values* for each head by multiplying our input by the weights."
      ],
      "metadata": {
        "id": "HmwGKV9qgch-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Geneated queries, keys, and values for the first head.\n",
        "q0 = np.dot(x, wq0)\n",
        "k0 = np.dot(x, wk0)\n",
        "v0 = np.dot(x, wv0)\n",
        "\n",
        "# Geneated queries, keys, and values for the second head.\n",
        "q1 = np.dot(x, wq1)\n",
        "k1 = np.dot(x, wk1)\n",
        "v1 = np.dot(x, wv1)\n",
        "\n",
        "# Geneated queries, keys, and values for the third head.\n",
        "q2 = np.dot(x, wq2)\n",
        "k2 = np.dot(x, wk2)\n",
        "v2 = np.dot(x, wv2)"
      ],
      "metadata": {
        "id": "NucbYNNSX3uX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the resulting *query*, *key*, and *value* vectors for the first head."
      ],
      "metadata": {
        "id": "AIDiwWZ0gqhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q, K, and V for first head:\\n\")\n",
        "\n",
        "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
        "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
        "print(f\"v0 {v0.shape}:\\n\", v0)"
      ],
      "metadata": {
        "id": "NMcMmbkqX3uX",
        "outputId": "a6472bda-149f-4457-af38-5e2de14365f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q, K, and V for first head:\n",
            "\n",
            "q0 (1, 3, 4):\n",
            " [[[2.1  3.39 2.03 2.34]\n",
            "  [2.04 2.52 2.43 2.5 ]\n",
            "  [2.24 2.78 2.71 2.44]]] \n",
            "\n",
            "k0 (1, 3, 4):\n",
            " [[[2.38 2.64 3.51 2.73]\n",
            "  [1.37 1.98 2.92 2.11]\n",
            "  [2.03 2.88 3.44 2.31]]] \n",
            "\n",
            "v0 (1, 3, 4):\n",
            " [[[3.43 1.81 1.73 2.01]\n",
            "  [3.73 1.65 2.08 2.15]\n",
            "  [3.75 1.61 2.03 1.65]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our Q, K, V vectors, we can just pass them to our self-attention operation. Here we're calculating the output and attention weights for the first head."
      ],
      "metadata": {
        "id": "iw5CQ9i6qZDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_dot_product_attention(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "id": "i7tHIvXKX3uX",
        "outputId": "95541be6-b598-408c-ad8a-a7a3442fbb91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first attention head:  tf.Tensor(\n",
            "[[[3.552432  1.7339897 1.846055  1.8811185]\n",
            "  [3.5430186 1.7399838 1.8375058 1.893627 ]\n",
            "  [3.5420089 1.74048   1.8362217 1.8924185]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Attention weights from first head:  tf.Tensor(\n",
            "[[[0.6162399  0.01854475 0.36521524]\n",
            "  [0.6454069  0.02256118 0.3320319 ]\n",
            "  [0.6488697  0.01765081 0.33347952]]], shape=(1, 3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the other two (attention weights are ignored)."
      ],
      "metadata": {
        "id": "DoYEXSm7qr_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out1, _ = scaled_dot_product_attention(q1, k1, v1)\n",
        "out2, _ = scaled_dot_product_attention(q2, k2, v2)\n",
        "\n",
        "print(\"Output from second attention head: \", out1, \"\\n\")\n",
        "print(\"Output from third attention head: \", out2,)"
      ],
      "metadata": {
        "id": "otnqbaDSqpJ7",
        "outputId": "74e0a49e-cbd8-4f14-fa22-db9afb9f1c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from second attention head:  tf.Tensor(\n",
            "[[[2.8143632 3.2294424 1.9993955 2.9046278]\n",
            "  [2.7964728 3.2433422 1.9857107 2.8767433]\n",
            "  [2.805351  3.2363636 1.9928755 2.891967 ]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Output from third attention head:  tf.Tensor(\n",
            "[[[3.342678  2.4526718 2.493952  2.5026987]\n",
            "  [3.3433924 2.4535856 2.495035  2.501644 ]\n",
            "  [3.338001  2.453367  2.498007  2.4990222]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we covered in the slides, once we have each head's output, we concatenate them and then put them through a linear layer for further processing."
      ],
      "metadata": {
        "id": "lOV717bqX3uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_a = np.concatenate((out0, out1, out2), axis=-1)\n",
        "print(f\"Combined output from all heads {combined_out_a.shape}:\")\n",
        "print(combined_out_a)\n",
        "\n",
        "# The final step would be to run combined_out_a through a linear/dense layer\n",
        "# for further processing."
      ],
      "metadata": {
        "id": "gmSv5trtt2v9",
        "outputId": "d96f37f2-096e-4597-8a3f-54064d7845cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined output from all heads (1, 3, 12):\n",
            "[[[3.552432  1.7339897 1.846055  1.8811185 2.8143632 3.2294424 1.9993955\n",
            "   2.9046278 3.342678  2.4526718 2.493952  2.5026987]\n",
            "  [3.5430186 1.7399838 1.8375058 1.893627  2.7964728 3.2433422 1.9857107\n",
            "   2.8767433 3.3433924 2.4535856 2.495035  2.501644 ]\n",
            "  [3.5420089 1.74048   1.8362217 1.8924185 2.805351  3.2363636 1.9928755\n",
            "   2.891967  3.338001  2.453367  2.498007  2.4990222]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that's a complete run of **multi-head self-attention** using separate sets of weights per head.<br>\n",
        "\n",
        "Let's now get the same thing done using a single query weight matrix, single key weight matrix, and single value weight matrix.<br><br>\n",
        "These were our separate per-head query weights:"
      ],
      "metadata": {
        "id": "RRZpFR0Wt8h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query weights for first head: \\n\", wq0, \"\\n\")\n",
        "print(\"Query weights for second head: \\n\", wq1, \"\\n\")\n",
        "print(\"Query weights for third head: \\n\", wq2)"
      ],
      "metadata": {
        "id": "XoJmLAsUX3uX",
        "outputId": "a4cbfc65-335d-46cc-a424-94dae5b003a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query weights for first head: \n",
            " [[0.2 0.2 0.  0.4]\n",
            " [0.1 0.8 1.  0.4]\n",
            " [0.  0.  0.1 0.2]\n",
            " [0.8 0.4 0.1 0.2]\n",
            " [0.3 0.6 0.9 0.2]\n",
            " [0.5 1.  0.3 1. ]\n",
            " [0.6 0.7 0.3 0.4]\n",
            " [0.4 0.1 0.9 0.5]\n",
            " [0.2 0.9 0.1 0.3]\n",
            " [0.6 0.3 0.5 0.9]\n",
            " [0.9 0.7 0.5 0.9]\n",
            " [0.4 0.8 0.  0.3]] \n",
            "\n",
            "Query weights for second head: \n",
            " [[1.  0.1 0.2 0.6]\n",
            " [0.9 0.3 0.1 0.3]\n",
            " [0.5 0.3 0.6 0.1]\n",
            " [0.2 0.7 0.5 0.4]\n",
            " [0.3 0.  0.3 0.8]\n",
            " [0.3 0.9 0.4 0.9]\n",
            " [0.4 0.6 0.9 0.2]\n",
            " [0.9 0.5 0.7 0.3]\n",
            " [0.  0.8 0.9 0.1]\n",
            " [0.9 0.1 0.9 0.3]\n",
            " [0.2 0.3 0.2 0.9]\n",
            " [0.5 0.5 0.1 0.8]] \n",
            "\n",
            "Query weights for third head: \n",
            " [[0.9 0.3 0.2 1. ]\n",
            " [0.8 0.2 0.3 0.1]\n",
            " [0.3 0.  0.  0.7]\n",
            " [0.4 0.4 0.6 0.3]\n",
            " [0.9 0.6 0.2 0. ]\n",
            " [0.3 0.1 0.6 0.6]\n",
            " [0.4 0.3 0.5 0.6]\n",
            " [0.  1.  0.6 0.5]\n",
            " [0.  0.5 0.2 0.1]\n",
            " [0.4 0.5 0.7 0.7]\n",
            " [0.7 0.4 0.8 0.5]\n",
            " [0.1 0.9 0.  0.2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose instead of declaring three separate query weight matrices, we had declared one. i.e. a single $d\\ x\\ d$ matrix. We're concatenating our per-head query weights here instead of declaring a new set of weights so that we get the same results."
      ],
      "metadata": {
        "id": "oa_p3bk8mO9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wq = np.concatenate((wq0, wq1, wq2), axis=1)\n",
        "print(f\"Single query weight matrix {wq.shape}: \\n\", wq)"
      ],
      "metadata": {
        "id": "7jh6zeg1X3uX",
        "outputId": "cb21a6bf-d87c-4865-d4fd-993af155a968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single query weight matrix (12, 12): \n",
            " [[0.2 0.2 0.  0.4 1.  0.1 0.2 0.6 0.9 0.3 0.2 1. ]\n",
            " [0.1 0.8 1.  0.4 0.9 0.3 0.1 0.3 0.8 0.2 0.3 0.1]\n",
            " [0.  0.  0.1 0.2 0.5 0.3 0.6 0.1 0.3 0.  0.  0.7]\n",
            " [0.8 0.4 0.1 0.2 0.2 0.7 0.5 0.4 0.4 0.4 0.6 0.3]\n",
            " [0.3 0.6 0.9 0.2 0.3 0.  0.3 0.8 0.9 0.6 0.2 0. ]\n",
            " [0.5 1.  0.3 1.  0.3 0.9 0.4 0.9 0.3 0.1 0.6 0.6]\n",
            " [0.6 0.7 0.3 0.4 0.4 0.6 0.9 0.2 0.4 0.3 0.5 0.6]\n",
            " [0.4 0.1 0.9 0.5 0.9 0.5 0.7 0.3 0.  1.  0.6 0.5]\n",
            " [0.2 0.9 0.1 0.3 0.  0.8 0.9 0.1 0.  0.5 0.2 0.1]\n",
            " [0.6 0.3 0.5 0.9 0.9 0.1 0.9 0.3 0.4 0.5 0.7 0.7]\n",
            " [0.9 0.7 0.5 0.9 0.2 0.3 0.2 0.9 0.7 0.4 0.8 0.5]\n",
            " [0.4 0.8 0.  0.3 0.5 0.5 0.1 0.8 0.1 0.9 0.  0.2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the same vein, pretend we declared a single key weight matrix, and single value weight matrix."
      ],
      "metadata": {
        "id": "-9MzE5Okmdbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wk = np.concatenate((wk0, wk1, wk2), axis=1)\n",
        "wv = np.concatenate((wv0, wv1, wv2), axis=1)\n",
        "\n",
        "print(f\"Single key weight matrix {wk.shape}:\\n\", wk, \"\\n\")\n",
        "print(f\"Single value weight matrix {wv.shape}:\\n\", wv)"
      ],
      "metadata": {
        "id": "xq2guuobX3uX",
        "outputId": "073e7c11-c5a6-477d-e876-f26a5ef5fb26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single key weight matrix (12, 12):\n",
            " [[0.1 0.6 0.5 0.7 0.3 0.5 0.  0.4 0.9 0.7 0.1 1. ]\n",
            " [0.4 0.4 0.8 0.  0.8 0.9 0.2 0.1 1.  0.1 0.8 0.8]\n",
            " [0.1 0.5 0.9 0.9 0.6 0.8 0.6 0.4 0.  0.6 0.3 0.7]\n",
            " [0.9 0.2 0.5 0.  0.4 0.8 0.5 0.8 0.1 0.3 0.3 0. ]\n",
            " [0.  0.7 0.7 0.7 0.2 0.8 0.3 0.6 0.4 0.1 0.4 0.6]\n",
            " [1.  0.1 0.7 0.6 0.2 0.2 0.1 0.5 0.3 0.2 0.  0.8]\n",
            " [0.1 0.  0.2 0.3 0.6 1.  0.6 0.4 0.7 0.7 0.9 0.1]\n",
            " [0.1 0.2 0.7 0.2 0.9 0.5 1.  0.8 1.  0.6 0.2 0.9]\n",
            " [0.9 0.9 0.9 0.6 0.4 0.5 0.7 0.1 1.  0.7 0.2 0.6]\n",
            " [0.1 0.8 0.7 0.7 0.  0.7 0.2 0.8 0.2 0.3 0.2 0.7]\n",
            " [0.6 0.9 0.3 0.8 0.4 1.  0.5 0.7 0.1 0.1 0.1 0.9]\n",
            " [0.1 0.4 0.5 0.9 0.9 0.3 0.2 0.8 0.6 0.7 0.  0.4]] \n",
            "\n",
            "Single value weight matrix (12, 12):\n",
            " [[1.  0.  0.3 0.3 1.  0.7 0.3 0.  0.9 0.4 0.7 0.6]\n",
            " [0.9 0.1 0.2 0.2 0.  0.2 0.5 0.4 0.5 0.3 0.4 0.2]\n",
            " [0.3 0.6 0.5 0.4 0.7 0.  0.2 0.4 0.6 0.4 0.7 0.7]\n",
            " [0.9 0.1 0.1 0.1 0.8 0.4 0.8 0.9 0.8 0.7 0.5 0.5]\n",
            " [0.9 0.2 0.1 0.1 0.4 0.9 0.3 0.4 0.7 0.2 0.6 0.7]\n",
            " [0.1 0.5 0.7 0.6 0.2 0.8 0.5 0.5 0.3 0.2 0.3 0.7]\n",
            " [0.7 0.6 0.1 0.7 0.3 1.  0.2 0.3 0.6 0.7 0.5 0.1]\n",
            " [0.7 0.6 0.7 0.5 0.5 1.  0.  1.  0.7 1.  0.8 0.9]\n",
            " [0.2 0.6 0.1 0.2 0.7 0.3 0.  0.5 0.1 0.3 0.  0.4]\n",
            " [0.8 0.6 0.9 0.4 0.9 0.7 0.6 1.  0.9 0.2 0.2 0.4]\n",
            " [0.2 0.1 0.8 0.7 0.5 0.4 0.8 0.6 0.8 0.5 0.1 0.7]\n",
            " [0.1 0.3 0.7 0.9 0.2 0.8 0.1 0.9 0.4 0.7 0.7 0.1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can calculate all our *queries*, *keys*, and *values* with three dot products."
      ],
      "metadata": {
        "id": "WA7dl1VRnXHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_s = np.dot(x, wq)\n",
        "k_s = np.dot(x, wk)\n",
        "v_s = np.dot(x, wv)"
      ],
      "metadata": {
        "id": "UQ5i98bLX3uX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are our resulting query vectors (we'll call them \"combined queries\"). How do we simulate different heads with this?"
      ],
      "metadata": {
        "id": "xkAzG-bgnx1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query vectors using a single weight matrix {q_s.shape}:\\n\", q_s)"
      ],
      "metadata": {
        "id": "H-qKM3jZr242",
        "outputId": "3dcec2e4-1ad6-411a-8c20-c781f72bf7ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vectors using a single weight matrix (1, 3, 12):\n",
            " [[[2.1  3.39 2.03 2.34 2.63 2.52 2.65 2.54 2.7  2.13 1.95 2.25]\n",
            "  [2.04 2.52 2.43 2.5  3.51 1.91 2.38 2.23 2.63 2.17 2.18 2.65]\n",
            "  [2.24 2.78 2.71 2.44 2.94 1.95 2.55 2.38 2.64 2.55 2.3  1.98]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Somehow, we need to separate these vectors such they're treated like three separate sets by the self-attention operation."
      ],
      "metadata": {
        "id": "qsUULAgRsB2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "id": "FKXYVHbJvnGp",
        "outputId": "3bf7b054-b57b-426d-a4ea-12b2ec8d4197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[2.1  3.39 2.03 2.34]\n",
            "  [2.04 2.52 2.43 2.5 ]\n",
            "  [2.24 2.78 2.71 2.44]]] \n",
            "\n",
            "[[[2.63 2.52 2.65 2.54]\n",
            "  [3.51 1.91 2.38 2.23]\n",
            "  [2.94 1.95 2.55 2.38]]] \n",
            "\n",
            "[[[2.7  2.13 1.95 2.25]\n",
            "  [2.63 2.17 2.18 2.65]\n",
            "  [2.64 2.55 2.3  1.98]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how each set of per-head queries looks like we took the combined queries, and chopped them vertically every four dimensions.\n",
        "<br><br>\n",
        "We can split our combined queries into $\\text{d}\\ \\text{x}\\ \\text{d/h}$ heads using **reshape** and **transpose**.<br><br>\n",
        "The first step is to *reshape* our combined queries from a shape of:<br>\n",
        "(batch_size, seq_len, embed_dim)<br>\n",
        "\n",
        "into a shape of<br>\n",
        " (batch_size, seq_len, num_heads, head_dim).\n",
        " <br>\n",
        "\n",
        " https://www.tensorflow.org/api_docs/python/tf/reshape"
      ],
      "metadata": {
        "id": "twXi0Sx-sTut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we can achieve the same thing by passing -1 instead of seq_len.\n",
        "q_s_reshaped = tf.reshape(q_s, (batch_size, seq_len, num_heads, head_dim))\n",
        "print(f\"Combined queries: {q_s.shape}\\n\", q_s, \"\\n\")\n",
        "print(f\"Reshaped into separate heads: {q_s_reshaped.shape}\\n\", q_s_reshaped)"
      ],
      "metadata": {
        "id": "d3iHh7XxX3uY",
        "outputId": "8cbcf697-fa8a-45ef-8218-f33b99597370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined queries: (1, 3, 12)\n",
            " [[[2.1  3.39 2.03 2.34 2.63 2.52 2.65 2.54 2.7  2.13 1.95 2.25]\n",
            "  [2.04 2.52 2.43 2.5  3.51 1.91 2.38 2.23 2.63 2.17 2.18 2.65]\n",
            "  [2.24 2.78 2.71 2.44 2.94 1.95 2.55 2.38 2.64 2.55 2.3  1.98]]] \n",
            "\n",
            "Reshaped into separate heads: (1, 3, 3, 4)\n",
            " tf.Tensor(\n",
            "[[[[2.1  3.39 2.03 2.34]\n",
            "   [2.63 2.52 2.65 2.54]\n",
            "   [2.7  2.13 1.95 2.25]]\n",
            "\n",
            "  [[2.04 2.52 2.43 2.5 ]\n",
            "   [3.51 1.91 2.38 2.23]\n",
            "   [2.63 2.17 2.18 2.65]]\n",
            "\n",
            "  [[2.24 2.78 2.71 2.44]\n",
            "   [2.94 1.95 2.55 2.38]\n",
            "   [2.64 2.55 2.3  1.98]]]], shape=(1, 3, 3, 4), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have our desired shape. The next step is to *transpose* it such that simulates vertically chopping our combined queries. By transposing, our matrix dimensions become:<br>\n",
        "(batch_size, num_heads, seq_len, head_dim)<br>\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/transpose"
      ],
      "metadata": {
        "id": "6fIWohaZvVs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_s_transposed = tf.transpose(q_s_reshaped, perm=[0, 2, 1, 3]).numpy()\n",
        "print(f\"Queries transposed into \\\"separate\\\" heads {q_s_transposed.shape}:\\n\",\n",
        "      q_s_transposed)"
      ],
      "metadata": {
        "id": "6Vv3kV3jX3uY",
        "outputId": "d5436581-f98b-4465-f342-8bfd420a1f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries transposed into \"separate\" heads (1, 3, 3, 4):\n",
            " [[[[2.1  3.39 2.03 2.34]\n",
            "   [2.04 2.52 2.43 2.5 ]\n",
            "   [2.24 2.78 2.71 2.44]]\n",
            "\n",
            "  [[2.63 2.52 2.65 2.54]\n",
            "   [3.51 1.91 2.38 2.23]\n",
            "   [2.94 1.95 2.55 2.38]]\n",
            "\n",
            "  [[2.7  2.13 1.95 2.25]\n",
            "   [2.63 2.17 2.18 2.65]\n",
            "   [2.64 2.55 2.3  1.98]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare this against the separate per-head queries we calculated previously, we see the same result except we now have all our queries in a single matrix."
      ],
      "metadata": {
        "id": "J2DOWEPewUns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The separate per-head query matrices from before: \")\n",
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "id": "ZMLEBmtowQ02",
        "outputId": "d94d7776-45e0-4701-b331-046d108ff346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The separate per-head query matrices from before: \n",
            "[[[2.1  3.39 2.03 2.34]\n",
            "  [2.04 2.52 2.43 2.5 ]\n",
            "  [2.24 2.78 2.71 2.44]]] \n",
            "\n",
            "[[[2.63 2.52 2.65 2.54]\n",
            "  [3.51 1.91 2.38 2.23]\n",
            "  [2.94 1.95 2.55 2.38]]] \n",
            "\n",
            "[[[2.7  2.13 1.95 2.25]\n",
            "  [2.63 2.17 2.18 2.65]\n",
            "  [2.64 2.55 2.3  1.98]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the exact same thing with our combined keys and values."
      ],
      "metadata": {
        "id": "kmVPAaE3wmGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_s_transposed = tf.transpose(tf.reshape(k_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()\n",
        "v_s_transposed = tf.transpose(tf.reshape(v_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()\n",
        "\n",
        "print(f\"Keys for all heads in a single matrix {k_s.shape}: \\n\", k_s_transposed, \"\\n\")\n",
        "print(f\"Values for all heads in a single matrix {v_s.shape}: \\n\", v_s_transposed)"
      ],
      "metadata": {
        "id": "vauGkBv3X3uY",
        "outputId": "46cd26cd-54b7-4b25-b8ff-1037357b334f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys for all heads in a single matrix (1, 3, 12): \n",
            " [[[[2.38 2.64 3.51 2.73]\n",
            "   [1.37 1.98 2.92 2.11]\n",
            "   [2.03 2.88 3.44 2.31]]\n",
            "\n",
            "  [[2.53 3.76 2.07 2.5 ]\n",
            "   [2.64 3.56 1.9  2.58]\n",
            "   [2.32 3.73 2.17 2.81]]\n",
            "\n",
            "  [[3.34 2.41 1.92 3.3 ]\n",
            "   [3.41 2.24 1.99 3.37]\n",
            "   [3.14 1.9  1.69 3.44]]]] \n",
            "\n",
            "Values for all heads in a single matrix (1, 3, 12): \n",
            " [[[[3.43 1.81 1.73 2.01]\n",
            "   [3.73 1.65 2.08 2.15]\n",
            "   [3.75 1.61 2.03 1.65]]\n",
            "\n",
            "  [[2.86 3.21 1.96 2.7 ]\n",
            "   [2.53 3.44 1.83 2.64]\n",
            "   [2.96 3.11 2.14 3.24]]\n",
            "\n",
            "  [[3.18 2.36 2.44 2.56]\n",
            "   [3.45 2.58 2.64 2.36]\n",
            "   [3.46 2.35 2.25 2.73]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up this way, we can now calculate the outputs from all attention heads with a single call to our self-attention operation."
      ],
      "metadata": {
        "id": "ebGFAKGrxCoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_heads_output, all_attn_weights = scaled_dot_product_attention(q_s_transposed,\n",
        "                                                                  k_s_transposed,\n",
        "                                                                  v_s_transposed)\n",
        "print(\"Self attention output:\\n\", all_heads_output)"
      ],
      "metadata": {
        "id": "hIElo1ObX3uY",
        "outputId": "aac74a2b-0797-443f-b94c-db1b691a3168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self attention output:\n",
            " tf.Tensor(\n",
            "[[[[3.552432  1.7339897 1.846055  1.8811185]\n",
            "   [3.5430188 1.7399838 1.8375058 1.893627 ]\n",
            "   [3.5420089 1.74048   1.8362217 1.8924185]]\n",
            "\n",
            "  [[2.8143635 3.2294426 1.9993956 2.904628 ]\n",
            "   [2.7964725 3.243342  1.9857105 2.8767433]\n",
            "   [2.8053508 3.2363634 1.9928751 2.8919668]]\n",
            "\n",
            "  [[3.342678  2.4526718 2.493952  2.5026991]\n",
            "   [3.3433924 2.4535856 2.4950347 2.5016437]\n",
            "   [3.338001  2.453367  2.4980073 2.4990222]]]], shape=(1, 3, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a sanity check, we can compare this against the outputs from individual heads we calculated earlier:"
      ],
      "metadata": {
        "id": "PCPtOI_awd-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Per head outputs from using separate sets of weights per head:\")\n",
        "print(out0, \"\\n\")\n",
        "print(out1, \"\\n\")\n",
        "print(out2)"
      ],
      "metadata": {
        "id": "bXIB_z11xsh7",
        "outputId": "e33bc5fb-be77-42d5-bb86-72659216dd11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per head outputs from using separate sets of weights per head:\n",
            "tf.Tensor(\n",
            "[[[3.552432  1.7339897 1.846055  1.8811185]\n",
            "  [3.5430186 1.7399838 1.8375058 1.893627 ]\n",
            "  [3.5420089 1.74048   1.8362217 1.8924185]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[2.8143632 3.2294424 1.9993955 2.9046278]\n",
            "  [2.7964728 3.2433422 1.9857107 2.8767433]\n",
            "  [2.805351  3.2363636 1.9928755 2.891967 ]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[3.342678  2.4526718 2.493952  2.5026987]\n",
            "  [3.3433924 2.4535856 2.495035  2.501644 ]\n",
            "  [3.338001  2.453367  2.498007  2.4990222]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the final concatenated result, we need to reverse our **reshape** and **transpose** operation, starting with the **transpose** this time."
      ],
      "metadata": {
        "id": "hPlpXbZI74mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_b = tf.reshape(tf.transpose(all_heads_output, perm=[0, 2, 1, 3]),\n",
        "                            shape=(batch_size, seq_len, embed_dim))\n",
        "print(\"Final output from using single query, key, value matrices:\\n\",\n",
        "      combined_out_b, \"\\n\")\n",
        "print(\"Final output from using separate query, key, value matrices per head:\\n\",\n",
        "      combined_out_a)"
      ],
      "metadata": {
        "id": "9lWtCPk1wuod",
        "outputId": "f965f6ca-c91c-4de4-8017-7228c7881e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output from using single query, key, value matrices:\n",
            " tf.Tensor(\n",
            "[[[3.552432  1.7339897 1.846055  1.8811185 2.8143635 3.2294426 1.9993956\n",
            "   2.904628  3.342678  2.4526718 2.493952  2.5026991]\n",
            "  [3.5430188 1.7399838 1.8375058 1.893627  2.7964725 3.243342  1.9857105\n",
            "   2.8767433 3.3433924 2.4535856 2.4950347 2.5016437]\n",
            "  [3.5420089 1.74048   1.8362217 1.8924185 2.8053508 3.2363634 1.9928751\n",
            "   2.8919668 3.338001  2.453367  2.4980073 2.4990222]]], shape=(1, 3, 12), dtype=float32) \n",
            "\n",
            "Final output from using separate query, key, value matrices per head:\n",
            " [[[3.552432  1.7339897 1.846055  1.8811185 2.8143632 3.2294424 1.9993955\n",
            "   2.9046278 3.342678  2.4526718 2.493952  2.5026987]\n",
            "  [3.5430186 1.7399838 1.8375058 1.893627  2.7964728 3.2433422 1.9857107\n",
            "   2.8767433 3.3433924 2.4535856 2.495035  2.501644 ]\n",
            "  [3.5420089 1.74048   1.8362217 1.8924185 2.805351  3.2363636 1.9928755\n",
            "   2.891967  3.338001  2.453367  2.498007  2.4990222]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can encapsulate everything we just covered in a class."
      ],
      "metadata": {
        "id": "Wi8WnhwL9UIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.d_head = self.d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    # Linear layer to generate the final output.\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
        "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def merge_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "    qs = self.wq(q)\n",
        "    ks = self.wk(k)\n",
        "    vs = self.wv(v)\n",
        "\n",
        "    qs = self.split_heads(qs)\n",
        "    ks = self.split_heads(ks)\n",
        "    vs = self.split_heads(vs)\n",
        "\n",
        "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
        "    output = self.merge_heads(output)\n",
        "\n",
        "    return self.dense(output), attn_weights\n"
      ],
      "metadata": {
        "id": "Sd_IgJI34vP4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa = MultiHeadSelfAttention(12, 3)\n",
        "\n",
        "output, attn_weights = mhsa(x, x, x, None)\n",
        "print(f\"MHSA output{output.shape}:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "nuvv-8cg6owq",
        "outputId": "5686b7ef-c8a8-4a48-fe78-b3e3bd6b4d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MHSA output(1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[-0.75220907  0.60848594 -0.24099663  0.8574363  -1.7627103\n",
            "    1.0225685   0.06094044  0.5666399  -0.13977948 -0.63689446\n",
            "   -0.660741    1.1435963 ]\n",
            "  [-0.7572313   0.6086106  -0.23586354  0.8668075  -1.7626736\n",
            "    1.0215359   0.06591715  0.56524444 -0.12876031 -0.6342495\n",
            "   -0.6591168   1.1385059 ]\n",
            "  [-0.7551215   0.6213727  -0.22492132  0.85145015 -1.7764089\n",
            "    1.0195215   0.04244602  0.54266554 -0.15470764 -0.6431295\n",
            "   -0.6555153   1.1523046 ]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Block"
      ],
      "metadata": {
        "id": "uAk-GG2yMM59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now build our **Encoder Block**. In addition to the **Multi-Head Self Attention** layer, the **Encoder Block** also has **skip connections**, **layer normalization steps**, and a **two-layer feed-forward neural network**. The original **Attention Is All You Need** paper also included some **dropout** applied to the self-attention output which isn't shown in the illustration below (see references for a link to the paper).\n"
      ],
      "metadata": {
        "id": "BHrQaN_B_rLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since a two-layer feed forward neural network is used in multiple places in the transformer, here's a function which creates and returns one."
      ],
      "metadata": {
        "id": "S7Yc_FnvDNx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(d_model, hidden_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "mN5B0vduMM9a"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our encoder block containing all the layers and steps from the preceding illustration (plus dropout)."
      ],
      "metadata": {
        "id": "4FrRAMJFDnVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    mhsa_output, attn_weights = self.mhsa(x, x, x, mask)\n",
        "    mhsa_output = self.dropout1(mhsa_output, training=training)\n",
        "    mhsa_output = self.layernorm1(x + mhsa_output)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    output = self.layernorm2(mhsa_output + ffn_output)\n",
        "\n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "q8uu0mISAb0n"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have an embedding dimension of 12, and we want 3 attention heads and a feed forward network with a hidden dimension of 48 (4x the embedding dimension). We would declare and use a single encoder block like so:"
      ],
      "metadata": {
        "id": "q3_2uXRBFBEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_block = EncoderBlock(12, 3, 48)\n",
        "\n",
        "block_output,  _ = encoder_block(x, True, None)\n",
        "print(f\"Output from single encoder block {block_output.shape}:\")\n",
        "print(block_output)"
      ],
      "metadata": {
        "id": "vBnumPJ7C7Jj",
        "outputId": "a2bc3fba-cc71-4245-a42c-6c329c913a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from single encoder block (1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[ 1.6569743   1.3320812  -1.1963488  -0.15327895 -0.8070753\n",
            "   -0.3270114   0.02493571  0.4750662   1.5313147  -0.67409134\n",
            "   -0.43159893 -1.4309677 ]\n",
            "  [ 1.7250888   1.208477   -1.2379202  -0.7765264  -0.1966951\n",
            "   -0.4522387   0.57110393  1.5487726  -0.4439479  -0.40991902\n",
            "   -0.09402011 -1.4421747 ]\n",
            "  [ 0.80964327  1.3949454  -1.4053912  -0.16600242 -0.22739841\n",
            "   -0.8344146  -0.8908817   1.2394375   1.3162789   0.34700018\n",
            "   -0.01541197 -1.5678049 ]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word and Positional Embeddings"
      ],
      "metadata": {
        "id": "I5z32v2QKYdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now deal with the actual input to the **initial** encoder block. The inputs are going to be *positional word embeddings*. That is, word embeddings with some positional information added to them.\n",
        "<br>\n",
        "\n",
        "Let's start with **subword** tokenization. For demonstration, we'll use a subword tokenizer called **BPEmb**. It uses **Byte-Pair Encoding** and supports over two hundred languages.\n",
        "\n",
        "https://bpemb.h-its.org/\n"
      ],
      "metadata": {
        "id": "S4NuyQpYGBUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English tokenizer.\n",
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "metadata": {
        "id": "nmMOHYDEKdvQ",
        "outputId": "9c67338e-31f4-4bcd-af27-eaaf01e18114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 899022.12B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3784656/3784656 [00:00<00:00, 4157039.92B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The library comes with embeddings for a number of words."
      ],
      "metadata": {
        "id": "uAjjB6ykHHyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_vocab_size, bpemb_embed_size = bpemb_en.vectors.shape\n",
        "print(\"Vocabulary size:\", bpemb_vocab_size)\n",
        "print(\"Embedding size:\", bpemb_embed_size)"
      ],
      "metadata": {
        "id": "FhtnbTmdH6jU",
        "outputId": "c59ab7f6-4e1f-4311-f17d-a9a961a688c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Embedding size: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding for the word \"car\".\n",
        "bpemb_en.vectors[bpemb_en.words.index('car')]"
      ],
      "metadata": {
        "id": "vKvODSJDIdt0",
        "outputId": "cc8e5622-9326-4d38-ad5f-674fa3890fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.305548, -0.325598, -0.134716, -0.078735, -0.660545,  0.076211,\n",
              "       -0.735487,  0.124533, -0.294402,  0.459688,  0.030137,  0.174041,\n",
              "       -0.224223,  0.486189, -0.504649, -0.459699,  0.315747,  0.477885,\n",
              "        0.091398,  0.427867,  0.016524, -0.076833, -0.899727,  0.493158,\n",
              "       -0.022309, -0.422785, -0.154148,  0.204981,  0.379834,  0.070588,\n",
              "        0.196073, -0.368222,  0.473406,  0.007409,  0.004303, -0.007823,\n",
              "       -0.19103 , -0.202509,  0.109878, -0.224521, -0.35741 , -0.611633,\n",
              "        0.329958, -0.212956, -0.497499, -0.393839, -0.130101, -0.216903,\n",
              "       -0.105595, -0.076007, -0.483942, -0.139704, -0.161647,  0.136985,\n",
              "        0.415363, -0.360143,  0.038601, -0.078804, -0.030421,  0.324129,\n",
              "        0.223378, -0.523636, -0.048317, -0.032248, -0.117367,  0.470519,\n",
              "        0.225816, -0.222065, -0.225007, -0.165904, -0.334389, -0.20157 ,\n",
              "        0.572352, -0.268794,  0.301929, -0.005563,  0.387491,  0.261031,\n",
              "       -0.11613 ,  0.074982, -0.008433,  0.259987, -0.099893, -0.268875,\n",
              "       -0.054047, -0.534776, -0.111101, -0.051742,  0.214114,  0.04293 ,\n",
              "        0.039873, -0.453112,  0.087382, -0.333201, -0.034079, -0.833045,\n",
              "        0.155232, -1.132393, -0.294766,  0.327572], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need the embeddings since we're going to use our own embedding layer. What we're interested in are the subword tokens and their respective ids. The ids will be used as indexes into our embedding layer.<br>\n",
        "\n",
        "If this doesn't sound familiar, refer to the module on word vectors:<br>\n",
        "https://www.nlpdemystified.org/course/word-vectors"
      ],
      "metadata": {
        "id": "YZ7wTWoUI4Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the subword tokens for our example sentence from the slides. **BPEmb** places underscores in front of any tokens which are whole words or intended to begin words.<br>\n",
        "\n",
        "Remember that subword tokenizers are trained using count frequencies over a corpus. So these subword tokens are specific to **BPEmb**. Another subword tokenizer may output something different. This is why it's important that when we use a pretrained model, we make sure to use the pretrained model's tokenizer. We'll see this when we use pretrained transformers later in this module."
      ],
      "metadata": {
        "id": "JnW_aHliJdRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"Where can I find a pizzeria?\"\n",
        "tokens = bpemb_en.encode(sample_sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "AIgpfG3hKjbZ",
        "outputId": "c51c4bea-5832-4ac7-dde4-7c965478bc30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can retrieve each subword token's respective id using the *encode_ids* method."
      ],
      "metadata": {
        "id": "-WIjAEwLKwwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_seq = np.array(bpemb_en.encode_ids(\"Where can I find a pizzeria?\"))\n",
        "print(token_seq)"
      ],
      "metadata": {
        "id": "grMR-DHEKjWx",
        "outputId": "ef2b5bc6-af60-4c57-85f1-1e7fb72a40d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 571  280  386 1934    4   24  248 4339  177 9967]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a way to tokenize and vectorize sentences, we can declare and use an embedding layer with the same vocabulary size as **BPEmb** and a desired embedding size."
      ],
      "metadata": {
        "id": "Mqz7PY5nSGiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embed = tf.keras.layers.Embedding(bpemb_vocab_size, embed_dim)\n",
        "token_embeddings = token_embed(token_seq)\n",
        "\n",
        "# The untrained embeddings for our sample sentence.\n",
        "print(\"Embeddings for: \", sample_sentence)\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "id": "UO7eOOrWKjSc",
        "outputId": "5bfdad89-9339-4ed7-e0b6-d948c400b8f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings for:  Where can I find a pizzeria?\n",
            "tf.Tensor(\n",
            "[[ 0.00400496  0.04831305  0.04303933  0.01233246 -0.01075653 -0.04128264\n",
            "   0.02047274 -0.01051215  0.00373044  0.03636954  0.00092835 -0.04483579]\n",
            " [ 0.01745489  0.02249477 -0.04775207 -0.00345278  0.01651353 -0.01490047\n",
            "  -0.00393187  0.03352263  0.01207221  0.02686096  0.01110649 -0.00380627]\n",
            " [-0.00967174 -0.03257737 -0.00390768  0.00575004  0.0338132  -0.01474513\n",
            "   0.02396044 -0.04178037 -0.04993768  0.04088691 -0.02797516 -0.0249719 ]\n",
            " [ 0.00880096 -0.03210081  0.04768225 -0.04392649 -0.01147266  0.04357504\n",
            "  -0.04509336 -0.03205345 -0.03869456 -0.04352302  0.00881774 -0.04740187]\n",
            " [ 0.02461859  0.01440511  0.00757957  0.03171093 -0.04165054 -0.00717484\n",
            "   0.03559117 -0.01511825  0.04637286 -0.01531883  0.00926406 -0.00846667]\n",
            " [ 0.02212783  0.00310277 -0.00833982  0.0086063   0.01740291 -0.03091501\n",
            "   0.02407627  0.04326409 -0.04548197  0.01329801  0.02752054 -0.04685855]\n",
            " [-0.04001486  0.01312245 -0.02921597  0.03383039  0.03113751  0.03910092\n",
            "   0.03785039 -0.01863844 -0.03420297 -0.00525854 -0.00027468 -0.01952448]\n",
            " [ 0.03205439 -0.0134112   0.01760255 -0.00588813 -0.03135715  0.04201624\n",
            "   0.00853731 -0.04937469  0.04855554 -0.01216799 -0.04658788 -0.04345956]\n",
            " [-0.03350498  0.01305172 -0.0267699   0.03165681 -0.01099559 -0.03740237\n",
            "   0.02603699  0.04137218  0.03357048 -0.00114843 -0.00597314  0.02426874]\n",
            " [-0.02744256 -0.01074381 -0.02101452  0.00152152 -0.00373648  0.01194479\n",
            "  -0.03708304 -0.01868795  0.0434688  -0.03380711 -0.02711552 -0.0064506 ]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to add *positional* information to each token embedding. As we covered in the slides, the original paper used sinusoidals but it's more common these days to just use another set of embeddings. We'll do the latter here.<br>\n",
        "\n",
        "Here, we're declaring an embedding layer with rows equalling a maximum sequence length and columns equalling our token embedding size. We then generate a vector of position ids."
      ],
      "metadata": {
        "id": "20Bg_sB5TzEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 256\n",
        "pos_embed = tf.keras.layers.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "# Generate ids for each position of the token sequence.\n",
        "pos_idx = tf.range(len(token_seq))\n",
        "print(pos_idx)"
      ],
      "metadata": {
        "id": "pcurqcv3KjNY",
        "outputId": "ac2a7734-4e6c-4975-a398-6afe438ea7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use these position ids to index into the positional embedding layer."
      ],
      "metadata": {
        "id": "z4jK-iJP4Fve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are our positon embeddings.\n",
        "position_embeddings = pos_embed(pos_idx)\n",
        "print(\"Position embeddings for the input sequence\\n\", position_embeddings)"
      ],
      "metadata": {
        "id": "6vIgau8YMTgi",
        "outputId": "221c995c-f18f-4aba-8e62-e5071160b671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position embeddings for the input sequence\n",
            " tf.Tensor(\n",
            "[[ 0.04475912  0.01136164  0.00024324 -0.03845688  0.02116496 -0.00829645\n",
            "  -0.03297579 -0.04601423  0.01971662  0.00454406  0.0427576   0.01658064]\n",
            " [-0.0142136  -0.04328866 -0.00930465 -0.02044349 -0.02645006  0.02946636\n",
            "   0.02990742 -0.04853205  0.02957973 -0.04273129  0.00504352  0.01616876]\n",
            " [-0.03708373 -0.01538137  0.01870489 -0.01481106  0.02658054  0.04030814\n",
            "   0.0020091  -0.03595635 -0.03257509  0.00683299 -0.0383482  -0.02164124]\n",
            " [ 0.02580347 -0.04198921  0.02124074  0.0096035   0.0012651   0.0342189\n",
            "  -0.01892256  0.00388256  0.01768252 -0.00308634  0.00898805  0.00752416]\n",
            " [-0.04895539 -0.00249588 -0.02158953 -0.03222498 -0.00417761 -0.04998023\n",
            "  -0.02186648  0.02509338  0.0342718   0.02292876  0.00195863  0.0058117 ]\n",
            " [ 0.01011858  0.02461368  0.01578208 -0.03094249 -0.03148978  0.01228408\n",
            "   0.03565062 -0.03808091 -0.0099959  -0.0015573   0.00535196 -0.04983938]\n",
            " [ 0.00206789 -0.04287862 -0.00140119  0.00992222 -0.01193416  0.01602603\n",
            "   0.02797588  0.01402998  0.02338434 -0.03295859 -0.04677201 -0.00993322]\n",
            " [ 0.00790675 -0.0499494  -0.03793336 -0.02526318  0.04493964  0.00807432\n",
            "   0.03031167  0.00316157 -0.01718757 -0.03598303  0.02484909 -0.03686292]\n",
            " [ 0.01933955 -0.03032041 -0.01051897 -0.03470893  0.03535989 -0.00927032\n",
            "   0.01510377 -0.02376567 -0.00305985  0.02299512 -0.01573763 -0.00524415]\n",
            " [ 0.02090091 -0.02283262 -0.01247336  0.01104982 -0.03998274  0.04394547\n",
            "  -0.04345297 -0.0100079   0.01501261 -0.04886566  0.00357474 -0.01248946]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to add our token and position embeddings. The result will be the input to the first encoder block."
      ],
      "metadata": {
        "id": "UC6V2IodUhbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = token_embeddings + position_embeddings\n",
        "print(\"Input to the initial encoder block:\\n\", input)"
      ],
      "metadata": {
        "id": "K6x9JVlTKjIi",
        "outputId": "304ce51b-310f-49bd-9fd4-36df2c9a7b37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the initial encoder block:\n",
            " tf.Tensor(\n",
            "[[ 0.04876408  0.05967468  0.04328257 -0.02612442  0.01040844 -0.0495791\n",
            "  -0.01250305 -0.05652638  0.02344706  0.04091359  0.04368596 -0.02825515]\n",
            " [ 0.00324129 -0.02079389 -0.05705673 -0.02389627 -0.00993653  0.01456589\n",
            "   0.02597555 -0.01500941  0.04165194 -0.01587032  0.01615001  0.01236249]\n",
            " [-0.04675547 -0.04795875  0.01479721 -0.00906102  0.06039374  0.02556302\n",
            "   0.02596954 -0.07773672 -0.08251277  0.04771991 -0.06632335 -0.04661315]\n",
            " [ 0.03460443 -0.07409002  0.06892299 -0.03432299 -0.01020757  0.07779393\n",
            "  -0.06401591 -0.02817088 -0.02101204 -0.04660936  0.01780579 -0.03987772]\n",
            " [-0.0243368   0.01190922 -0.01400997 -0.00051405 -0.04582815 -0.05715507\n",
            "   0.0137247   0.00997513  0.08064467  0.00760993  0.01122269 -0.00265497]\n",
            " [ 0.03224641  0.02771645  0.00744226 -0.02233618 -0.01408686 -0.01863093\n",
            "   0.05972689  0.00518319 -0.05547787  0.01174071  0.0328725  -0.09669793]\n",
            " [-0.03794698 -0.02975617 -0.03061715  0.04375261  0.01920335  0.05512695\n",
            "   0.06582627 -0.00460846 -0.01081863 -0.03821713 -0.04704669 -0.0294577 ]\n",
            " [ 0.03996114 -0.0633606  -0.02033081 -0.03115131  0.01358249  0.05009056\n",
            "   0.03884898 -0.04621312  0.03136797 -0.04815102 -0.02173879 -0.08032248]\n",
            " [-0.01416543 -0.01726868 -0.03728887 -0.00305212  0.0243643  -0.04667269\n",
            "   0.04114076  0.01760651  0.03051063  0.02184669 -0.02171077  0.01902459]\n",
            " [-0.00654165 -0.03357643 -0.03348788  0.01257133 -0.04371922  0.05589026\n",
            "  -0.08053601 -0.02869585  0.05848141 -0.08267277 -0.02354078 -0.01894007]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "LDctrWODMNG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have an encoder block and a way to embed our tokens with position information, we can create the **encoder** itself.<br>\n",
        "\n",
        "Given a batch of vectorized sequences, the encoder creates positional embeddings, runs them through its encoder blocks, and returns contextualized tokens."
      ],
      "metadata": {
        "id": "LmV5KuIXWSUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(src_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    # The original Attention Is All You Need paper applied dropout to the\n",
        "    # input before feeding it to the first encoder block.\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    # Create encoder blocks.\n",
        "    self.blocks = [EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate)\n",
        "    for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, input, training, mask):\n",
        "    token_embeds = self.token_embed(input)\n",
        "\n",
        "    # Generate position indices for a batch of input sequences.\n",
        "    num_pos = input.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, input.shape)\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    # Run input through successive encoder blocks.\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(x, training, mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "NinUihSpC6K-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're wondering about this code block here:\n",
        "\n",
        "\n",
        "```\n",
        "num_pos = input.shape[0] * self.max_seq_len\n",
        "pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "pos_idx = np.reshape(pos_idx, input.shape)\n",
        "pos_embeds = self.pos_embed(pos_idx)\n",
        "```\n",
        "\n",
        "\n",
        "This generates positional embeddings for a *batch* of input sequences. Suppose this was our batch of input sequences to the encoder."
      ],
      "metadata": {
        "id": "xb7v8lKuYTT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch of 3 sequences, each of length 10 (10 is also the\n",
        "# maximum sequence length in this case).\n",
        "seqs = np.random.randint(0, 10000, size=(3, 10))\n",
        "print(seqs.shape)\n",
        "print(seqs)"
      ],
      "metadata": {
        "id": "Cllud1-mJhNi",
        "outputId": "ca989586-e23c-4c01-8c8e-8bcf1b4123a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[8875 5397 4993 4270 4891 4655 9074 9709 5166 6044]\n",
            " [4026  823 3671 7558 8165  840 9570 3884 3508 8435]\n",
            " [5566 1853 7246  304 1098  715 5040 4420 4135 4374]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to retrieve a positional embedding for every element in this batch. The first step is to create the respective positional ids..."
      ],
      "metadata": {
        "id": "DUjolKY8ZC-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.resize(np.arange(seqs.shape[1]), seqs.shape[0] * seqs.shape[1])\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "id": "WgfMkY6fk4I4",
        "outputId": "1fe3604c-fb99-472b-f1da-86d51cbd41d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and then reshape them to match the input batch dimensions."
      ],
      "metadata": {
        "id": "5OMssAJLZbAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.reshape(pos_ids, (3, 10))\n",
        "print(pos_ids.shape)\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "id": "ah0t-pZznGWt",
        "outputId": "ae8468df-9677-46ae-8343-b90844e78351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now retrieve position embeddings for every token embedding."
      ],
      "metadata": {
        "id": "TphnVF8_ZxzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embed(pos_ids)"
      ],
      "metadata": {
        "id": "cAODAGYAwpAr",
        "outputId": "491c58b0-4cfb-4b44-a89f-c006e66831ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10, 12), dtype=float32, numpy=\n",
              "array([[[ 0.04475912,  0.01136164,  0.00024324, -0.03845688,\n",
              "          0.02116496, -0.00829645, -0.03297579, -0.04601423,\n",
              "          0.01971662,  0.00454406,  0.0427576 ,  0.01658064],\n",
              "        [-0.0142136 , -0.04328866, -0.00930465, -0.02044349,\n",
              "         -0.02645006,  0.02946636,  0.02990742, -0.04853205,\n",
              "          0.02957973, -0.04273129,  0.00504352,  0.01616876],\n",
              "        [-0.03708373, -0.01538137,  0.01870489, -0.01481106,\n",
              "          0.02658054,  0.04030814,  0.0020091 , -0.03595635,\n",
              "         -0.03257509,  0.00683299, -0.0383482 , -0.02164124],\n",
              "        [ 0.02580347, -0.04198921,  0.02124074,  0.0096035 ,\n",
              "          0.0012651 ,  0.0342189 , -0.01892256,  0.00388256,\n",
              "          0.01768252, -0.00308634,  0.00898805,  0.00752416],\n",
              "        [-0.04895539, -0.00249588, -0.02158953, -0.03222498,\n",
              "         -0.00417761, -0.04998023, -0.02186648,  0.02509338,\n",
              "          0.0342718 ,  0.02292876,  0.00195863,  0.0058117 ],\n",
              "        [ 0.01011858,  0.02461368,  0.01578208, -0.03094249,\n",
              "         -0.03148978,  0.01228408,  0.03565062, -0.03808091,\n",
              "         -0.0099959 , -0.0015573 ,  0.00535196, -0.04983938],\n",
              "        [ 0.00206789, -0.04287862, -0.00140119,  0.00992222,\n",
              "         -0.01193416,  0.01602603,  0.02797588,  0.01402998,\n",
              "          0.02338434, -0.03295859, -0.04677201, -0.00993322],\n",
              "        [ 0.00790675, -0.0499494 , -0.03793336, -0.02526318,\n",
              "          0.04493964,  0.00807432,  0.03031167,  0.00316157,\n",
              "         -0.01718757, -0.03598303,  0.02484909, -0.03686292],\n",
              "        [ 0.01933955, -0.03032041, -0.01051897, -0.03470893,\n",
              "          0.03535989, -0.00927032,  0.01510377, -0.02376567,\n",
              "         -0.00305985,  0.02299512, -0.01573763, -0.00524415],\n",
              "        [ 0.02090091, -0.02283262, -0.01247336,  0.01104982,\n",
              "         -0.03998274,  0.04394547, -0.04345297, -0.0100079 ,\n",
              "          0.01501261, -0.04886566,  0.00357474, -0.01248946]],\n",
              "\n",
              "       [[ 0.04475912,  0.01136164,  0.00024324, -0.03845688,\n",
              "          0.02116496, -0.00829645, -0.03297579, -0.04601423,\n",
              "          0.01971662,  0.00454406,  0.0427576 ,  0.01658064],\n",
              "        [-0.0142136 , -0.04328866, -0.00930465, -0.02044349,\n",
              "         -0.02645006,  0.02946636,  0.02990742, -0.04853205,\n",
              "          0.02957973, -0.04273129,  0.00504352,  0.01616876],\n",
              "        [-0.03708373, -0.01538137,  0.01870489, -0.01481106,\n",
              "          0.02658054,  0.04030814,  0.0020091 , -0.03595635,\n",
              "         -0.03257509,  0.00683299, -0.0383482 , -0.02164124],\n",
              "        [ 0.02580347, -0.04198921,  0.02124074,  0.0096035 ,\n",
              "          0.0012651 ,  0.0342189 , -0.01892256,  0.00388256,\n",
              "          0.01768252, -0.00308634,  0.00898805,  0.00752416],\n",
              "        [-0.04895539, -0.00249588, -0.02158953, -0.03222498,\n",
              "         -0.00417761, -0.04998023, -0.02186648,  0.02509338,\n",
              "          0.0342718 ,  0.02292876,  0.00195863,  0.0058117 ],\n",
              "        [ 0.01011858,  0.02461368,  0.01578208, -0.03094249,\n",
              "         -0.03148978,  0.01228408,  0.03565062, -0.03808091,\n",
              "         -0.0099959 , -0.0015573 ,  0.00535196, -0.04983938],\n",
              "        [ 0.00206789, -0.04287862, -0.00140119,  0.00992222,\n",
              "         -0.01193416,  0.01602603,  0.02797588,  0.01402998,\n",
              "          0.02338434, -0.03295859, -0.04677201, -0.00993322],\n",
              "        [ 0.00790675, -0.0499494 , -0.03793336, -0.02526318,\n",
              "          0.04493964,  0.00807432,  0.03031167,  0.00316157,\n",
              "         -0.01718757, -0.03598303,  0.02484909, -0.03686292],\n",
              "        [ 0.01933955, -0.03032041, -0.01051897, -0.03470893,\n",
              "          0.03535989, -0.00927032,  0.01510377, -0.02376567,\n",
              "         -0.00305985,  0.02299512, -0.01573763, -0.00524415],\n",
              "        [ 0.02090091, -0.02283262, -0.01247336,  0.01104982,\n",
              "         -0.03998274,  0.04394547, -0.04345297, -0.0100079 ,\n",
              "          0.01501261, -0.04886566,  0.00357474, -0.01248946]],\n",
              "\n",
              "       [[ 0.04475912,  0.01136164,  0.00024324, -0.03845688,\n",
              "          0.02116496, -0.00829645, -0.03297579, -0.04601423,\n",
              "          0.01971662,  0.00454406,  0.0427576 ,  0.01658064],\n",
              "        [-0.0142136 , -0.04328866, -0.00930465, -0.02044349,\n",
              "         -0.02645006,  0.02946636,  0.02990742, -0.04853205,\n",
              "          0.02957973, -0.04273129,  0.00504352,  0.01616876],\n",
              "        [-0.03708373, -0.01538137,  0.01870489, -0.01481106,\n",
              "          0.02658054,  0.04030814,  0.0020091 , -0.03595635,\n",
              "         -0.03257509,  0.00683299, -0.0383482 , -0.02164124],\n",
              "        [ 0.02580347, -0.04198921,  0.02124074,  0.0096035 ,\n",
              "          0.0012651 ,  0.0342189 , -0.01892256,  0.00388256,\n",
              "          0.01768252, -0.00308634,  0.00898805,  0.00752416],\n",
              "        [-0.04895539, -0.00249588, -0.02158953, -0.03222498,\n",
              "         -0.00417761, -0.04998023, -0.02186648,  0.02509338,\n",
              "          0.0342718 ,  0.02292876,  0.00195863,  0.0058117 ],\n",
              "        [ 0.01011858,  0.02461368,  0.01578208, -0.03094249,\n",
              "         -0.03148978,  0.01228408,  0.03565062, -0.03808091,\n",
              "         -0.0099959 , -0.0015573 ,  0.00535196, -0.04983938],\n",
              "        [ 0.00206789, -0.04287862, -0.00140119,  0.00992222,\n",
              "         -0.01193416,  0.01602603,  0.02797588,  0.01402998,\n",
              "          0.02338434, -0.03295859, -0.04677201, -0.00993322],\n",
              "        [ 0.00790675, -0.0499494 , -0.03793336, -0.02526318,\n",
              "          0.04493964,  0.00807432,  0.03031167,  0.00316157,\n",
              "         -0.01718757, -0.03598303,  0.02484909, -0.03686292],\n",
              "        [ 0.01933955, -0.03032041, -0.01051897, -0.03470893,\n",
              "          0.03535989, -0.00927032,  0.01510377, -0.02376567,\n",
              "         -0.00305985,  0.02299512, -0.01573763, -0.00524415],\n",
              "        [ 0.02090091, -0.02283262, -0.01247336,  0.01104982,\n",
              "         -0.03998274,  0.04394547, -0.04345297, -0.0100079 ,\n",
              "          0.01501261, -0.04886566,  0.00357474, -0.01248946]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try our encoder on a batch of sentences."
      ],
      "metadata": {
        "id": "e-4hBnztXfN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = [\n",
        "    \"Where can I find a pizzeria?\",\n",
        "    \"Mass hysteria over listeria.\",\n",
        "    \"I ain't no circle back girl.\"\n",
        "]\n",
        "\n",
        "bpemb_en.encode(input_batch)"
      ],
      "metadata": {
        "id": "jbX82NUpwyGL",
        "outputId": "c34739db-e98d-4676-82ba-0916ff353a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?'],\n",
              " ['▁mass', '▁hy', 'ster', 'ia', '▁over', '▁l', 'ister', 'ia', '.'],\n",
              " ['▁i', '▁a', 'in', \"'\", 't', '▁no', '▁circle', '▁back', '▁girl', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = bpemb_en.encode_ids(input_batch)\n",
        "print(\"Vectorized inputs:\")\n",
        "input_seqs"
      ],
      "metadata": {
        "id": "wOXHqq2Kxh5r",
        "outputId": "36d3ffcd-d058-4d57-c446-ca5f84152330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized inputs:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[571, 280, 386, 1934, 4, 24, 248, 4339, 177, 9967],\n",
              " [1535, 1354, 1238, 177, 380, 43, 871, 177, 9935],\n",
              " [386, 4, 6, 9937, 9915, 467, 5410, 810, 3692, 9935]]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how the input sequences aren't the same length in this batch. In this case, we need to pad them out so that they are. If you're unfamiliar with why, refer to the notebook on Recurrent Neural Networks:<br>\n",
        "https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_recurrent_neural_networks.ipynb<br>\n",
        "\n",
        "We'll do this using *pad_sequences*.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences"
      ],
      "metadata": {
        "id": "EOgoulJTb7Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding=\"post\")\n",
        "print(\"Input to the encoder:\")\n",
        "print(padded_input_seqs.shape)\n",
        "print(padded_input_seqs)"
      ],
      "metadata": {
        "id": "np2vsXpwxMS8",
        "outputId": "62a9af31-72a8-4ca6-aa78-ded658a543f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the encoder:\n",
            "(3, 10)\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our input now has padding, now's a good time to cover **masking**.\n",
        "<br>\n",
        "\n",
        "So given a mask, wherever there's a mask position set to 0, the corresponding position in the attention scores will be set to *-inf*. The resulting attention weight for the position will then be zero and no attending will occur for that position.\n",
        "<br>\n",
        "\n",
        "In the slides, we covered *look-ahead* masks for the decoder to prevent it from attending to future tokens, but we also need masks for padding.\n",
        "<br>\n",
        "\n",
        "In total, there are three masks involved:\n",
        "1. The *encoder mask* to mask out any padding in the encoder sequences.\n",
        "\n",
        "2. The *decoder mask* which is used in the decoder's **first** multi-head self-attention layer. It's a <u>combination of two masks</u>: one to account for the padding in target sequences, and the look-ahead mask.\n",
        "\n",
        "3. The *memory mask* which is used in the decoder's **second** multi-head self-attention layer. The keys and values for this layer are going to be the encoder's output, and this mask will ensure the decoder doesn't attend to any encoder output which corresponds to padding. In practice, 1 and 3 are often the same.\n",
        "\n",
        "The *scaled_dot_product_attention* function has this line:\n",
        "```\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "```"
      ],
      "metadata": {
        "id": "PqkDdMKJVSa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an encoder mask for our batch of input sequences.<br>\n",
        "\n",
        "Wherever there's padding, we want the mask position set to zero."
      ],
      "metadata": {
        "id": "41HyT3jSVq0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_mask = tf.cast(tf.math.not_equal(padded_input_seqs, 0), tf.float32)\n",
        "print(\"Input:\")\n",
        "print(padded_input_seqs, '\\n')\n",
        "print(\"Encoder mask:\")\n",
        "print(enc_mask)"
      ],
      "metadata": {
        "id": "AHvAAVhnZouZ",
        "outputId": "ea97a0bd-9e54-46c9-d3e5-71e067a3b8eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]] \n",
            "\n",
            "Encoder mask:\n",
            "tf.Tensor(\n",
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(3, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind that the dimension of the attention matrix (for this example) is going to be:<br>\n",
        "*(batch size, number of heads, query size, key size)*<br>\n",
        "(3, 3, 10, 10)"
      ],
      "metadata": {
        "id": "idqcJwFhZ7zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we need to expand the mask dimensions like so:"
      ],
      "metadata": {
        "id": "vgVXwdwra84q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_mask = enc_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "enc_mask"
      ],
      "metadata": {
        "id": "aYPlbsrvZu8_",
        "outputId": "069704a5-3e67-4212-aa9e-48b39ca2078f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 10), dtype=float32, numpy=\n",
              "array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way, the encoder mask will now be *broadcasted*.<br>\n",
        "https://www.tensorflow.org/xla/broadcasting"
      ],
      "metadata": {
        "id": "nsJEDxNPckz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can declare an encoder and pass it batches of vectorized sequences."
      ],
      "metadata": {
        "id": "X87_VQmiVbSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_blocks = 6\n",
        "\n",
        "# d_model is the embedding dimension used throughout.\n",
        "d_model = 12\n",
        "\n",
        "num_heads = 3\n",
        "\n",
        "# Feed-forward network hidden dimension width.\n",
        "ffn_hidden_dim = 48\n",
        "\n",
        "src_vocab_size = bpemb_vocab_size\n",
        "max_input_seq_len = padded_input_seqs.shape[1]\n",
        "\n",
        "encoder = Encoder(\n",
        "    num_encoder_blocks,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    ffn_hidden_dim,\n",
        "    src_vocab_size,\n",
        "    max_input_seq_len)"
      ],
      "metadata": {
        "id": "Ns8G5ujRVQMv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now pass our input sequences and mask to the encoder."
      ],
      "metadata": {
        "id": "hGQ6lg3fJhIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_output, attn_weights = encoder(padded_input_seqs, training=True,\n",
        "                                       mask=enc_mask)\n",
        "print(f\"Encoder output {encoder_output.shape}:\")\n",
        "print(encoder_output)"
      ],
      "metadata": {
        "id": "rf6q86hBj8eV",
        "outputId": "fed8edef-509d-40e8-dcc9-655b884ee9e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output (3, 10, 12):\n",
            "tf.Tensor(\n",
            "[[[-0.20844007  0.30584323 -0.6369229   0.859495   -0.01236432\n",
            "   -0.50558317 -1.475137    0.64451116  2.2667232   0.5639428\n",
            "   -0.2757076  -1.5263603 ]\n",
            "  [ 1.0040327  -0.90436524 -0.6418579   0.2745777  -0.27483612\n",
            "    0.22397393 -0.4392896   0.9346314   0.8046457   1.3477252\n",
            "    0.12138812 -2.4506254 ]\n",
            "  [-0.8286879  -0.44620386  1.5278351   1.7705213  -0.16286017\n",
            "   -0.04525811 -1.8712609  -0.32909733  1.0411451   0.3862133\n",
            "   -0.19126712 -0.85107934]\n",
            "  [ 0.73303103 -2.185224   -0.22593173  1.1442285   1.1382056\n",
            "   -0.5883463  -0.99556524 -0.00529124  1.4696492   0.36057872\n",
            "   -0.2918209  -0.55351365]\n",
            "  [ 0.38723007 -0.9957127  -1.3574787  -0.07031898  0.59186244\n",
            "   -1.1414843   0.39921477  1.678804    1.5273733   0.24442498\n",
            "    0.13695456 -1.4008698 ]\n",
            "  [ 0.72962576 -0.95066804 -2.1555886   0.38998362  2.2392745\n",
            "    0.22314106 -0.4409975   0.5537008  -0.12353037  0.05372484\n",
            "   -0.1253061  -0.39336   ]\n",
            "  [ 0.4999636  -2.4096107   0.56715846  0.08888942  1.3543295\n",
            "    0.3935043  -0.0029774  -0.2895506  -1.1352321   0.8837715\n",
            "    0.8786628  -0.82890856]\n",
            "  [ 0.09399717 -0.29785347 -0.7223872   0.7619211   0.20292321\n",
            "   -0.44155386 -1.652044    0.2029805   2.4610295   0.8101037\n",
            "   -0.5283209  -0.8907955 ]\n",
            "  [-0.6667768  -0.5214265  -0.7719064   0.6200046   0.14011556\n",
            "   -0.7758303  -1.5419018   1.01541     2.1957364   0.2061023\n",
            "    0.8859018  -0.78542846]\n",
            "  [ 0.2709771  -0.7535329  -2.0304322  -0.14754817  1.1381593\n",
            "   -0.571757   -0.35918579  1.3023031   1.5558282   0.16049741\n",
            "    0.4768147  -1.0421238 ]]\n",
            "\n",
            " [[ 0.31308502 -1.044371    0.3399091   1.3426204   1.209045\n",
            "   -0.3474041   0.70159847  0.94595945 -2.1179163  -1.0113014\n",
            "    0.2535053  -0.58472985]\n",
            "  [-0.02377666 -1.150693   -1.2951796  -1.3185778   0.58957046\n",
            "    0.31678066 -0.01410195  1.179338    1.4052719  -0.2657199\n",
            "    1.5539532  -0.97686505]\n",
            "  [-0.3249168  -0.7019572   0.65329766 -0.6746953   1.3192661\n",
            "    0.28373146 -1.2580994   0.6653888  -0.10081063  0.31582317\n",
            "    1.719513   -1.896541  ]\n",
            "  [ 0.17841315 -1.4929242  -0.7309516  -0.9197092   1.2291964\n",
            "    0.09630524 -0.11165776  0.46562222  1.453656   -1.0464771\n",
            "    1.6658195  -0.7872924 ]\n",
            "  [ 0.6482874  -1.7113714   0.27763426 -1.3567799   0.8508638\n",
            "    0.29495034 -0.07986779  0.7162645   1.0044043  -0.85018075\n",
            "    1.4527357  -1.246941  ]\n",
            "  [ 0.93594486 -0.2437636   1.7700216   0.87143534 -0.48954836\n",
            "    0.20628856 -0.3401493   0.11490081 -0.21882501 -1.5285614\n",
            "    0.8370023  -1.914746  ]\n",
            "  [ 0.890138   -0.71036863  0.95565295 -0.71507126  0.69198966\n",
            "    0.51862806 -0.10390089  0.69043046  0.02647053 -1.1315598\n",
            "    1.1950585  -2.3074672 ]\n",
            "  [-0.84995675 -0.9269228  -0.87761164 -0.8615952   1.504208\n",
            "    0.05420898 -0.99532515  0.6636892   1.191199    0.02480573\n",
            "    1.8027978  -0.72949713]\n",
            "  [-0.5928443  -1.2910898  -1.0721332  -0.31884164  1.428323\n",
            "   -0.3837154  -0.8073934   0.8511214   1.2847956  -0.05599498\n",
            "    1.7127239  -0.7549515 ]\n",
            "  [ 0.8795504  -1.2777691   0.7326041  -0.10960266 -0.6228439\n",
            "    0.42779276  0.43415132  1.2208143  -0.5058944  -1.1802484\n",
            "    1.6033736  -1.601928  ]]\n",
            "\n",
            " [[-0.6619574   0.38499385 -0.1546658   0.5398481  -0.04338668\n",
            "   -0.06898666 -1.1130533   1.2995427   0.8868068  -0.4405408\n",
            "    1.5531657  -2.1817665 ]\n",
            "  [ 0.00371492 -1.3462362  -1.2200476  -0.9997123   1.496605\n",
            "   -0.4307818  -0.24178602  1.747723    0.1623655   0.4908665\n",
            "    1.125284   -0.78799486]\n",
            "  [-0.6570456  -0.9101478  -0.95551866  1.2798734   0.8009933\n",
            "   -0.56518346  0.02422371  1.2974441   0.07037793 -0.40426904\n",
            "    1.6471705  -1.6279185 ]\n",
            "  [ 0.32788926 -1.5600249  -1.4349382  -0.26566756  0.9814935\n",
            "    0.38007626  0.39786565  1.8105747   0.13770442 -0.1636399\n",
            "    0.8223767  -1.4337101 ]\n",
            "  [ 0.05198662 -0.6641135  -1.4070841  -0.6835152   0.50105876\n",
            "    0.15275857 -0.6093891   2.288635    0.886761   -0.48669758\n",
            "    1.0319376  -1.0623381 ]\n",
            "  [ 0.37370074 -0.46810907 -0.33496952  0.8716653   0.77398694\n",
            "    0.32178038 -0.72905475  1.6925122   0.04014524 -0.6333781\n",
            "    0.5349695  -2.4432485 ]\n",
            "  [ 0.6098335  -1.1347739  -1.0002792   0.27001655  0.5791506\n",
            "    0.87212616 -0.40300328  2.031886    0.21062526 -0.4367001\n",
            "    0.2859888  -1.8848705 ]\n",
            "  [-1.8509951   0.16908805 -1.0523443   0.76690435  0.68389845\n",
            "   -0.47807086 -1.3148873   0.2241729   1.2984445   0.1441561\n",
            "    1.6167216  -0.20708853]\n",
            "  [-0.5607777  -1.5536487  -0.76820666  0.11500587  0.9659379\n",
            "   -0.65018386 -0.9086726   2.1793294   0.4985325   0.3825293\n",
            "    0.95228976 -0.652135  ]\n",
            "  [ 0.44216934 -1.0101144  -0.7881076   0.3110491  -0.18181823\n",
            "    0.5366507   0.05543397  1.8783308   0.69583    -0.9250778\n",
            "    0.9674455  -1.9817914 ]]], shape=(3, 10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Block"
      ],
      "metadata": {
        "id": "24TYaX3zMNAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the **Decoder Block**. Everything we did to create the **encoder** block applies here. The major differences are that the **Decoder Block** has:\n",
        "1. a **Multi-Head Cross-Attention** layer which uses the encoder's outputs as the keys and values.\n",
        "\n",
        "2. an extra skip/residual connection along with an extra layer normalization step.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WVT4SX49bnta4uscOTF4xrsxFI4PbPER\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "uH-5iDDXeU_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  # Note the decoder block takes two masks. One for the first MHSA, another\n",
        "  # for the second MHSA.\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
        "    mhsa_output1 = self.dropout1(mhsa_output1, training=training)\n",
        "    mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
        "\n",
        "    mhsa_output2, attn_weights = self.mhsa2(mhsa_output1, encoder_output,\n",
        "                                            encoder_output,\n",
        "                                            memory_mask)\n",
        "    mhsa_output2 = self.dropout2(mhsa_output2, training=training)\n",
        "    mhsa_output2 = self.layernorm2(mhsa_output2 + mhsa_output1)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    output = self.layernorm3(ffn_output + mhsa_output2)\n",
        "\n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "Hco1IwfutNqD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "YVstTioxMNDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is almost the same as the encoder except it takes the encoder's output as part of its input, and it takes two masks: the decoder mask and memory mask."
      ],
      "metadata": {
        "id": "M3iT7wyOi_bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    token_embeds = self.token_embed(target)\n",
        "\n",
        "    # Generate position indices.\n",
        "    num_pos = target.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, target.shape)\n",
        "\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "27zG_wV3MNJ_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we try the decoder, let's cover the masks involved. The decoder takes two masks:\n",
        "\n",
        "The *decoder mask* which is a <u>combination of two masks</u>: one to account for the padding in target sequences, and the look-ahead mask. This mask is used in the decoder's **first** multi-head self-attention layer.\n",
        "\n",
        "The *memory mask* which is used in the decoder's **second** multi-head self-attention. The keys and values for this layer are going to be the encoder's output, and this mask will ensure the decoder doesn't attend to any encoder output which corresponds to padding."
      ],
      "metadata": {
        "id": "gkZ1T-hSscOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose this is our batch of vectorized target *input* sequences for the decoder. These values are just made up.<br>\n",
        "\n",
        "**Note**: If you need a refresher on how to prepare target input and output sequences for the decoder, refer to the [seq2seq notebook](https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_seq2seq_and_attention.ipynb).\n",
        "\n"
      ],
      "metadata": {
        "id": "EjiEOx5WoOb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Made up values.\n",
        "target_input_seqs = [\n",
        "    [1, 652, 723, 123, 62],\n",
        "    [1, 25,  98, 129, 248, 215, 359, 249],\n",
        "    [1, 2369, 1259, 125, 486],\n",
        "]"
      ],
      "metadata": {
        "id": "0X6gKNzgv0gP"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did with the encoder input sequences, we need to pad out this batch so that all sequences within it are the same length."
      ],
      "metadata": {
        "id": "SgriJUKgyxNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_target_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_input_seqs, padding=\"post\")\n",
        "print(\"Padded target inputs to the decoder:\")\n",
        "print(padded_target_input_seqs.shape)\n",
        "print(padded_target_input_seqs)"
      ],
      "metadata": {
        "id": "4hFp1nkSypnz",
        "outputId": "cb8c4bb5-db71-4665-9272-2a87b2414a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded target inputs to the decoder:\n",
            "(3, 8)\n",
            "[[   1  652  723  123   62    0    0    0]\n",
            " [   1   25   98  129  248  215  359  249]\n",
            " [   1 2369 1259  125  486    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create the padding mask the same way we did for the encoder."
      ],
      "metadata": {
        "id": "qZysfgvUzNBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_padding_mask = tf.cast(tf.math.not_equal(padded_target_input_seqs, 0), tf.float32)\n",
        "dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(dec_padding_mask)"
      ],
      "metadata": {
        "id": "PLKeI4R20axA",
        "outputId": "267fb66e-936e-413f-e99c-2f5cc601775a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 1, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we covered in the slides, the look-ahead mask is a diagonal where the lower half are 1s and the upper half are zeros. This is easy to create using the *band_part* method:<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/linalg/band_part"
      ],
      "metadata": {
        "id": "S7EwYtJa0uvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_input_seq_len = padded_target_input_seqs.shape[1]\n",
        "look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len,\n",
        "                                               target_input_seq_len)), -1, 0)\n",
        "print(look_ahead_mask)"
      ],
      "metadata": {
        "id": "yZFnGgJa04a-",
        "outputId": "415c5589-a72e-459e-8936-2479a0bba54e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1.]], shape=(8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the decoder mask, we just need to combine the padding and look-ahead masks. Note how the columns of the resulting decoder mask are all zero for padding positions."
      ],
      "metadata": {
        "id": "WPzxVG2S87T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
        "print(\"The decoder mask:\")\n",
        "print(dec_mask)"
      ],
      "metadata": {
        "id": "vArTOY1x2bzn",
        "outputId": "223b1fd9-52f4-4ca8-aa40-aea9790bc873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decoder mask:\n",
            "tf.Tensor(\n",
            "[[[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now declare a decoder and pass it everything it needs. In our case, the *memory* mask is the same as the *encoder* mask."
      ],
      "metadata": {
        "id": "iLHbt7nJ9xUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(6, 12, 3, 48, 10000, 8)\n",
        "decoder_output, _ = decoder(encoder_output, padded_target_input_seqs,\n",
        "                            True, dec_mask, enc_mask)\n",
        "print(f\"Decoder output {decoder_output.shape}:\")\n",
        "print(decoder_output)"
      ],
      "metadata": {
        "id": "bFE-VaCrmLKu",
        "outputId": "6505ac24-a628-48ea-e5b8-f6e2415d0747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output (3, 8, 12):\n",
            "tf.Tensor(\n",
            "[[[-0.7259985  -0.97611994 -0.28941667  1.3896121   1.4586916\n",
            "    1.3402506  -0.46452475 -0.14873719  0.95775634 -0.7594686\n",
            "   -0.10043172 -1.6816127 ]\n",
            "  [-0.5760346  -1.0997835   0.1528048   1.1035895   1.5172669\n",
            "    1.1260314   0.7329282  -0.9900956   0.35885364 -1.8439285\n",
            "    0.2483876  -0.73002   ]\n",
            "  [-0.6364259  -0.7748547   0.28305164  0.9715581   1.0522184\n",
            "    1.7745788   0.17995088 -0.20129976  0.47856066 -1.450359\n",
            "    0.1348984  -1.8118775 ]\n",
            "  [-1.1627475  -1.111045   -0.1093525   1.4833279   0.96182495\n",
            "    0.9992485  -0.07959682  0.230573    1.4518487  -1.1573806\n",
            "   -0.1933207  -1.3133799 ]\n",
            "  [-1.050879   -0.7926479  -0.2938709   0.3102484   0.45639268\n",
            "    2.0269377  -0.42594066 -0.67904633  1.3039109  -1.1693729\n",
            "    1.1561573  -0.8418889 ]\n",
            "  [-1.3144761  -0.4774183   0.26200217  0.5275038   1.0509723\n",
            "    0.8476583  -0.19162361  0.13945742  1.8465501  -1.4121077\n",
            "    0.252084   -1.5306022 ]\n",
            "  [-1.3734267  -0.80572534  0.21410486  1.0241771   1.0019125\n",
            "    0.8891526   0.17353465 -0.25722954  1.4187233  -1.6885953\n",
            "    0.54665893 -1.1432872 ]\n",
            "  [-0.7900257  -0.93288815  1.2848481   0.43035227  0.58279467\n",
            "    1.554548    0.5007039  -0.05976558  0.7808122  -1.6909635\n",
            "   -0.1962142  -1.4642017 ]]\n",
            "\n",
            " [[-1.2028667   1.5407349   1.2632904  -0.24482703 -0.14708002\n",
            "    1.3044605  -0.45142618  0.9074623   0.3305863  -1.1929964\n",
            "   -0.9855122  -1.1218263 ]\n",
            "  [-1.5652034   1.3410788   1.0378405  -0.0957035   0.53401357\n",
            "    0.9825594  -1.0374359   1.2194906   0.23692942 -1.0405952\n",
            "   -0.35871163 -1.2542624 ]\n",
            "  [-1.355387   -0.26279664  1.2399673   0.31335455  0.55479544\n",
            "    0.8410838   1.1339144   1.0390139  -0.07567763 -1.9382962\n",
            "   -0.43291807 -1.0570544 ]\n",
            "  [-1.367506    0.12276076  0.8785081   0.7380507   0.33968592\n",
            "    1.5252693  -0.57651985  0.8107512   1.0449606  -1.4672394\n",
            "   -1.1020737  -0.94664764]\n",
            "  [-1.6255317  -0.42267358  0.72608984  0.22986005  0.31514567\n",
            "    1.5071716  -0.3652974   1.7265896   0.6250993  -1.206433\n",
            "   -0.9945751  -0.5154455 ]\n",
            "  [-1.3890325  -0.06139967  0.59116715  0.67666876  0.41870898\n",
            "    1.3203588  -0.2728266   1.7763375  -0.0404519  -1.8459989\n",
            "   -0.5788684  -0.5946637 ]\n",
            "  [-1.1643099   1.2669632   0.98543775  0.04680402  0.06515152\n",
            "    1.4336486  -0.34479916  1.279376   -0.01096857 -1.3079797\n",
            "   -1.124698   -1.1246269 ]\n",
            "  [-1.2701371   0.17461129  0.6489769   0.5963375   0.26789662\n",
            "    1.9889696  -0.00865059  0.95288336 -0.05640895 -1.7999755\n",
            "   -0.3773338  -1.1171689 ]]\n",
            "\n",
            " [[-1.1318638  -0.9366901  -0.02019606  2.0279703   0.6960103\n",
            "    0.82913524 -0.41910806  0.39230767  1.1826428  -0.5326752\n",
            "   -1.4646206  -0.62291235]\n",
            "  [-1.0517658  -0.4437705   0.02880469  1.6907445  -0.6360323\n",
            "   -0.08360275 -0.2537878   1.49039     1.4206363   0.19995587\n",
            "   -1.5577465  -0.8038255 ]\n",
            "  [-0.78582925 -1.4366354   0.6109061   1.1401502  -0.31829977\n",
            "    0.06230279 -1.1658467   1.2804255   1.3522216   0.5025984\n",
            "   -1.5363928   0.29439938]\n",
            "  [-1.247365   -0.5489193   0.47687158  1.3004495  -1.0534747\n",
            "   -0.26019427 -0.69334304  0.8059786   1.9568372   0.5702367\n",
            "   -1.3235714   0.01649438]\n",
            "  [-1.1454905  -0.731186    0.78466016  1.1395687  -0.41740316\n",
            "   -0.21149988 -0.8926939   1.1189597   2.1084886  -0.31976458\n",
            "   -1.1549779  -0.2786613 ]\n",
            "  [-0.67329586 -0.5748696   0.63357407  0.8253847  -0.4728357\n",
            "   -0.02119629 -0.6435621   1.550664    1.7199944  -0.03113681\n",
            "   -2.00662    -0.30610117]\n",
            "  [-1.2988143   0.74778354  0.49934587  0.617802    0.2173918\n",
            "   -0.5493795   0.33576214  1.2542725   1.1010135  -0.92086655\n",
            "   -2.2284615   0.2241505 ]\n",
            "  [-1.2443997  -0.6425629   0.2282063   1.3660114  -0.42544657\n",
            "   -0.02815381 -0.47818643  1.0930717   1.6493356   0.07258973\n",
            "   -1.9157319   0.3252662 ]]], shape=(3, 8, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "UgFtxMQxMNNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the pieces to build the **Transformer** itself, and it's pretty simple."
      ],
      "metadata": {
        "id": "bYFJuqbl-Jt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
        "               target_vocab_size, max_input_len, max_target_len, dropout_rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
        "                           max_input_len, dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "                           max_target_len, dropout_rate)\n",
        "\n",
        "    # The final dense layer to generate logits from the decoder output.\n",
        "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, input_seqs, target_input_seqs, training, encoder_mask,\n",
        "           decoder_mask, memory_mask):\n",
        "    encoder_output, encoder_attn_weights = self.encoder(input_seqs,\n",
        "                                                        training, encoder_mask)\n",
        "\n",
        "    decoder_output, decoder_attn_weights = self.decoder(encoder_output,\n",
        "                                                        target_input_seqs, training,\n",
        "                                                        decoder_mask, memory_mask)\n",
        "\n",
        "    return self.output_layer(decoder_output), encoder_attn_weights, decoder_attn_weights\n"
      ],
      "metadata": {
        "id": "DfNkAsv8MNQ8"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_blocks = 6,\n",
        "    d_model = 12,\n",
        "    num_heads = 3,\n",
        "    hidden_dim = 48,\n",
        "    source_vocab_size = bpemb_vocab_size,\n",
        "    target_vocab_size = 7000, # made-up target vocab size.\n",
        "    max_input_len = padded_input_seqs.shape[1],\n",
        "    max_target_len = padded_target_input_seqs.shape[1])\n",
        "\n",
        "transformer_output, _, _ = transformer(padded_input_seqs,\n",
        "                                       padded_target_input_seqs, True,\n",
        "                                       enc_mask, dec_mask, memory_mask=enc_mask)\n",
        "print(f\"Transformer output {transformer_output.shape}:\")\n",
        "print(transformer_output) # If training, we would use this output to calculate losses."
      ],
      "metadata": {
        "id": "1VOou7zjQ7el",
        "outputId": "dbe38f02-1f23-48cf-8561-befc0c171fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output (3, 8, 7000):\n",
            "tf.Tensor(\n",
            "[[[ 0.00051915  0.04052141 -0.00557879 ... -0.09527363  0.03418413\n",
            "   -0.05295348]\n",
            "  [ 0.05880791  0.06468092 -0.10652198 ... -0.08754595  0.07245212\n",
            "   -0.08023539]\n",
            "  [ 0.02981197  0.09833822 -0.12007275 ... -0.04704471  0.04017722\n",
            "   -0.06536498]\n",
            "  ...\n",
            "  [ 0.02788904  0.06437693 -0.12579641 ... -0.04132092  0.06407294\n",
            "   -0.04562572]\n",
            "  [ 0.02527493  0.05832984 -0.12033436 ... -0.01126624  0.06646024\n",
            "   -0.08118615]\n",
            "  [ 0.02376048  0.0748547  -0.12640293 ... -0.03679956  0.06358375\n",
            "   -0.06299862]]\n",
            "\n",
            " [[ 0.0142796   0.02680932  0.02348097 ... -0.12607291  0.00915394\n",
            "   -0.00041237]\n",
            "  [ 0.01274822 -0.02192468 -0.01512134 ... -0.14233251 -0.08021986\n",
            "   -0.01355399]\n",
            "  [ 0.01097837  0.01521777  0.00570864 ... -0.10456658  0.00124418\n",
            "   -0.01136646]\n",
            "  ...\n",
            "  [ 0.03087964  0.00507124 -0.03227877 ... -0.13468564 -0.0152267\n",
            "   -0.0382706 ]\n",
            "  [ 0.03148315  0.05182709  0.00459516 ... -0.0714085  -0.00991692\n",
            "   -0.01888603]\n",
            "  [ 0.00623365  0.03615354  0.00705636 ... -0.07338416  0.03492629\n",
            "   -0.03649822]]\n",
            "\n",
            " [[-0.00369231  0.0944063  -0.04579535 ...  0.02434719  0.03607051\n",
            "   -0.02255307]\n",
            "  [-0.0068489   0.06023322 -0.06302023 ...  0.03158976  0.04693423\n",
            "   -0.04473332]\n",
            "  [-0.00768456  0.0466329  -0.04689573 ...  0.02666005 -0.00785149\n",
            "   -0.03793136]\n",
            "  ...\n",
            "  [ 0.02846778  0.02298648 -0.08006187 ... -0.01476826  0.01046741\n",
            "   -0.00452107]\n",
            "  [ 0.03308854  0.02251674 -0.06095785 ... -0.0045741  -0.01902023\n",
            "    0.02239389]\n",
            "  [ 0.03951726  0.07964903 -0.08521529 ... -0.03304616  0.02542707\n",
            "    0.00524234]]], shape=(3, 8, 7000), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's the whole original transformer from scratch. From here, if you want to train this transformer, you can use the same approach we used when we built the translation model with attention in the [seq2seq notebook](https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_seq2seq_and_attention.ipynb#scrollTo=x8Ef_eWXjWMn&line=3&uniqifier=1). Remember to use a learning rate warmup (Refer to the paper for more information on this)."
      ],
      "metadata": {
        "id": "BV_fyVfIPzjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's useful to know how these models work under the hood, but to train our own transformer to get impressive results is expensive. Both in terms of compute and data.<br>\n",
        "\n",
        "Fortunately, there's a zoo of **pretrained** transformer models we can use. We'll explore that next."
      ],
      "metadata": {
        "id": "UReJEI3rFKN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer"
      ],
      "metadata": {
        "id": "KwbLU1cwPFzf",
        "outputId": "b42d1a0b-3aea-4a8b-8af2-cca97f0d0e9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Transformer at 0x78c5e9dd3340>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=10e-5, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "1i_1L-PaPzU7"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxGJtoDuYIHL"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlhsJMm0TW_B"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mkd0OZ5KUQ76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "VVT9RBA4Ptgk"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "iosguCo0P7O6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "import time\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,\n",
        "                                 True,\n",
        "                                 enc_padding_mask,\n",
        "                                 combined_mask,\n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "metadata": {
        "id": "siqJJKzjQDF0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                train_loss.result(),\n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "kYt9zVA3QEIz",
        "outputId": "117478ee-a3cc-4782-8205-5944033dc94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loss' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-3111c0068ac9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tfds-nightly"
      ],
      "metadata": {
        "id": "IKmVhjEAQHez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "jB-NIajIR-mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "id": "gjL0_arcR0O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "ceNPbZQ3R0x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_string = 'Transformer is awesome.'\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_en.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "metadata": {
        "id": "Tokij5X9SHX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en"
      ],
      "metadata": {
        "id": "e85y55XoSLgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_pt"
      ],
      "metadata": {
        "id": "AsMCa9mFS9rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9KJWJjrsZ4Y"
      },
      "source": [
        "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf2ntBxjkqK6"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcRp7VcQ5m6g"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGi4PoVakxdc"
      },
      "source": [
        "Add a start and end token to the input and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZwnPr4R055s"
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
        "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "\n",
        "  return lang1, lang2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx1sFbR-9fRs"
      },
      "source": [
        "You want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
        "\n",
        "* Graph tensors do not have a value.\n",
        "* In graph mode you can only use TensorFlow Ops and functions.\n",
        "\n",
        "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mah1cS-P70Iz"
      },
      "source": [
        "def tf_encode(pt, en):\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "  result_pt.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "\n",
        "  return result_pt, result_en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JrGp5Gek6Ql"
      },
      "source": [
        "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QEgbjntk6Yf"
      },
      "source": [
        "MAX_LENGTH = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c081xPGv1CPI"
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mk9AZdZ5bcS"
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fXvfYVfQr2n"
      },
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\n",
        "pt_batch, en_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gs5JUG0xSzGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6APsFrgImLW"
      },
      "source": [
        "The following steps are used for evaluation:\n",
        "\n",
        "* Encode the input sentence using the Portuguese tokenizer (`tokenizer_pt`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
        "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
        "* Calculate the padding masks and the look ahead masks.\n",
        "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
        "* Select the last word and calculate the argmax of that.\n",
        "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
        "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
        "\n",
        "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5buvMlnvyrFm"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_pt.vocab_size]\n",
        "  end_token = [tokenizer_pt.vocab_size + 1]\n",
        "\n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input = [tokenizer_en.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input,\n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_en.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN-BV43FMBej"
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  sentence = tokenizer_pt.encode(sentence)\n",
        "\n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "\n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "    # plot the attention weights\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "\n",
        "    ax.set_xticks(range(len(sentence)+2))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "\n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'],\n",
        "        fontdict=fontdict, rotation=90)\n",
        "\n",
        "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result\n",
        "                        if i < tokenizer_en.vocab_size],\n",
        "                       fontdict=fontdict)\n",
        "\n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU2_yG_vBGza"
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "\n",
        "  predicted_sentence = tokenizer_en.decode([i for i in result\n",
        "                                            if i < tokenizer_en.vocab_size])\n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\n",
        "\n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsxrAlvFG8SZ"
      },
      "source": [
        "translate(\"este é um problema que temos que resolver.\")\n",
        "print (\"Real translation: this is a problem we have to solve .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EH5y_aqI4t1"
      },
      "source": [
        "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
        "print (\"Real translation: and my neighboring homes heard about this idea .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-hVCTSUMlkb"
      },
      "source": [
        "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
        "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1MxkSZvz0jX"
      },
      "source": [
        "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-kFyiOLH0xg"
      },
      "source": [
        "translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
        "print (\"Real translation: this is the first book i've ever done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqQ1fIsLwkGE"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.\n",
        "\n",
        "Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models. Futhermore, you can implement beam search to get better predictions."
      ]
    }
  ]
}