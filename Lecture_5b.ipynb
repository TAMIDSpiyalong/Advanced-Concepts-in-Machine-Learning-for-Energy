{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAMIDSpiyalong/Introduction-to-Machine-Learning-for-Energy/blob/main/Lecture_5b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RhhFbexSqjo"
      },
      "source": [
        "# Lab: YOLO for Construction Personal-Protective-Equipment (PPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvBhWDOpVKRL"
      },
      "source": [
        "To improve safety compliance in the construction industry, we have developed three innovative deep learning models based on the You-Only-Look-Once (YOLO) architecture, which is an extremely fast accurate object detection. These models enable real-time verification of PPE usage by construction workers from image and video data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrbpQYiL7bOQ"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w16OgEZ7iFQ"
      },
      "source": [
        "1. Build a YOLO model from scratch and study it structure\n",
        "2. Understand the output of FPN and anchor boxes\n",
        "3. Visualize the output and post-processing techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOkx0FqFzNlk"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrdpNzLHzShj"
      },
      "source": [
        "This dataset contains 784 images taken by Yalong Pi from construction sites in Zhuhai Gree Real Estate Company. The annotation is prepared by Yalong Pi and Nipun Nath. You might find them in the dataset. More details can be found in this paper https://www.sciencedirect.com/science/article/pii/S0926580519308325. This dataset is loaded in three different ways but we are going to look at approach 1 for the purpose of this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9riqZ4MosK8W",
        "outputId": "a77a86b9-32f1-4b6c-c804-9ec8c3890650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pictor-ppe.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of pictor-ppe.zip or\n",
            "        pictor-ppe.zip.zip, and cannot find pictor-ppe.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "! unzip -q pictor-ppe.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ad1uloSqjq"
      },
      "source": [
        "## Build YOLO from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqO3_5_YNzVg"
      },
      "source": [
        "First, let's import the necessary packages we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K6WD22O7Sqjq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D\n",
        "from tensorflow.keras.layers import LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import plot_model, model_to_dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJdW9MdlNzVh"
      },
      "source": [
        "The yolo_ConvBlock function we provided defines a convolutional block for a YOLO-like neural network. This block is a fundamental building block often used in YOLO models and similar architectures for object detection and image classification tasks. Let's break down the function and its components:\n",
        "\n",
        "'input_tensor': This is the input tensor to the convolutional block. It represents the feature map or image that you want to process.\n",
        "\n",
        "'num_filters': The number of filters (also known as output channels) in the convolutional layer. Each filter learns to detect a specific feature in the input data.\n",
        "\n",
        "'filter_size': The size of the convolutional filters. It is typically specified as a tuple (height, width).\n",
        "\n",
        "'strides': The stride of the convolution operation. It is a tuple (vertical_stride, horizontal_stride) that determines the step size of the filter as it moves across the input tensor. If strides is (2, 2), the padding is set to 'valid' to reduce the feature map size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q3Jiws3BTIVa"
      },
      "outputs": [],
      "source": [
        "def yolo_ConvBlock (input_tensor, num_filters, filter_size, strides = (1,1) ):\n",
        "    padding = 'valid' if strides == (2,2) else 'same'\n",
        "\n",
        "    '''Layers'''\n",
        "    x = Conv2D( num_filters, filter_size, strides, padding, use_bias=False, kernel_regularizer=l2(5e-4) ) (input_tensor)\n",
        "    x = BatchNormalization() (x)\n",
        "    x = LeakyReLU(alpha=0.1) (x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO4rD0LB9Tfi"
      },
      "source": [
        "YOLO v3 also use the ResNet identify block. The yolo_ResidualBlocks function we provided defines a series of residual blocks, a type of architecture commonly used in deep convolutional neural networks to improve gradient flow and enable the training of very deep networks. Residual blocks help mitigate the vanishing gradient problem and facilitate training of deeper models. Let's break down the function and its components:\n",
        "\n",
        "'input_tensor': This is the input tensor to the residual blocks. It represents the feature map or image that you want to process.\n",
        "\n",
        "'num_filters': The number of filters (output channels) used in the convolutional layers within the residual blocks.\n",
        "\n",
        "'num_blocks': The number of residual blocks to create in the architecture.\n",
        "\n",
        "The yolo_ResidualBlocks function constructs a stack of residual blocks, which can be added to a neural network architecture to create a deeper and more expressive model. Residual blocks are a fundamental concept in modern deep learning architectures, enhancing the training and performance of deep neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3SBzzq8fTUN0"
      },
      "outputs": [],
      "source": [
        "def yolo_ResidualBlocks (input_tensor, num_filters, num_blocks ):\n",
        "\n",
        "    '''Layers'''\n",
        "    x = ZeroPadding2D( ((1,0),(1,0)) ) (input_tensor) # left & top padding\n",
        "    x = yolo_ConvBlock ( x, num_filters, filter_size=(3,3), strides = (2,2) )\n",
        "    for _ in range( num_blocks ):\n",
        "        y = yolo_ConvBlock ( x, num_filters//2, filter_size=(1,1), strides = (1,1) )\n",
        "        y = yolo_ConvBlock ( y, num_filters   , filter_size=(3,3), strides = (1,1) )\n",
        "        x = Add() ([x, y])\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJmQu7CC93RR"
      },
      "source": [
        "This OutputBlock provides the features and also passes down and info to the next layer. This block is responsible for producing the final predictions of bounding boxes and object classes for the YOLO object detection model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tE9AuTbCTkfF"
      },
      "outputs": [],
      "source": [
        "def yolo_OutputBlock (x, num_filters, out_filters ):\n",
        "\n",
        "    '''Layers'''\n",
        "    x = yolo_ConvBlock ( x, 1*num_filters, filter_size=(1,1), strides = (1,1) )\n",
        "    x = yolo_ConvBlock ( x, 2*num_filters, filter_size=(3,3), strides = (1,1) )\n",
        "    x = yolo_ConvBlock ( x, 1*num_filters, filter_size=(1,1), strides = (1,1) )\n",
        "    x = yolo_ConvBlock ( x, 2*num_filters, filter_size=(3,3), strides = (1,1) )\n",
        "    x = yolo_ConvBlock ( x, 1*num_filters, filter_size=(1,1), strides = (1,1) )\n",
        "\n",
        "    y = yolo_ConvBlock ( x, 2*num_filters, filter_size=(3,3), strides = (1,1) )\n",
        "    y = Conv2D ( filters=out_filters, kernel_size=(1,1), strides=(1,1),\n",
        "                padding='same', use_bias=True, kernel_regularizer=l2(5e-4) )(y)\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-dt1l5I-P96"
      },
      "source": [
        "We put everything together and the output will be y1, y2, and y3. Each of them has the num_out_filters equal the numer of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DE5XqwKdTry3"
      },
      "outputs": [],
      "source": [
        "def yolo_body (input_tensor, num_out_filters):\n",
        "    '''\n",
        "    Input:\n",
        "        input_tensor   = Input( shape=( *input_shape, 3 ) )\n",
        "        num_out_filter = ( num_anchors // 3 ) * ( 5 + num_classes )\n",
        "    Output:\n",
        "        complete YOLO-v3 model\n",
        "    '''\n",
        "\n",
        "    # 1st Conv block\n",
        "    x = yolo_ConvBlock( input_tensor, num_filters=32, filter_size=(3,3), strides=(1,1) )\n",
        "\n",
        "    # 5 Resblocks\n",
        "    x = yolo_ResidualBlocks ( x, num_filters=  64, num_blocks=1 )\n",
        "    x = yolo_ResidualBlocks ( x, num_filters= 128, num_blocks=2 )\n",
        "    x = yolo_ResidualBlocks ( x, num_filters= 256, num_blocks=8 )\n",
        "    x = yolo_ResidualBlocks ( x, num_filters= 512, num_blocks=8 )\n",
        "    x = yolo_ResidualBlocks ( x, num_filters=1024, num_blocks=4 )\n",
        "\n",
        "    darknet = Model( input_tensor, x ) # will use it just in a moment\n",
        "\n",
        "    # 1st output block\n",
        "    x, y1 = yolo_OutputBlock( x, num_filters= 512, out_filters=num_out_filters )\n",
        "\n",
        "    # 2nd output block\n",
        "    x = yolo_ConvBlock( x, num_filters=256, filter_size=(1,1), strides=(1,1) )\n",
        "    x = UpSampling2D(2) (x)\n",
        "    x = Concatenate() ( [x, darknet.layers[152].output] )\n",
        "    x, y2 = yolo_OutputBlock( x, num_filters= 256, out_filters=num_out_filters )\n",
        "\n",
        "    # 3rd output block\n",
        "    x = yolo_ConvBlock( x, num_filters=128, filter_size=(1,1), strides=(1,1) )\n",
        "    x = UpSampling2D(2) (x)\n",
        "    x = Concatenate() ( [x, darknet.layers[92].output] )\n",
        "    x, y3 = yolo_OutputBlock( x, num_filters= 128, out_filters=num_out_filters )\n",
        "\n",
        "    # Final model\n",
        "    model = Model( input_tensor, [y1, y2, y3] )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji5hHfwwSqjr"
      },
      "source": [
        "### Configuaration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J67L5J7JPRp"
      },
      "source": [
        "In this lab, we modify the YOLO model to detect three classes of objects of interests: hardhat, vest, and worker. The anchor boxes are learned from the image annotation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xOoysXzhSqjr"
      },
      "outputs": [],
      "source": [
        "class_names = ['Hardhat', 'Vest', 'Worker']\n",
        "\n",
        "anchor_boxes = np.array(\n",
        "        [\n",
        "        np.array([[ 76,  59], [ 84, 136], [188, 225]]) /32, # output-1 anchor boxes\n",
        "        np.array([[ 25,  15], [ 46,  29], [ 27,  56]]) /16, # output-2 anchor boxes\n",
        "        np.array([[ 5,    3], [ 10,   8], [ 12,  26]]) /8   # output-3 anchor boxes\n",
        "        ],\n",
        "        dtype='float64'\n",
        "    )\n",
        "\n",
        "input_shape  = (416, 416)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VHcSITw7Sqjr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqghoJyQSqjs"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9QWURxUJe8r"
      },
      "source": [
        "We build the model and load the pretrained weights. Keras has a nice tool to see the structure. Try to find the y1, y2, and y3 output in the visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m4AepQfiSqjs"
      },
      "outputs": [],
      "source": [
        "K.clear_session() # clear memory\n",
        "\n",
        "# number of classes and number of anchors\n",
        "num_classes = 3\n",
        "num_anchors = anchor_boxes.shape[0] * anchor_boxes.shape[1]\n",
        "\n",
        "# input and output\n",
        "input_tensor = Input( shape=(input_shape[0], input_shape[1], 3) ) # input\n",
        "num_out_filters = ( num_anchors//3 ) * ( 5 + num_classes )        # output\n",
        "\n",
        "# build the model\n",
        "model = yolo_body(input_tensor, num_out_filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcjDHowaJ9Ln"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rkMfP6ZSqjs"
      },
      "source": [
        "### Load weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUnDJDB_Sqjs"
      },
      "outputs": [],
      "source": [
        "weight_path = 'pictor-ppe-v302-a1-yolo-v3-weights.h5'\n",
        "\n",
        "model.load_weights( weight_path )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7QykXDASqjt"
      },
      "source": [
        "## Pre-processing Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZFZ7Ww-2r7G"
      },
      "source": [
        "The purpose of the letterbox function is to resize an input image while preserving its aspect ratio. The technique achieves this by adding padding (usually in the form of black bars) to the image so that it fits into a fixed-size canvas without distortion. The resulting image has the same aspect ratio as the original, but it is centered within the fixed-size canvas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGn4vNBASqjt"
      },
      "outputs": [],
      "source": [
        "def letterbox_image(image, size):\n",
        "    '''\n",
        "    Resize image with unchanged aspect ratio using padding\n",
        "    '''\n",
        "\n",
        "    # original image size\n",
        "    ih, iw, ic = image.shape\n",
        "\n",
        "    # given size\n",
        "    h, w = size\n",
        "\n",
        "    # scale and new size of the image\n",
        "    scale = min(w/iw, h/ih)\n",
        "    nw = int(iw*scale)\n",
        "    nh = int(ih*scale)\n",
        "\n",
        "    # placeholder letter box\n",
        "    new_image = np.zeros((h, w, ic), dtype='uint8') + 128\n",
        "\n",
        "    # top-left corner\n",
        "    top, left = (h - nh)//2, (w - nw)//2\n",
        "\n",
        "    # paste the scaled image in the placeholder anchoring at the top-left corner\n",
        "    new_image[top:top+nh, left:left+nw, :] = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    return new_image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "act_img = cv2.imread('Images/image_from_china(2685).jpg')\n",
        "image_shape = act_img.shape[:-1]\n",
        "img = letterbox_image(act_img, (416,416))/255.\n",
        "img = np.expand_dims(img, 0)\n",
        "\n",
        "'''Show the image'''\n",
        "plt.imshow( act_img[:,:,::-1] )\n",
        "# plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuqtXoVoSqjt"
      },
      "outputs": [],
      "source": [
        "'''Show the letterbox image'''\n",
        "plt.imshow( letterbox_image(act_img, (416,416))[:,:,::-1] )\n",
        "# plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA3cDQY9Sqjt"
      },
      "source": [
        "## Get prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSc1v3PtSqjt"
      },
      "outputs": [],
      "source": [
        "yolo_pred = model.predict(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANQgKdjqSqjt"
      },
      "source": [
        "## Understand the Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCs_6FQiSqjt"
      },
      "source": [
        "### Analyze the predicted values\n",
        "\n",
        "- The predictions in each output layer is a 4-dimensional tensor with a shape of:  \n",
        "( batch size, # of grids in x-dir, # of grids in y-dir, # anchor boxes in this layer * ( 5 + # of classes ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meDQM2_YSqju"
      },
      "outputs": [],
      "source": [
        "for i in range( len(yolo_pred) ):\n",
        "    print('Output-y{} shape: '.format(i+1), yolo_pred[i].shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akAe8aVvSqju"
      },
      "source": [
        "- Let's reshape it to a 5-dimensional tensor with a shape of:  \n",
        "( batch size, # of grids in x-dir, # of grids in y-dir, # of anchor boxes in this layer,  ( 5 + # of classes ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuTODWQpSqju"
      },
      "outputs": [],
      "source": [
        "pred = []\n",
        "for i in range( len(yolo_pred) ):\n",
        "    pred.append( yolo_pred[i].reshape( [-1, *yolo_pred[i].shape[1:3], 3, (5+num_classes) ] ) )\n",
        "\n",
        "for i in range( len(yolo_pred) ):\n",
        "    print('Output y{} shape: '.format(i+1), pred[i].shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_jEw_EfSqju"
      },
      "source": [
        "- Let's find which grid in the $1^{st}$ output layer has the highest probability of presence of an object (a.k.a. objectness score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyeruR-dSqju"
      },
      "outputs": [],
      "source": [
        "yolo_pred[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U9DWFQ-Sqju"
      },
      "outputs": [],
      "source": [
        "grid_shape = pred[0].shape[1:3] # shape of the grids\n",
        "\n",
        "obj_scores = pred[0] [0, :, :, :, 4] # objectness scores\n",
        "grid_i, grid_j, abox_k = np.unravel_index( np.argmax( obj_scores, axis=None ), obj_scores.shape )\n",
        "\n",
        "abox = anchor_boxes[0][abox_k] # corresponding anchor box\n",
        "\n",
        "print( \"Grid size: ({}, {})\".format( *grid_shape ) )\n",
        "print( \"Most likely there is an object in grid ({}, {}) with anchor box of size {}\".format( grid_i, grid_j, abox ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Ytg_aILQxL"
      },
      "source": [
        "We can see the object score  for each anchor box. At this point, all the numbers are raw values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZMic0tYSqjv"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(obj_scores[:,:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biNvpCYuSqjv"
      },
      "source": [
        "- Predicted raw values in one grid for one bounding box: ( $t_x$, $t_y$, $t_w$, $t_h$, $t_{p_0}$, $t_{p_1}$, ..., $t_{p_n}$ )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYbADJEkSqjv"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions( precision=4 )\n",
        "\n",
        "print( 'box parameters (x, y) : {}'.format( pred [0] [0, grid_i, grid_j, abox_k, 0:2] ) )\n",
        "print( 'box parameters (w, h) : {}'.format( pred [0] [0, grid_i, grid_j, abox_k, 2:4] ) )\n",
        "print( 'objectness score      : {}'.format( pred [0] [0, grid_i, grid_j, abox_k, 4:5] ) )\n",
        "print( 'class probabilities   : {}'.format( pred [0] [0, grid_i, grid_j, abox_k, 5:5+num_classes] ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx-Ib9WpSqjw"
      },
      "source": [
        "- Transformed values in one grid for one bounding box: ( $x$, $y$, $w$, $h$, $p_0$, $p_1$, ..., $p_n$ )  \n",
        "- Coordinates of the center of the box: x, y = sigmoid($t_x$), sigmoid($t_y$)  \n",
        "- Width and height of the box: w, h = exp($t_w$), exp($t_h$)  \n",
        "- Objectness score: $p_0$ =  sigmoid($t_{p_0}$)  \n",
        "- Class probabilities: $p_i$ =  sigmoid($t_{p_i}$) for $i$ = 1, ..., n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEQWVIAlSqjw"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions( precision=2, suppress=True )\n",
        "\n",
        "# define the sigmoid function\n",
        "sigmoid = lambda x : 1 / ( 1 + np.exp(-x) )\n",
        "\n",
        "bx_xy     = sigmoid( pred [0] [0, grid_i, grid_j, abox_k, 0:2] ) # box's center (x and y)\n",
        "bx_wh     = np.exp ( pred [0] [0, grid_i, grid_j, abox_k, 2:4] ) # box's width and height\n",
        "obj_score = sigmoid( pred [0] [0, grid_i, grid_j, abox_k, 4:5] ) # objectness score\n",
        "cls_probs = sigmoid( pred [0] [0, grid_i, grid_j, abox_k, 5:5+num_classes] ) # conditional class probabilities\n",
        "\n",
        "print( 'box parameters (x, y) : {}'.format( bx_xy ) )\n",
        "print( 'box parameters (w, h) : {}'.format( bx_wh ) )\n",
        "print( 'objectness score      : {}'.format( obj_score ) )\n",
        "print( 'class probabilities   : {}'.format( cls_probs ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBT_m4HvSqjw"
      },
      "source": [
        "### Interpretation of the box parameters (center)\n",
        "- each grid cell's width and height is assumed to be 1.\n",
        "- origin of the coordinate system is top-left corner.\n",
        "- let the index of the cell is ($i$, $j$), i.e., the cell is at $i^{th}$ row from top and at $j^{th}$ column from left.\n",
        "- box parameter (x, y) indicates the offset of the box's center from the grid cell's upper-left corner.\n",
        "- therefore, the box's center measured from origin (top-left) would be: ($j$ + $x$, $i$ + $y$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ScQN8obSqjw"
      },
      "outputs": [],
      "source": [
        "# grid's index\n",
        "grid_ji = np.array([grid_j, grid_i], dtype=float)\n",
        "\n",
        "# center of the box, measured from origin (top-left)\n",
        "bx_xy_origin = bx_xy + grid_ji\n",
        "\n",
        "print( 'center of the box in the {0}x{1} grid : ({2:.2f}, {3:.2f})'.format( *pred[0].shape[1:3], *bx_xy_origin ) )\n",
        "\n",
        "'''Plot the center of the box in the grid'''\n",
        "plt.figure()\n",
        "plt.scatter(*bx_xy_origin, marker='x')\n",
        "plt.xticks(np.arange(obj_scores.shape[1]))\n",
        "plt.yticks(np.arange(obj_scores.shape[0]))\n",
        "plt.gca().xaxis.tick_top()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.gca().set_aspect('equal')\n",
        "ax = plt.gca()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4RsduJnSqjw"
      },
      "source": [
        "### Interpretation of the box parameters (width and height)\n",
        "- Box parameter (w,h) indicates the how much to scale the anchor boxes\n",
        "- Given, the width and height of anchor boxes be $w_a$ and $h_a$, respectively. The width and height of the detected bounding would be: $w_a*w$ and $h_a*h$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW30M4smSqjw"
      },
      "outputs": [],
      "source": [
        "# bounding box's actual width and height\n",
        "# (considering each grid cell's size is 1 unit)\n",
        "bx_wh_origin = bx_wh * abox\n",
        "\n",
        "'''Plot the bounding box in the grid'''\n",
        "plt.figure()\n",
        "plt.scatter(*bx_xy_origin, marker='x')\n",
        "bx = mpl.patches.Rectangle(bx_xy_origin-bx_wh_origin/2, *bx_wh_origin, fill=False, lw=2)\n",
        "plt.gca().add_patch(bx)\n",
        "plt.xticks(np.arange(obj_scores.shape[1]))\n",
        "plt.yticks(np.arange(obj_scores.shape[0]))\n",
        "plt.gca().xaxis.tick_top()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.gca().set_aspect('equal')\n",
        "ax = plt.gca()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2CvfzMkSqjw"
      },
      "source": [
        "### Interpretation of the objectness score and conditional class probabilities\n",
        "- Objectness score $p_0$ represents the probability of the presence of an object.\n",
        "- Given an object present, $p_i$ represents the conditional probability of object belong to class $i$ (for $i$ = 0.9, ..., n).\n",
        "- In other words, $p_0$ = P( object ), and $p_i$ = P( class = $i$ | object )\n",
        "- Therefore,  \n",
        "P( class = $i$ )  \n",
        "= $$\\frac{P( class = i | object ) \\times P( object )}{\\sum_i P( class = i | object ) \\times P( object )}$$\n",
        "= $$\\frac{p_0 \\times p_i}{\\sum_i (p_0 \\times p_i)}$$\n",
        "- Final class, c = $$\\text{argmax}_i \\frac{p_0 \\times p_i}{\\sum_i (p_0 \\times p_i)}$$ = $\\text{argmax}_i (p_0 \\times p_i)$\n",
        "- $p_0 \\times p_i$ is called the confidence score of the box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KYgZa0SSqjx"
      },
      "outputs": [],
      "source": [
        "cls_id     = np.argmax( obj_score * cls_probs )\n",
        "conf_score = obj_score * cls_probs[cls_id]\n",
        "\n",
        "print( 'The predicted class of object inside the box is {}'.format( class_names[cls_id] ) )\n",
        "print( 'The corresponding confidence score is {}'.format( conf_score[0] ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59dvOZ2VSqjx"
      },
      "source": [
        "### Rescale the boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y39TbsCbSqjx"
      },
      "outputs": [],
      "source": [
        "input_shape = np.array(input_shape).astype('float')\n",
        "image_shape = np.array(image_shape).astype('float')\n",
        "grid_shape  = np.array(grid_shape).astype('float')\n",
        "\n",
        "# the resized new shape of the image\n",
        "resized_shape = np.round( image_shape * np.min( input_shape / image_shape ) )\n",
        "\n",
        "bx_offset = (image_shape.max() - image_shape) / 2.\n",
        "bx_scale  = input_shape * image_shape / resized_shape / grid_shape\n",
        "\n",
        "bx_xy_on_img = bx_xy_origin * bx_scale[::-1] - bx_offset[::-1]\n",
        "bx_wh_on_img = bx_wh_origin * bx_scale[::-1]\n",
        "\n",
        "'''Plot the bounding box in the grid'''\n",
        "plt.figure()\n",
        "plt.scatter( *bx_xy_on_img, marker='x' )\n",
        "bx = mpl.patches.Rectangle( bx_xy_on_img-bx_wh_on_img/2, *bx_wh_on_img, fill=False, lw=2 )\n",
        "plt.gca().add_patch(bx)\n",
        "plt.xlim([0, image_shape[1]])\n",
        "plt.ylim([0, image_shape[0]])\n",
        "plt.gca().xaxis.tick_top()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eujzEE1_Sqj6"
      },
      "source": [
        "### Show on image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHwjhq4uSqj6"
      },
      "outputs": [],
      "source": [
        "### Calculate top-left and bottom-right corner of the box\n",
        "t,l = np.array( bx_xy_on_img - bx_wh_on_img/2 ).astype('int')\n",
        "b,r = np.array( bx_xy_on_img + bx_wh_on_img/2 ).astype('int')\n",
        "\n",
        "'''Show the image with the box'''\n",
        "detected_img = cv2.rectangle(act_img, (t,l), (b,r), (0,0,255), 4)\n",
        "plt.imshow( detected_img[:,:,::-1] )\n",
        "# plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UfQ5PlRSqj6"
      },
      "source": [
        "## Get All Boxes\n",
        "Now let's get all the boxes at the $1^{st}$ output layers.\n",
        "\n",
        "We will use tensors to calculate properties of the boxes altogether."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbLPQz8nSqj6"
      },
      "outputs": [],
      "source": [
        "'''Inputs'''\n",
        "output  = yolo_pred[0]\n",
        "anchors = anchor_boxes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sgiuQHaSqj6"
      },
      "outputs": [],
      "source": [
        "'''Some preprocessing'''\n",
        "grid_h, grid_w = output.shape[1:3] # grid_height, grid_width @ output layer\n",
        "\n",
        "# reshape to [batch_size, grid_height, grid_width, num_anchors, box_params]\n",
        "output = tf.reshape( output, [ -1, grid_h, grid_w, len(anchors), num_classes+5 ] )\n",
        "\n",
        "# create a tensor for the anchor boxes\n",
        "anchors_tensor = tf.constant(anchors, dtype=output.dtype)\n",
        "anchors_tensor = tf.reshape( anchors_tensor, [1, 1, 1, len(anchors), 2] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xj5GI2gSqj7"
      },
      "source": [
        "### **Geometric properties**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oe6aL_TSqj7"
      },
      "outputs": [],
      "source": [
        "'''Grid positions'''\n",
        "grid_i = tf.reshape( np.arange(grid_h), [-1, 1, 1, 1] )\n",
        "grid_i = tf.tile( grid_i, [1, grid_w, 1, 1] )\n",
        "\n",
        "grid_j = tf.reshape( np.arange(grid_w), [1, -1, 1, 1] )\n",
        "grid_j = tf.tile( grid_j, [grid_h, 1, 1, 1] )\n",
        "\n",
        "grid_ji = tf.concat( [grid_j, grid_i], axis=-1 )\n",
        "grid_ji = tf.cast( grid_ji, output.dtype )\n",
        "\n",
        "'''Box centers'''\n",
        "box_xy  = output[..., 0:2]\n",
        "box_xy  = tf.sigmoid( box_xy ) + grid_ji\n",
        "\n",
        "'''Box sizes'''\n",
        "box_wh  = output[..., 2:4]\n",
        "box_wh  = tf.exp( box_wh ) * anchors_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQVHzsjySqj7"
      },
      "source": [
        "Let's see how do `grid_j`, `grid_i` and `box_xy` look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RG4emHpSqj7"
      },
      "outputs": [],
      "source": [
        "print('grid_j is the x-coordinates of the grid cells:\\n')\n",
        "print( grid_j[:,:,0,0].numpy().astype(int) )\n",
        "\n",
        "print('\\ngrid_i is the y-coordinates of the grid cells:\\n')\n",
        "print( grid_i[:,:,0,0].numpy().astype(int) )\n",
        "\n",
        "np.set_printoptions(precision =2, suppress =True, linewidth=120)\n",
        "\n",
        "print(\"\\nbox_xy[...,0] is the x-coordinates of the boxes' centers:\\n\")\n",
        "print( box_xy[:,:,:,0,0].numpy().astype(float) )\n",
        "\n",
        "print(\"\\nbox_xy[...,1] is the y-coordinates of the boxes' centers:\\n\")\n",
        "print( box_xy[:,:,:,0,1].numpy().astype(float) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV8R8LP9Sqj7"
      },
      "source": [
        "### **Probabilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIyTK7V6Sqj7"
      },
      "outputs": [],
      "source": [
        "'''Class and confidence scores'''\n",
        "# class probabilities = objectness score * conditional class probabilities\n",
        "classs_probs = tf.sigmoid( output[..., 4:5] ) * tf.sigmoid( output[..., 5:] )\n",
        "\n",
        "# final classes\n",
        "box_cl = tf.argmax( classs_probs, axis=-1 )\n",
        "\n",
        "# confidence scores\n",
        "box_sc = tf.reduce_max( classs_probs, axis=-1 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjFgRW3HSqj8"
      },
      "source": [
        "`box_sc` indicates which cell might contain an object of interest. Let's see how does it look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fgFmWlTXhob"
      },
      "outputs": [],
      "source": [
        "np.max(box_sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6U8ERRTWp1R"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(box_cl[0][:,:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOzRzR7POu4V"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(box_sc[0][:,:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2Xr9f-7WvPc"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(box_cl[0][:,:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ZLyaV0L_1b"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(box_sc[0][:,:,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDxdKkFqWyqE"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(box_cl[0][:,:,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSsgm7-ASqj8"
      },
      "source": [
        "### **Rescaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4WYUdYyPcsP"
      },
      "source": [
        "Rescaling is very important in customized models. If not properly rescaled back, the output does not make sense. Many pre-trained models does rescaling for you so you do not have to worry about it. In YOLO, rescaling from anchor box offset back to the original size is even more complicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7Az5U6HSqj8"
      },
      "outputs": [],
      "source": [
        "'''Get all the shapes'''\n",
        "image_shape = tf.cast( image_shape,       output.dtype)  # actual image's shape\n",
        "input_shape = tf.cast( input_shape,       output.dtype)  # yolo input image's shape\n",
        "grids_shape = tf.cast( output.shape[1:3], output.dtype ) # grid_height, grid_width @ output layer\n",
        "\n",
        "# the resized new shape of the image\n",
        "# i.e. shape after resizing the image to fit 'input shape' without changing the aspect ratio\n",
        "resized_shape = tf.round( image_shape * np.min( input_shape / image_shape ) )\n",
        "\n",
        "'''Scaling factors'''\n",
        "# to scale the boxes from grid's unit to actual image's pixel unit\n",
        "box_scaling = input_shape * image_shape / resized_shape / grids_shape\n",
        "# to offset the boxes\n",
        "box_offsets = (tf.reduce_max(image_shape) - image_shape) / 2.\n",
        "\n",
        "'''Scale to actual pixel unit'''\n",
        "box_xy_on_img  = box_xy * box_scaling - box_offsets[::-1]\n",
        "box_wh_on_img  = box_wh * box_scaling\n",
        "\n",
        "'''Check boundaries'''\n",
        "# calculate top-left and bottom-right corner of the boxex\n",
        "box_tl = box_xy_on_img - box_wh_on_img/2\n",
        "box_br = box_xy_on_img + box_wh_on_img/2\n",
        "\n",
        "# top-left corner cannot be negative\n",
        "box_tl = tf.maximum(0, box_tl)\n",
        "box_tl = tf.cast( tf.round( box_tl ), dtype='int16' )\n",
        "# bottom-right corner cannot be more than actual image size\n",
        "box_br = tf.minimum(box_br, image_shape[::-1])\n",
        "box_br = tf.cast( tf.round( box_br ), dtype='int16' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqa63ce8Sqj8"
      },
      "source": [
        "### **Reorganizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_AZLT-CSqj8"
      },
      "outputs": [],
      "source": [
        "boxes  = tf.reshape( tf.concat( [box_tl, box_br], axis=-1 ), [-1,4] )\n",
        "labels = tf.reshape( box_cl, [-1] )\n",
        "scores = tf.reshape( box_sc, [-1] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSd5npg4RZ5a"
      },
      "outputs": [],
      "source": [
        "boxes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd-1On1zbmVn"
      },
      "outputs": [],
      "source": [
        "best_box = boxes[np.argmax(scores)].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L2jtEa-RdaL"
      },
      "outputs": [],
      "source": [
        "best_box[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gC3v4p4Sqj9"
      },
      "source": [
        "Show all the positve scored boxes on image and you will see a lot of boxes. Out of all the boxes, there could be more than one high confidence prediction. We visualize the best one to show the results. In reality, we do not know in advance how many objects are there so we use the threshold to filter the low quality detections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2csPg3BSqj9"
      },
      "outputs": [],
      "source": [
        "'''Get the boxes with some non-zero confidence'''\n",
        "idc = scores > 0 # indices\n",
        "num_boxes = tf.math.count_nonzero(idc) # number of boxes\n",
        "\n",
        "'''Put the rectangular boxes on the image'''\n",
        "act_img = cv2.imread('Images/image_from_china(2685).jpg')\n",
        "\n",
        "for (t,l,b,r), class_id, conf in zip( boxes [idc].numpy(),\n",
        "                                      labels[idc].numpy(),\n",
        "                                      scores[idc].numpy()\n",
        "                                    ):\n",
        "    detected_img = cv2.rectangle(act_img, (t,l), (b,r), (0,0,255), 1)\n",
        "\n",
        "cv2.rectangle(act_img, (best_box[0],best_box[1]), (best_box[2],best_box[3]), (255,255,255), 5)\n",
        "\n",
        "plt.imshow( detected_img[:,:,::-1] )\n",
        "# plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print('There are a total of {} boxes in y1! The best box is marked white with confidence of {}.'.format(num_boxes,np.max(scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVaeZ_5vSqj9"
      },
      "source": [
        "### **Non-max-suppression**\n",
        "Most of the detected boxes are flase positive, reduntant, and/or duplicate of other boxes. To get rid of these boxes and to keep only the most confident boxes, we will use *non-max-suppression* (NMS) algorithm.\n",
        "\n",
        "\n",
        "For each class:\n",
        "1. First, get rid of all the boxes with confidence lower than a certain threshold (e.g. 0.3, or `score_threshold=0.3`).\n",
        "2. a. Select each box in descending order of confidence level.\\\n",
        "b. Remove boxes that have high overlap (e.g. intersection-over-union, IoU > 0.45, or `iou_threshold=0.45`) with previously selected boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tfH2EvcSqj9"
      },
      "source": [
        "## Make a Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJhmvKR7fsWI"
      },
      "source": [
        "In example above, the y1 output does not have good (high confidence) prediction. Let's create a function that goes through y1, y2, and y3 with NMS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfLgYiL2Sqj-"
      },
      "outputs": [],
      "source": [
        "def detection(\n",
        "    prediction,\n",
        "    anchor_boxes,\n",
        "    num_classes,\n",
        "    image_shape,\n",
        "    input_shape,\n",
        "    max_boxes = 20,\n",
        "    score_threshold=0.3,\n",
        "    iou_threshold=0.45,\n",
        "    classes_can_overlap=True,\n",
        "):\n",
        "    '''\n",
        "    INPUT:\n",
        "    OUTPUT:\n",
        "    '''\n",
        "\n",
        "    all_boxes  = []\n",
        "\n",
        "    '''@ Each output layer'''\n",
        "    for output, anchors in zip( prediction, anchor_boxes ):\n",
        "\n",
        "        '''Preprocessing'''\n",
        "        '''-------------'''\n",
        "        # shapes\n",
        "        batch_size     = output.shape[0]\n",
        "        grid_h, grid_w = output.shape[1:3]\n",
        "\n",
        "        # reshape to [batch_size, grid_height, grid_width, num_anchors, box_params]\n",
        "        output = tf.reshape( output, [ -1, grid_h, grid_w, len(anchors), num_classes+5 ] )\n",
        "\n",
        "        # create a tensor for the anchor boxes\n",
        "        anchors_tensor = tf.constant(anchors, dtype=output.dtype)\n",
        "\n",
        "        '''Scaling factors'''\n",
        "        '''---------------'''\n",
        "        image_shape_tensor = tf.cast( image_shape,       output.dtype ) # actual image's shape\n",
        "        grids_shape_tensor = tf.cast( output.shape[1:3], output.dtype ) # grid_height, grid_width @ output layer\n",
        "        input_shape_tensor = tf.cast( input_shape,       output.dtype )  # yolo input image's shape\n",
        "\n",
        "        # reshape\n",
        "        image_shape_tensor = tf.reshape( image_shape_tensor, [-1, 1, 1, 1, 2] )\n",
        "        grids_shape_tensor = tf.reshape( grids_shape_tensor, [-1, 1, 1, 1, 2] )\n",
        "        input_shape_tensor = tf.reshape( input_shape_tensor, [-1, 1, 1, 1, 2] )\n",
        "\n",
        "        ### Scaling factors\n",
        "        sized_shape_tensor = tf.round( image_shape_tensor * tf.reshape( tf.reduce_min( input_shape_tensor / image_shape_tensor, axis=-1 ), [-1,1,1,1,1] ) )\n",
        "        # to scale the boxes from grid's unit to actual image's pixel unit\n",
        "        box_scaling = input_shape_tensor * image_shape_tensor / sized_shape_tensor / grids_shape_tensor\n",
        "        # to offset the boxes\n",
        "        box_offsets = (tf.expand_dims(tf.reduce_max(image_shape_tensor, axis=-1), axis=-1) - image_shape_tensor) / 2.\n",
        "\n",
        "        '''Box geometric properties'''\n",
        "        '''------------------------'''\n",
        "        grid_h, grid_w = output.shape[1:3] # grid_height, grid_width @ output layer\n",
        "\n",
        "        grid_i = tf.reshape( np.arange(grid_h), [-1, 1, 1, 1] )\n",
        "        grid_i = tf.tile( grid_i, [1, grid_w, 1, 1] )\n",
        "\n",
        "        grid_j = tf.reshape( np.arange(grid_w), [1, -1, 1, 1] )\n",
        "        grid_j = tf.tile( grid_j, [grid_h, 1, 1, 1] )\n",
        "\n",
        "        grid_ji = tf.concat( [grid_j, grid_i], axis=-1 )\n",
        "        grid_ji = tf.cast( grid_ji, output.dtype )\n",
        "\n",
        "        # Box centers\n",
        "        box_xy  = output[..., 0:2]\n",
        "        box_xy  = tf.sigmoid( box_xy ) + grid_ji\n",
        "\n",
        "        # Box sizes\n",
        "        box_wh  = output[..., 2:4]\n",
        "        box_wh  = tf.exp( box_wh ) * anchors_tensor\n",
        "\n",
        "        # scale to actual pixel unit\n",
        "        box_xy  = box_xy * box_scaling - box_offsets[...,::-1]\n",
        "        box_wh  = box_wh * box_scaling\n",
        "\n",
        "        # calculate top-left corner (x1, y1) and bottom-right corner (x2, y2) of the boxex\n",
        "        box_x1_y1 = box_xy - box_wh / 2\n",
        "        box_x2_y2 = box_xy + box_wh / 2\n",
        "\n",
        "        # top-left corner cannot be negative\n",
        "        box_x1_y1 = tf.maximum(0, box_x1_y1)\n",
        "        # bottom-right corner cannot be more than actual image size\n",
        "        box_x2_y2 = tf.minimum(box_x2_y2, image_shape_tensor[..., ::-1])\n",
        "\n",
        "        '''Box labels and confidences'''\n",
        "        '''--------------------------'''\n",
        "        # class probabilities = objectness score * conditional class probabilities\n",
        "        if classes_can_overlap:\n",
        "            # use sigmoid for the conditional class probabilities\n",
        "            classs_probs = tf.sigmoid( output[..., 4:5] ) * tf.sigmoid( output[..., 5:] )\n",
        "        else:\n",
        "            # use softmax for the conditional class probabilities\n",
        "            classs_probs = tf.sigmoid( output[..., 4:5] ) * tf.nn.softmax( output[..., 5:] )\n",
        "\n",
        "        box_cl = tf.argmax( classs_probs, axis=-1 )     # final classes\n",
        "        box_sc = tf.reduce_max( classs_probs, axis=-1 ) # confidence scores\n",
        "\n",
        "        '''Organize'''\n",
        "        '''--------'''\n",
        "        # take care of dtype and dimensions\n",
        "        box_cl = tf.cast( box_cl, output.dtype )\n",
        "        box_cl = tf.expand_dims(box_cl, axis=-1)\n",
        "        box_sc = tf.expand_dims(box_sc, axis=-1)\n",
        "\n",
        "        # store all information as: [ left(x1), top(y1), right(x2), bottom(y2),  confidence, label ]\n",
        "        boxes  = tf.reshape( tf.concat( [ box_x1_y1, box_x2_y2, box_sc, box_cl ], axis=-1 ),\n",
        "                              [batch_size, -1, 6] )\n",
        "\n",
        "        all_boxes. append( boxes  )\n",
        "\n",
        "    # Merge across all output layers\n",
        "    all_boxes  = tf.concat( all_boxes,  axis=1 )\n",
        "\n",
        "    # To store all the final results of all images in the batch\n",
        "    all_final_boxes = []\n",
        "\n",
        "    '''For each image in the batch'''\n",
        "    for _boxes_ in all_boxes:\n",
        "\n",
        "        if classes_can_overlap:\n",
        "            '''Perform NMS for each class individually'''\n",
        "\n",
        "            # to stote the final results of this image\n",
        "            final_boxes = []\n",
        "\n",
        "            for class_id in range(num_classes):\n",
        "\n",
        "                # Get the boxes and scores for this class\n",
        "                class_boxes  = _boxes_[ _boxes_[...,-1] == class_id ]\n",
        "\n",
        "                '''Non-max-suppression'''\n",
        "                selected_idc = tf.image.non_max_suppression(\n",
        "                    class_boxes[...,:4], # boxes' (y1,x1,y2,x2)\n",
        "                    class_boxes[...,-2], # boxes' scores\n",
        "                    max_output_size = max_boxes,\n",
        "                    iou_threshold = iou_threshold,\n",
        "                    score_threshold = score_threshold\n",
        "                )\n",
        "\n",
        "                # boxes selected by nms\n",
        "                class_boxes = tf.gather( class_boxes,  selected_idc )\n",
        "                final_boxes.append( class_boxes )\n",
        "\n",
        "            # concatenate boxes for each class in the image\n",
        "            final_boxes  = tf.concat( final_boxes,  axis=0 )\n",
        "\n",
        "        else:\n",
        "            '''Perform NMS for all classes'''\n",
        "\n",
        "            # nms indices\n",
        "            selected_idc = tf.image.non_max_suppression(\n",
        "                _boxes_[...,:4], # boxes' (y1,x1,y2,x2)\n",
        "                _boxes_[...,-2], # boxes' scores\n",
        "                max_output_size = max_boxes,\n",
        "                iou_threshold = iou_threshold,\n",
        "                score_threshold = score_threshold\n",
        "            )\n",
        "\n",
        "            # boxes selected by nms\n",
        "            final_boxes = tf.gather( _boxes_,  selected_idc )\n",
        "\n",
        "        # append final boxes for each image in the batch\n",
        "        all_final_boxes.append( final_boxes )\n",
        "\n",
        "    return all_final_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXlW4lIlSqj-"
      },
      "source": [
        "### **See the function in action**\n",
        "\n",
        "#### Single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSVeIyj61jRC"
      },
      "outputs": [],
      "source": [
        "def draw_detection(\n",
        "    img,\n",
        "    boxes,\n",
        "    class_names,\n",
        "    # drawing configs\n",
        "    font=cv2.FONT_HERSHEY_DUPLEX,\n",
        "    font_scale=0.5,\n",
        "    box_thickness=2,\n",
        "    border=5,\n",
        "    text_color=(255, 255, 255),\n",
        "    text_weight=1\n",
        "):\n",
        "    '''\n",
        "    Draw the bounding boxes on the image\n",
        "    '''\n",
        "    # generate some colors for different classes\n",
        "    num_classes = len(class_names) # number of classes\n",
        "    colors = [mpl.colors.hsv_to_rgb((i/num_classes, 1, 1)) * 255 for i in range(num_classes)]\n",
        "\n",
        "    # draw the detections\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = box[:4].astype(int)\n",
        "        score = box[-2]\n",
        "        label = int(box[-1])\n",
        "\n",
        "        clr = colors[label]\n",
        "\n",
        "        # draw the bounding box\n",
        "        img = cv2.rectangle(img, (x1, y1), (x2, y2), clr, box_thickness)\n",
        "\n",
        "        # text: <object class> (<confidence score in percent>%)\n",
        "        text = f'{class_names[label]} ({score*100:.0f}%)'\n",
        "\n",
        "        # get width (tw) and height (th) of the text\n",
        "        (tw, th), _ = cv2.getTextSize(text, font, font_scale, 1)\n",
        "\n",
        "        # background rectangle for the text\n",
        "        tb_x1 = x1 - box_thickness//2\n",
        "        tb_y1 = y1 - box_thickness//2 - th - 2*border\n",
        "        tb_x2 = x1 + tw + 2*border\n",
        "        tb_y2 = y1\n",
        "\n",
        "        # draw the background rectangle\n",
        "        img = cv2.rectangle(img, (tb_x1, tb_y1), (tb_x2, tb_y2), clr, -1)\n",
        "\n",
        "        # put the text\n",
        "        img = cv2.putText(img, text, (x1 + border, y1 - border), font, font_scale, text_color, text_weight, cv2.LINE_AA)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbfuecgdSqj-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import display, Math\n",
        "from time import time\n",
        "\n",
        "'''Get the image'''\n",
        "image_data  = []\n",
        "image_shape = []\n",
        "\n",
        "act_img = cv2.imread( 'Images/image_from_china(2685).jpg')\n",
        "image_shape.append( act_img.shape[:-1] )\n",
        "image_data.append( letterbox_image(act_img, (416,416))/255. )\n",
        "\n",
        "image_data  = np.array( image_data  )\n",
        "image_shape = np.array( image_shape )\n",
        "\n",
        "'''Get the boxes'''\n",
        "t0 = time() # set a timer\n",
        "\n",
        "prediction = model.predict(image_data)\n",
        "\n",
        "boxes = detection(\n",
        "    prediction,\n",
        "    anchor_boxes,\n",
        "    num_classes,\n",
        "    image_shape,\n",
        "    input_shape = (416,416),\n",
        "    max_boxes = 10,\n",
        "    score_threshold=0.3,\n",
        "    iou_threshold=0.45)\n",
        "\n",
        "print('time taken to process : {:.2f} ms'.format( (time()-t0)*1000 ))\n",
        "\n",
        "'''Draw the boxes'''\n",
        "detected_img = draw_detection(act_img, boxes[0].numpy(), class_names)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow( detected_img[:,:,::-1] )\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNloaqsI1aEL"
      },
      "outputs": [],
      "source": [
        "boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUMcr_9mNKL1"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfaTZOPfNNKO"
      },
      "source": [
        "In this lab we built a YOLO v3 model from scratch to predict only three classes and spent a lot of time on understanding the output format of YOLO. The anchor box concept is the key to the success of YOLO series: they are fast and flexible to various sizes and locations. The output formats also determine the loss function of the model and the performance evaluation.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suBTPd4Ey8DX"
      },
      "source": [
        "# Lab: Object Detection Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3JJS1owzEDJ"
      },
      "source": [
        "Now we have predictions but do not know how good are those boxes. Performance evaluation of YOLO is essential to assess the accuracy and effectiveness of the object detection model.\n",
        "\n",
        "The evaluation process is generally standarized with metrics like Intersection over Union (IoU), Precision, and Recall that can gauge the model's detection accuracy and its ability to generalize to new data, while also gaining insights into class-specific performance and error analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives"
      ],
      "metadata": {
        "id": "JbUvudpEUnMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Evaluate the accuracy and effectiveness of the YOLO (You Only Look Once) object detection model.\n",
        "* Use standardized metrics like Intersection over Union (IoU), Precision, and Recall to gauge the model's detection accuracy\n",
        "* Understand the model ability to generalize to new data."
      ],
      "metadata": {
        "id": "ZblV-L00UqM4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwt3kUj00FSA"
      },
      "source": [
        "## Read Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saX8vkFf0IdV"
      },
      "source": [
        "We first read all the images with bounding box labels. Save all boxes with the image names as keys. In this code, the format for each box is [class,x1,y1,x2,y2]. One image could have multiple boxes or zero box. Remember to clear output after look at the format. Uncomment the break to see different examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ2DL7hfldyy"
      },
      "outputs": [],
      "source": [
        "PATH_LABELS='Labels/pictor_ppe_crowdsourced_approach-01_test.txt'\n",
        "DIR_IMAGES='Images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3jhkNqm0VKu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "groundtruths = {}\n",
        "\n",
        "with open(PATH_LABELS, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        # each line contains labels for each image\n",
        "        print(\"ANNOTATION LINE:\\n\", line, '\\n')\n",
        "        gt_data = []\n",
        "        # at each line, informaions are separated by a tab character ('\\t')\n",
        "        info = line.split(\"\\t\")\n",
        "\n",
        "        # first piece of info is the file name of the image\n",
        "        filename = info[0]\n",
        "        print(\"IMAGE FILE NAME:\", filename)\n",
        "\n",
        "        # check if the image exists in the folder\n",
        "        image_filepath = os.path.join(DIR_IMAGES, filename)\n",
        "        if not os.path.exists(image_filepath):\n",
        "            print(\"file not found:\", image_filepath)\n",
        "            break\n",
        "\n",
        "        # read the image\n",
        "        img = cv2.imread(image_filepath)\n",
        "\n",
        "        # convert BGR to RGB\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # next pieces of info are the boxes of\n",
        "        for i, box in enumerate(info[1:]):\n",
        "\n",
        "            print(f\"BOX NO {i}:\", box)\n",
        "\n",
        "            # each box contains 5 values\n",
        "            # x0, y0, x1, y1, c\n",
        "\n",
        "            # split the information and convert to integer\n",
        "            box_info_list = box.split(\",\")\n",
        "            box_info = [int(x) for x in box_info_list]\n",
        "            x0, y0, x1, y1, c = box_info\n",
        "\n",
        "            # (x0,y0) is the top-left corner point\n",
        "            # (x1,y1) is the bottom-right corner point\n",
        "            # c is the class\n",
        "\n",
        "\n",
        "\n",
        "            # choose some color to show different classes\n",
        "            if c == 0:\n",
        "                color = (255,0,0) #\n",
        "            elif c == 1:\n",
        "                color = (0,255,0) #\n",
        "            elif c == 2:\n",
        "                color = (0,0,255) #\n",
        "            gt_data.append([c,x0, y0, x1, y1])\n",
        "\n",
        "            # draw rectangle for each box\n",
        "            img = cv2.rectangle(img, (x0,y0), (x1,y1), color, 2)\n",
        "        groundtruths.update({filename:gt_data})\n",
        "        # breaking the loop to show only one sample\n",
        "        # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfNZNxa927EO"
      },
      "outputs": [],
      "source": [
        "groundtruths['image_from_china(769).jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKAR5tm93Nox"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX42XqW_0ZN-"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmGvB2Ee5X8G"
      },
      "source": [
        "## Save Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGPfV6Je5ast"
      },
      "source": [
        "It does not matter how you save the data as long as they keys match. This ensures the label boxes and prediction boxes will be compared for each image. Make prediction using the function made before and append the results using the same format. This will take some time. Switch the runtime type to GPU if you can and study the difference in processing speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HQl6i346ESy"
      },
      "outputs": [],
      "source": [
        "detections = {}\n",
        "\n",
        "skipped_images = []\n",
        "\n",
        "for img_name in list(groundtruths.keys()):\n",
        "\n",
        "    image_data  = []\n",
        "    image_shape = []\n",
        "\n",
        "    act_img = cv2.imread( 'Images/'+img_name)\n",
        "    image_shape.append( act_img.shape[:-1] )\n",
        "    image_data.append( letterbox_image(act_img, (416,416))/255. )\n",
        "\n",
        "    image_data  = np.array( image_data  )\n",
        "    image_shape = np.array( image_shape )\n",
        "\n",
        "    '''Get the boxes'''\n",
        "    t0 = time() # set a timer\n",
        "\n",
        "    prediction = model.predict(image_data)\n",
        "\n",
        "    boxes = detection(\n",
        "        prediction,\n",
        "        anchor_boxes,\n",
        "        num_classes,\n",
        "        image_shape,\n",
        "        input_shape = (416,416),\n",
        "        max_boxes = 10,\n",
        "        score_threshold=0.3,\n",
        "        iou_threshold=0.45)\n",
        "    results=[]\n",
        "    for box in boxes[0].numpy():\n",
        "        x1, y1, x2, y2 = box[:4].astype(int)\n",
        "        score = box[-2]\n",
        "        label = int(box[-1])\n",
        "        results.append([label,x1, y1, x2, y2,score])\n",
        "    detections.update({img_name : results})\n",
        "    print(\"_{0}_\".format(img_name),end=\"\")\n",
        "print(\"done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgWiPmpf7nfS"
      },
      "source": [
        "## Defined IoU function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMzufnoA8vif"
      },
      "source": [
        "For object detection tasks, the IoU is commonly used to determine whether a predicted bounding box is considered a true positive (correctly detected) or a false positive (incorrectly detected). If the IoU between the predicted bounding box and the ground truth bounding box exceeds a predefined threshold (often 0.5), the detection is considered correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6AgvYM37ws5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def iou (boxes1, boxes2):\n",
        "    '''\n",
        "    boxes1: m x 4 numpy array\n",
        "    boxes2: n x 4 numpy array\n",
        "    '''\n",
        "    boxes1 = np.array(boxes1, dtype='float32')\n",
        "    boxes2 = np.array(boxes2, dtype='float32')\n",
        "\n",
        "    m = boxes1.shape[0] # number of boxes1\n",
        "    n = boxes2.shape[0] # number of boxes2\n",
        "\n",
        "    boxes1_area = (boxes1[:,2]-boxes1[:,0])*(boxes1[:,3]-boxes1[:,1])\n",
        "    boxes1_area = boxes1_area.repeat(n).reshape((m,n)) # converts to mxn matrix\n",
        "\n",
        "    boxes2_area = (boxes2[:,2]-boxes2[:,0])*(boxes2[:,3]-boxes2[:,1])\n",
        "    boxes2_area = np.tile(boxes2_area, (1,m)).reshape((m,n)) # converts to mxn matrix\n",
        "\n",
        "    boxes1 = np.tile(boxes1, (1,n)).reshape((m,n,4))\n",
        "    boxes2 = np.tile(boxes2, (m,1)).reshape((m,n,4))\n",
        "\n",
        "    top = np.maximum(boxes1[:,:,:2],boxes2[:,:,:2])\n",
        "    bot = np.minimum(boxes1[:,:,2:],boxes2[:,:,2:])\n",
        "\n",
        "    diff = bot - top\n",
        "    diff[diff<0] = 0\n",
        "    intersection_area = diff[:,:,0] * diff[:,:,1]\n",
        "    union_area = boxes1_area + boxes2_area - intersection_area\n",
        "\n",
        "    # avoid division by zero\n",
        "    idx = np.logical_or(boxes1_area==0, boxes2_area==0)\n",
        "    union_area[idx] = 1\n",
        "\n",
        "    return intersection_area/union_area\n",
        "\n",
        "def is_TP (ious, iou_threshold=0.5):\n",
        "    '''\n",
        "    INPUT:\n",
        "        m x n numpy array.\n",
        "        - IoU between m detected boxes and n groud truth boxes\n",
        "        - m detected boxes are sorted in descending order of confidence\n",
        "    OUTPUT:\n",
        "        m x 1 boolean array\n",
        "        - indicates if corresponding detected box is true positve\n",
        "    '''\n",
        "    m, n = ious.shape\n",
        "\n",
        "    result = np.zeros(m,dtype=bool) # to store the result\n",
        "\n",
        "    for i in range(m):\n",
        "        idx = np.argmax( ious[i,:] ) # index of the max iou\n",
        "        if ious[i,idx] >= iou_threshold:\n",
        "            result[i] = True\n",
        "            ious[:,idx] = -1 # turn off the ground truth box that is already detected\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5mENq07L_QV"
      },
      "source": [
        "## Check IoU on One Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvx4qup3MFpM"
      },
      "source": [
        "Here we can select one image key to get the `bx_gt` and `bx_dt` to calculate the IoU. This example has 5 predictions and 3 of them can be match more than 0.5 witht he ground truth label boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgEMY2E7-pIy"
      },
      "outputs": [],
      "source": [
        "i='image_from_china(769).jpg'\n",
        "x=2\n",
        "bx_gt = np.array(groundtruths[i])\n",
        "c_index=bx_gt[:,0] == x\n",
        "bx_gt=bx_gt[c_index]\n",
        "\n",
        "bx_dt = np.array(detections[i])\n",
        "# if bx_dt.shape[0] == 0: continue\n",
        "bx_dt = bx_dt[ bx_dt[:,0]==x,: ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLw_abq5Ihk-"
      },
      "outputs": [],
      "source": [
        "ious = iou(bx_dt[:,1:5] , bx_gt[:,1:5] )\n",
        "tps  = is_TP(ious)\n",
        "print(ious,'\\n',tps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw8_5SwKAZ86"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('Images/'+i)\n",
        "\n",
        "# convert BGR to RGB\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for box in bx_gt:\n",
        "\n",
        "    c ,x0, y0, x1, y1 = box\n",
        "\n",
        "    img = cv2.rectangle(img, (x0,y0), (x1,y1), (0,255,0), 2)\n",
        "\n",
        "for box in bx_dt:\n",
        "\n",
        "    c ,x0, y0, x1, y1,p = box\n",
        "\n",
        "    # draw rectangle for each box\n",
        "    img = cv2.rectangle(img, (x0.astype(int),y0.astype(int)), (x1.astype(int),y1.astype(int)), (255,255,255), 2)\n",
        "\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkPPNYMjKW4K"
      },
      "source": [
        "In the image above, we can see the ground truth boxes as green and detection boxes as white. We maticulously marked all the worker on the far side but the model does not detect all of them. This will descrease the performance calculation. Closer, one worker is blocked by the rebars and these will be False Negetive (FN) case becaues the model should detect them.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the middle of the crane, there is one large white box indicate the model thinks there is a worker but there is no ground truth box to match with it. This is a False Positve (FP) case."
      ],
      "metadata": {
        "id": "9LfDJFRiT1LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision and Recall"
      ],
      "metadata": {
        "id": "KDbNC6MVQ9_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can put everything together and make serveral variables to store the counts for precision recall calculation. Essentially, we want to keep track of True Postive (TP) cases where the prediction and ground truth boxes have more than 50% IoU."
      ],
      "metadata": {
        "id": "Dl2iXienOlkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Precision = \\frac{True Positives}{True Positives + False Positives}$\n",
        "\n",
        "$Recall = \\frac{True Positives}{True Positives + False Negatives}$\n"
      ],
      "metadata": {
        "id": "eytkJCx3Sivt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on the Evaluation Metrics:\n",
        "\n",
        "* Precision: It is the ratio of true positive detections to the total number of detected bounding boxes at a given confidence threshold.\n",
        "* Recall: It is the ratio of true positive detections to the total number of ground truth bounding boxes at a given confidence threshold.\n",
        "\n",
        "* The function calculates the precision and recall for various confidence thresholds by sorting the detections based on confidence and iterating over them.\n",
        "* The iou function is used to calculate the intersection over union (IoU) between bounding boxes, and is_TP function determines whether a detection is a true positive based on IoU thresholding."
      ],
      "metadata": {
        "id": "NgjKcuEoRCdc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fItVV7yT7qI_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate (groundtruths, detections, included_class_names=class_names):\n",
        "    '''\n",
        "    groundtruths['image_name']:\n",
        "        shape = (m, 1+4)\n",
        "        [class_id, x0, y0, x1, y1]\n",
        "\n",
        "    detections['image_name']  :\n",
        "        shape=(n,1+4+1)\n",
        "        [class_id, x0, y0, x1, y1, confidence]\n",
        "    '''\n",
        "\n",
        "    auc       = {c: 0 for c in included_class_names}\n",
        "    precision = {c:[] for c in included_class_names}\n",
        "    recall    = {c:[] for c in included_class_names}\n",
        "    real_precision = {c:0 for c in included_class_names}\n",
        "    real_recall    = {c:0 for c in included_class_names}\n",
        "\n",
        "    for x,c in enumerate(included_class_names):\n",
        "        detections_tps = np.array([])\n",
        "        detections_confs = []\n",
        "        num_gt = 0\n",
        "        num_dt = 0\n",
        "        for i in groundtruths:\n",
        "\n",
        "            bx_gt = np.array(groundtruths[i])\n",
        "            bx_gt = bx_gt[ bx_gt[:,0] == x,: ]\n",
        "            num_gt += len(bx_gt)\n",
        "            bx_dt = np.array(detections[i])\n",
        "            if bx_dt.shape[0] == 0: continue\n",
        "            bx_dt = bx_dt[ bx_dt[:,0]==x,: ]\n",
        "            num_dt = num_dt + len(bx_dt)\n",
        "\n",
        "            # print(bx_dt)\n",
        "\n",
        "            if bx_gt.shape[0] != 0:\n",
        "\n",
        "                ious = iou(bx_dt[:,1:5] , bx_gt[:,1:5] )\n",
        "                tps  = is_TP(ious)\n",
        "            else:\n",
        "                tps = np.zeros(len(bx_dt))\n",
        "\n",
        "            confs = bx_dt[:,-1]\n",
        "            detections_tps = np.append(detections_tps,tps)\n",
        "            detections_confs =  np.append(detections_confs,confs)\n",
        "\n",
        "        # sort detection by confidence\n",
        "        idc = np.argsort(detections_confs)[::-1]\n",
        "        detections_tps = detections_tps[idc]\n",
        "\n",
        "        num_tp = 0\n",
        "        for i, tp in enumerate(detections_tps):\n",
        "            if tp: num_tp += 1\n",
        "            recall[c].append( num_tp/num_gt+0.000000000001 )\n",
        "            precision[c].append( num_tp/(i+1) )\n",
        "\n",
        "\n",
        "        for i in range(len(precision[c])):\n",
        "            precision[c][i] = max(precision[c][i:])\n",
        "        for i in range(1,len(precision[c])):\n",
        "            auc[c] += precision[c][i] * ( recall[c][i]-recall[c][i-1] )\n",
        "\n",
        "    # to make the graph close, nothing important here\n",
        "    for c in included_class_names:\n",
        "        recall[c].append(1.0)\n",
        "        precision[c].append(0.0)\n",
        "\n",
        "    real_auc=auc\n",
        "    m_a_p = sum(real_auc.values())/len(real_auc)\n",
        "    return m_a_p, real_auc, precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F9PVcGn8M4A"
      },
      "outputs": [],
      "source": [
        "m_a_p, real_auc, precision, recall=evaluate (groundtruths, detections, included_class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_a_p"
      ],
      "metadata": {
        "id": "j2p3wRiuX5-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the precision recall cases after sorting give the curve for each class. The area under this curve (AUC) is the average precision (AP) value and the mean AP across all classes is the mAP. We can see from this precision recall curve that class worker works better than others. Maybe the reason is the small size of the hardhat and vest."
      ],
      "metadata": {
        "id": "qmy5fmf1PIba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_auc"
      ],
      "metadata": {
        "id": "EkPnCjBUUQ-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in class_names:\n",
        "    plt.plot(recall[c], precision[c],  linestyle='-',label=f\"{c} AP: {real_auc[c]*100:.2f}%\")\n",
        "plt.legend()\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')"
      ],
      "metadata": {
        "id": "gvuYvg0eSfCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp4Xhr0AM2_3"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During this lab, we employed the annotated dataset to assess the performance of the YOLO model. The Intersection over Union (IoU) metric served as the foundation of the evaluation process. Using IoU, we identified the detection cases and computed the precision, recall, Average Precision (AP), and mean Average Precision (mAP) values to characterize the model's performance.\n",
        "\n",
        "It is worth noting that most object detection evaluation metrics follow a similar pattern. Additionally, precision and recall values are influenced by the threshold chosen, and it is common practice to opt for a very low threshold (e.g., 0.01) when reporting the overall performance of the model."
      ],
      "metadata": {
        "id": "L06PVjkzPr22"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuK_aZRBYXdw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}