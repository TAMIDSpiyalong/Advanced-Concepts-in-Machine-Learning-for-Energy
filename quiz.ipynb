{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46ed957-0cff-4cd5-8da0-87366ecb60f3",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a723fcf0-b2d0-4fa8-9acc-8b5987150f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between vector1 and vector2: 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between vectors a and b.\n",
    "    \n",
    "    Arguments:\n",
    "    a, b -- 1-D numpy arrays\n",
    "    \n",
    "    Returns:\n",
    "    cosine_sim -- Cosine similarity between a and b\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    cosine_sim = dot_product / (norm_a * norm_b)\n",
    "    return cosine_sim\n",
    "\n",
    "# Example usage:\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 5, 6])\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc17f1c-6f86-4904-a523-ce90991983db",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63b7783-bd59-4ebf-913e-178eb693a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8360188  0.11314284 0.05083836]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    s -- A numpy array of the same shape as x, containing softmax values\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))  # Subtracting the maximum value for numerical stability\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# Example usage:\n",
    "scores = np.array([3.0, 1.0, 0.2])\n",
    "print(softmax(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04d124-d743-456c-bcea-9a1a4c8ffb9d",
   "metadata": {},
   "source": [
    "# MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa93fd9-08c8-4bbd-b2e6-46fcd480d6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.12799999999999995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error between the true values and the predicted values.\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- 1-D numpy array of true values\n",
    "    y_pred -- 1-D numpy array of predicted values\n",
    "    \n",
    "    Returns:\n",
    "    mse -- Mean squared error between y_true and y_pred\n",
    "    \"\"\"\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse\n",
    "\n",
    "# Example usage:\n",
    "true_values = np.array([1, 2, 3, 4, 5])\n",
    "predicted_values = np.array([1.5, 2.5, 3.2, 4.1, 5.3])\n",
    "print(\"Mean Squared Error:\", mean_squared_error(true_values, predicted_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce42cb67-709c-4bfc-b257-a819ec8b72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.16836656419122906\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss between the true labels and the predicted probabilities.\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- 1-D numpy array of true labels (binary or categorical)\n",
    "    y_pred -- 1-D numpy array of predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "    ce_loss -- Cross-entropy loss between y_true and y_pred\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # to avoid log(0) which is undefined\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # clip probabilities to avoid log(0)\n",
    "    ce_loss = - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return ce_loss\n",
    "\n",
    "# Example usage:\n",
    "true_labels = np.array([1, 0, 1, 1, 0])\n",
    "predicted_probabilities = np.array([0.9, 0.1, 0.8, 0.95, 0.3])\n",
    "print(\"Cross-Entropy Loss:\", cross_entropy_loss(true_labels, predicted_probabilities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f473b680-c5ef-4329-b8a6-a40277370b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = np.array([[0.2, 0.3, 0.1, 0.2, 0.2],  # word 1\n",
    "                            [0.1, 0.2, 0.4, 0.1, 0.2]]) # word 2\n",
    "\n",
    "\n",
    "\n",
    "true_labels_one_hot = np.array([[0., 0., 1., 0., 0.],\n",
    "                               [0., 0., 0., 1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce36532c-35d5-4891-9742-1295f0205011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6657381381750814"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(true_labels_one_hot[0],predicted_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f1300c6-c255-4dbe-b367-8765efd5522b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6730116670092563"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(true_labels_one_hot[1],predicted_probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186ab75d-80da-4436-b249-d5953134dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention matrix:\n",
      "[[1.4 3.2]\n",
      " [3.2 7.7]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def attention_matrix(Q, K):\n",
    "    \"\"\"\n",
    "    Compute the attention matrix between query vectors (Q) and key vectors (K).\n",
    "    \n",
    "    Arguments:\n",
    "    Q -- 2-D numpy array of query vectors with shape (m, n_q)\n",
    "    K -- 2-D numpy array of key vectors with shape (m, n_k)\n",
    "    \n",
    "    Returns:\n",
    "    A -- Attention matrix with shape (n_q, n_k)\n",
    "    \"\"\"\n",
    "    A = np.dot(Q, K.T)\n",
    "    return A\n",
    "\n",
    "# Example usage:\n",
    "Q = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "K = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.4, 0.5, 0.6]])\n",
    "A = attention_matrix(Q, K)\n",
    "print(\"Attention matrix:\")\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cb5b8-789b-45c5-b064-d2f7a93b2630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
