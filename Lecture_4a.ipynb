{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TAMIDSpiyalong/Introduction-to-Machine-Learning-for-Energy/blob/main/Lecture_4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDqRHsHmYH_A"
   },
   "source": [
    "# Text Tokenziation and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgoqZrZLYPlS"
   },
   "source": [
    "Text tokenization is a crucial step in NLP that involves reformatting a piece of text into smaller units called “tokens.” These tokens serve as the building blocks for text vectorization, which converts text into numerical representations (vectors) that machine learning models can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htkT3qqsZCoB"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL1mg86kb4Ax"
   },
   "source": [
    "1. Use the bag of words method to process text and compare the difference between texts.\n",
    "2. Learn about TF-IDF (Term Frequency-Inverse Document Frequency) and use it to search texts.\n",
    "3. Use pretrained models to vectorize texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t81VT9JboTzt"
   },
   "source": [
    "## Basic Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILvS020Zzm6F"
   },
   "source": [
    "We want to build a basic bag-of-words (BOW) representation of our corpus. You can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy. We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts:<br>\n",
    "https://scikit-learn.org/stable/index.html<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "u_EAof8njfHz"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "2fwfWQDVyJpY"
   },
   "outputs": [],
   "source": [
    "# A corpus of sentences.\n",
    "corpus = [\n",
    "  \"Red Bull drops hint on F1 engine.\",\n",
    "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
    "  \"Hamilton eyes record eighth F1 title.\",\n",
    "  \"Aston Martin announces sponsor.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "IRhJPxbHwuj_"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAphZMVPBX9P"
   },
   "source": [
    "The *fit_transform* method does two things:\n",
    "1. It learns a vocabulary dictionary from the corpus.\n",
    "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "-5wi4_C7BAWv"
   },
   "outputs": [],
   "source": [
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3Bp1XNcF1FQ"
   },
   "source": [
    "We can take a look at the features and vocabulary dictionary. Notice the **CountVectorizer** took care of tokenization for us. It also removed punctuation and lower-cased everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "qlTKj8GPbQpD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['announces', 'aston', 'bull', 'drops', 'eighth', 'engine', 'exits',\n",
       "       'eyes', 'f1', 'hamilton', 'hint', 'honda', 'leaving', 'martin',\n",
       "       'on', 'partner', 'record', 'red', 'sponsor', 'title'], dtype=object)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>announces</th>\n",
       "      <th>aston</th>\n",
       "      <th>bull</th>\n",
       "      <th>drops</th>\n",
       "      <th>eighth</th>\n",
       "      <th>engine</th>\n",
       "      <th>exits</th>\n",
       "      <th>eyes</th>\n",
       "      <th>f1</th>\n",
       "      <th>hamilton</th>\n",
       "      <th>hint</th>\n",
       "      <th>honda</th>\n",
       "      <th>leaving</th>\n",
       "      <th>martin</th>\n",
       "      <th>on</th>\n",
       "      <th>partner</th>\n",
       "      <th>record</th>\n",
       "      <th>red</th>\n",
       "      <th>sponsor</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   announces  aston  bull  drops  eighth  engine  exits  eyes  f1  hamilton  \\\n",
       "0          0      0     1      1       0       1      0     0   1         0   \n",
       "1          0      0     1      0       0       0      1     0   2         0   \n",
       "2          0      0     0      0       1       0      0     1   1         1   \n",
       "3          1      1     0      0       0       0      0     0   0         0   \n",
       "\n",
       "   hint  honda  leaving  martin  on  partner  record  red  sponsor  title  \n",
       "0     1      0        0       0   1        0       0    1        0      0  \n",
       "1     0      1        1       0   0        1       0    1        0      0  \n",
       "2     0      0        0       0   0        0       1    0        0      1  \n",
       "3     0      0        0       1   0        0       0    0        1      0  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bywJ0XnGKPQ"
   },
   "source": [
    "Each scentence is represented in a row and each column corresponds to the word listed in the vocabulary. This is a very primitive way to carry information but it does its job to some level. Note the word sequence is not considered in such way of embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF0NVhdEUR1r"
   },
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leI1VuDVVP4W"
   },
   "source": [
    "Using the **spatial** package, which is a collection of spatial algorithms and data structures, we can measure how similar are these scentences to each other. To get the cosine *similarity*, we have to substract the distance from 1.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "kOQQ50IgXQfH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Red Bull drops hint on F1 engine.', 'Honda exits F1, leaving F1 partner Red Bull.', 'Hamilton eyes record eighth F1 title.', 'Aston Martin announces sponsor.']\n",
      "Doc 1 vs Doc 2: 0.47809144373375745\n",
      "Doc 1 vs Doc 3: 0.15430334996209194\n",
      "Doc 1 vs Doc 4: 0.0\n"
     ]
    }
   ],
   "source": [
    "# The cosine method expects array_like inputs, so we need to generate\n",
    "# arrays from our sparse matrix.\n",
    "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
    "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
    "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
    "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
    "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnC_i4oH2ARW"
   },
   "source": [
    "# TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrHI-1oMeKC6"
   },
   "source": [
    "The TF-IDF (Term Frequency-Inverse Document Frequency) helps in assessing the importance of a term in a document relative to a corpus of documents. The TF score for a term $t$ in a document $d$ is calculated as the product of the term frequency\n",
    "\n",
    "$TF(t,d)=f_{t,d}$\n",
    "\n",
    "and the IDF\n",
    "\n",
    "$IDF(t,D) =ln \\frac{N+1}{n_t+1}+1$\n",
    "\n",
    "where $N$ is the total number of documents and $n_t$ is the numbner of document $t$ appears in. The final TF-IDF score is expressed as:\n",
    "\n",
    "$s_t=TF(t,d)*IDF(t,D)$\n",
    "\n",
    "This formulation is consistent with the scikit-learn package. There are many variations of this score, e.g., with nomalization or $log_{10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7oXVC8vxq05"
   },
   "source": [
    "\n",
    "In the previous case, the TF-IDF score of \"f1\" in document 1 is equal to $1*(ln(5/4)+1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "68_HinJnhXTI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# A corpus of sentences.\n",
    "corpus = [\n",
    "  \"Red Bull drops hint on F1 engine.\",\n",
    "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
    "  \"Hamilton eyes record eighth F1 title.\",\n",
    "  \"Aston Martin announces sponsor.\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH4eYlDl1hPC"
   },
   "source": [
    "TF-IDF scores for each term in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "Z5HcMqAa1oJT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>announces</th>\n",
       "      <th>aston</th>\n",
       "      <th>bull</th>\n",
       "      <th>drops</th>\n",
       "      <th>eighth</th>\n",
       "      <th>engine</th>\n",
       "      <th>exits</th>\n",
       "      <th>eyes</th>\n",
       "      <th>f1</th>\n",
       "      <th>hamilton</th>\n",
       "      <th>hint</th>\n",
       "      <th>honda</th>\n",
       "      <th>leaving</th>\n",
       "      <th>martin</th>\n",
       "      <th>on</th>\n",
       "      <th>partner</th>\n",
       "      <th>record</th>\n",
       "      <th>red</th>\n",
       "      <th>sponsor</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.446287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.916291</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   announces     aston      bull     drops    eighth    engine     exits  \\\n",
       "0   0.000000  0.000000  1.510826  1.916291  0.000000  1.916291  0.000000   \n",
       "1   0.000000  0.000000  1.510826  0.000000  0.000000  0.000000  1.916291   \n",
       "2   0.000000  0.000000  0.000000  0.000000  1.916291  0.000000  0.000000   \n",
       "3   1.916291  1.916291  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       eyes        f1  hamilton      hint     honda   leaving    martin  \\\n",
       "0  0.000000  1.223144  0.000000  1.916291  0.000000  0.000000  0.000000   \n",
       "1  0.000000  2.446287  0.000000  0.000000  1.916291  1.916291  0.000000   \n",
       "2  1.916291  1.223144  1.916291  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.916291   \n",
       "\n",
       "         on   partner    record       red   sponsor     title  \n",
       "0  1.916291  0.000000  0.000000  1.510826  0.000000  0.000000  \n",
       "1  0.000000  1.916291  0.000000  1.510826  0.000000  0.000000  \n",
       "2  0.000000  0.000000  1.916291  0.000000  0.000000  1.916291  \n",
       "3  0.000000  0.000000  0.000000  0.000000  1.916291  0.000000  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "zNvZpjOn1IF7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33024164]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "1hgQ5KNI07DI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between first document and the others:\n",
      "Document 2: [[0.33024164]]\n",
      "Document 3: [[0.07370387]]\n",
      "Document 4: [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity between the first document and the others\n",
    "print(\"Cosine Similarity between first document and the others:\")\n",
    "for i, doc in enumerate(tfidf_matrix[1:]):\n",
    "    print(f\"Document {i+2}: {cosine_similarity(tfidf_matrix[0], tfidf_matrix[i+1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcTBtSx-XqZ"
   },
   "source": [
    "## A little search engine with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYkq3i7_-qhQ"
   },
   "source": [
    "The TF-IDF features can also be used to look up some articles, which is how roughly search engine works. This time around, rather than using a short toy corpus, let's use a larger dataset. scikit-learn has a **datasets** module with utilties to load datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "id": "CMwv39AfP7Ti"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYjxqxVBBINV"
   },
   "source": [
    "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.\n",
    "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "id": "T9to6gQNCGiN"
   },
   "outputs": [],
   "source": [
    "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
    "                            remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "id": "q6AgmbL0ES9I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of posts in our dataset.\n",
    "len(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "id": "qAjM4uNDEXGf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might a real wierd idea or maybe not..\n",
      "\n",
      "I have seen where people have blown up ballons then sprayed material into them\n",
      "that then drys and makes hard walls...\n",
      "\n",
      "Why not do the same thing for a space station..\n",
      "\n",
      "Fly up the docking rings and baloon materials and such, blow up the baloons,\n",
      "spin then around (I know a problem in micro gravity) let them dry/cure/harden?\n",
      "and cut a hole for the docking/attaching ring and bingo a space station..\n",
      "\n",
      "Of course the ballons would have to be foil covered or someother radiation\n",
      "protective covering/heat shield(?) and the material used to make the wals would\n",
      "have to meet the out gasing and other specs or atleast the paint/covering of\n",
      "the inner wall would have to be human safe.. Maybe a special congrete or maybe\n",
      "the same material as makes caplets but with some changes (saw where someone\n",
      "instea dof water put beer in the caplet mixture, got a mix that was just as\n",
      "strong as congret but easier to carry around and such..)\n",
      "\n",
      "Sorry for any spelling errors, I missed school today.. (grin)..\n",
      "\n",
      "Why musta  space station be so difficult?? why must we have girders? why be\n",
      "confined to earth based ideas, lets think new ideas, after all space is not\n",
      "earth, why be limited by earth based ideas??\n"
     ]
    }
   ],
   "source": [
    "# View posts.\n",
    "print(corpus.data[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAGP5pqX3Tam"
   },
   "source": [
    "There are many tokenization tools such as ByteLevelBPETokenizer and SpaCy. Spacy is helpful for named entity recognition (NER), part-of-speech (POS) tagging, dependency parsing, and more. ByteLevelBPETokenizer is based on the Byte-Pair Encoding (BPE) algorithm, which is a subword tokenization technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from tokenizers) (0.21.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (24.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\piyalong\\envs\\tamids\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword tokens: ['R', 'e', 'd', 'Ġ', 'B', 'u', 'l', 'l', 'Ġ', 'd', 'r', 'o', 'p', 's', 'Ġ', 'h', 'in', 't', 'Ġ', 'o', 'n', 'Ġ', 'F', '1', 'Ġ', 'e', 'n', 'g', 'in', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Sample text\n",
    "text = \"Red Bull drops hint on F1 engine.\"\n",
    "\n",
    "# Initialize ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Train tokenizer on the sample text\n",
    "tokenizer.train_from_iterator([text])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(text).tokens\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Subword tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "id": "pvDbb-z35x1O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Red', 'Bull', 'drops', 'hint', '                ', 'on', 'F1', 'engine', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Red', 'Bull', 'drops', 'hint', 'on', 'F1', 'engine', '.']"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load the English language model\n",
    "nlp = English()\n",
    "\n",
    "# Define a sample sentence\n",
    "sentence = \"Red Bull drops hint                 on F1 engine.\"\n",
    "\n",
    "# Process the sentence using SpaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "tokens = [token.text for token in doc if not token.is_space]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH99M6cxCpsz"
   },
   "source": [
    "### Creating TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il-0gY9LEiNv"
   },
   "source": [
    "The usage pattern is similar, but we pass a customized tokenizer to the TFIDF vectorizor to make the dataset smaller and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "id": "vtnQX-wWDhGh"
   },
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract tokens from the processed document\n",
    "    tokens = [token.text for token in doc if not token.is_space]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "id": "Shj6BS0BN6FU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.19 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the default settings of TfidfVectorizer.\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "features = vectorizer.fit_transform(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['accomplished', 'accomplishing', 'accomplishments', 'accordance',\n",
       "       'according', 'account', 'accounting', 'accounts', 'accredited',\n",
       "       'accretie', 'accretieschijven', 'accretion', 'accumulate',\n",
       "       'accuracy', 'accurate', 'accurately', 'acdis', 'ace', 'achieve',\n",
       "       'achieved', 'achievement', 'achievements', 'achieves', 'achieving',\n",
       "       'acid', 'acids', 'acknowledge', 'acknowledged', 'acm', 'acme',\n",
       "       'acording', 'acquainted', 'acquiring', 'acrilic', 'acro',\n",
       "       'acronym', 'acronyms', 'across', 'acrossed', 'acro{$key', 'acrv',\n",
       "       'acrylonitrile', 'act', 'act:-', 'acting', 'actinide', 'action',\n",
       "       'actions', 'activated', 'activation'], dtype=object)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary= vectorizer.get_feature_names_out()\n",
    "\n",
    "vocabulary[2000:2050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "CZ9w4gh9sobB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14484\n"
     ]
    }
   ],
   "source": [
    "# The number of unique tokens.\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2023], dtype=int64),)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vocabulary == 'achieving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "id": "6CxmKlPcNRLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 14484)\n"
     ]
    }
   ],
   "source": [
    "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "yJwnU8PZNdHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.12369473 0.08749726 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.19930627 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.09824926 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.02118455 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.12280447]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# What the encoding of the first document looks like in sparse format.\n",
    "print(features[:10,:10].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp7VTwYzONlt"
   },
   "source": [
    "As we mentioned, there are TF-IDF variations out there and scikit-learn, among other things, adds **smoothing** (adds a one to the numerator and denominator in the IDF component), and normalizes by default. These can be disabled if desired using the *smooth_idf* and *norm* parameters respectively. See here for more information:<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylKLM-IMOwbJ"
   },
   "source": [
    "### Querying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8oTtCg0QB71"
   },
   "source": [
    "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
    "1. *Transform* our query using the same vocabulary from our *fit* step on our corpus.\n",
    "2. Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
    "3. Sort them in descending order by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "id": "qNjEUzqlP6Oy"
   },
   "outputs": [],
   "source": [
    "# Transform the query into a TF-IDF vector.\n",
    "query = [\"lunar orbit is about 27.322 days\"]\n",
    "query_tfidf = vectorizer.transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "id": "4YwNba88DGvh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14484)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "id": "jEfdfkmpP8Tv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593,)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the cosine similarities between the query and each document.\n",
    "# We're calling flatten() here becaue cosine_similarity returns a list of lists and we just want a single list.\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
    "cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skuSFhLxXOMC"
   },
   "source": [
    "Now that we have our list of cosine similarities, we can use this utility function to return the indices of the top k documents with the highest cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "id": "H0PvqRDpUSYO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy's argsort() method returns a list of *indices* that\n",
    "# would sort an array:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
    "#\n",
    "# The sort is ascending, but we want the largest k cosine_similarites\n",
    "# at the bottom of the sort. So we negate k, and get the last k\n",
    "# entries of the indices list in reverse order. There are faster\n",
    "# ways to do this using things like argpartition but this is\n",
    "# more succinct.\n",
    "def top_k(arr, k):\n",
    "  kth_largest = (k + 1) * -1\n",
    "  return np.argsort(arr)[:kth_largest:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "id": "zFYpEldVUaAG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[249 108   0 329 312]\n"
     ]
    }
   ],
   "source": [
    "# So for our query above, these are the top five documents.\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "print(top_related_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "id": "4e86P3bQR1ZS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33214838 0.32070687 0.19990452 0.1510558  0.14020631]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at their respective cosine similarities.\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "kzdyTptURiTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actually, Hiten wasn't originally intended to go into lunar orbit at all,\n",
      "so it indeed didn't have much fuel on hand.  The lunar-orbit mission was\n",
      "an afterthought, after Hagoromo (a tiny subsatellite deployed by Hiten\n",
      "during a lunar flyby) had a transmitter failure and its proper insertion\n",
      "into lunar orbit couldn't be positively confirmed.\n",
      "\n",
      "It should be noted that the technique does have disadvantages.  It takes\n",
      "a long time, and you end up with a relatively inconvenient lunar orbit.\n",
      "If you want something useful like a low circular polar orbit, you do have\n",
      "to plan to expend a certain amount of fuel, although it is reduced from\n",
      "what you'd need for the brute-force approach.\n"
     ]
    }
   ],
   "source": [
    "# Top match.\n",
    "print(corpus.data[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "id": "zQwWXypfR8vh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Their Hiten engineering-test mission spent a while in a highly eccentric\n",
      "Earth orbit doing lunar flybys, and then was inserted into lunar orbit\n",
      "using some very tricky gravity-assist-like maneuvering.  This meant that\n",
      "it would crash on the Moon eventually, since there is no such thing as\n",
      "a stable lunar orbit (as far as anyone knows), and I believe I recall\n",
      "hearing recently that it was about to happen.\n"
     ]
    }
   ],
   "source": [
    "# Second-best match.\n",
    "print(corpus.data[top_related_indices[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "id": "w-5aqUbGSM5J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[373 333  68 344 153]\n",
      "[0.14367476 0.11035443 0.09059311 0.08404243 0.08174752]\n"
     ]
    }
   ],
   "source": [
    "# Try a different query\n",
    "query = [\"Texas A&M University football team\"]\n",
    "query_tfidf = vectorizer.transform(query)\n",
    "\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "\n",
    "print(top_related_indices)\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "id": "VHQtRQIcSbTj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I don't care who told you this it is not generally true. I see EVERY single\n",
      "line item on a contract and I have to sign it. There is no such thing as\n",
      "wrap at this university. I also asked around here. Ther is no wrap at \n",
      "Marquette, University of Wisconsin Madison, Utah State, Weber State or\n",
      "Embry Riddle U. I am not saying that it doees not happen but in every instance\n",
      "that I have been able to track down it does not. Also the president of our\n",
      "University who was Provost at University of West Virgina said that it did\n",
      "not happen there either and that this figure must be included in the overhead\n",
      "to be a legitimate charge.\n",
      "\n",
      "\n",
      "I did they never heard of it but suggest that, like our president did, that\n",
      "any percentage number like this is included in the overhead.\n",
      "\n",
      "\n",
      "No Allen you did not. You merely repeated allegations made by an Employee\n",
      "of the Overhead capital of NASA. Nothing that Reston does could not be dont\n",
      "better or cheaper at the Other NASA centers  where the work is going on.\n",
      "Kinda funny isn't it that someone who talks about a problem like this is\n",
      "at a place where everything is overhead.\n",
      "\n",
      "\n",
      "Why did the Space News artice point out that it was the congressionally\n",
      "demanded change that caused the problems? Methinks that you are being \n",
      "selective with the facts again.\n",
      "\n",
      "\n",
      "If it takes four flights a year to resupply the station and you have a cost\n",
      "of 500 million a flight then you pay 2 billion a year. You stated that your\n",
      "\"friend\" at Reston said that with the current station they could resupply it\n",
      "for a billion a year \"if the wrap were gone\". This merely points out a \n",
      "blatent contridiction in your numbers that understandably you fail to see.\n",
      "\n",
      "Dennis, University of Alabama in Huntsville.\n"
     ]
    }
   ],
   "source": [
    "print(corpus.data[top_related_indices[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4v5wQ4JaBIh"
   },
   "source": [
    "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google.\n",
    "<br>\n",
    "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an **inverted index**. Search applications such as Elasticsearch do that under the hood.\n",
    "- You'd also want to evaluate the efficacy of your search using metrics like **precision** and **recall**.\n",
    "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
    "- Beyond word presence, intent and meaning are playing a larger role.\n",
    "<br>\n",
    "\n",
    "Information Retrieval is a huge, rich topic and beyond search, it's also key in tasks such as question-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak3LXiETfGIY"
   },
   "source": [
    "## Using Pretrained, Third-Party Vectorizors\n",
    "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "58V-1VqHbQpI"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import spacy\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iz2FCCq1fsjz"
   },
   "outputs": [],
   "source": [
    "!gdown \"https://drive.google.com/uc?id=1BpfbHu4denceXiv8yfdY3EHgjKIcULku\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "9roZ_SeybQpJ"
   },
   "outputs": [],
   "source": [
    "embedding_file = 'data/GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQkCrzSEbQpJ"
   },
   "source": [
    "Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html. To save time and space, we'll limit ourselves to 200,000 word vectors for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "1jpuGamMbQpJ"
   },
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "11hDkconbQpJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "gh1WY8agbQpJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension: (300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "pizza = word_vectors['Texas']\n",
    "print(f'Vector dimension: {pizza.shape}')\n",
    "\n",
    "# The embedding for the word 'pizza'.\n",
    "print(pizza.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "yFArLjMJbQpJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108518735\n",
      "0.11185601\n",
      "0.14119941\n",
      "0.35505623\n",
      "0.36959887\n",
      "0.04774921\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.similarity('pizza', 'gorilla'))\n",
    "print(word_vectors.similarity('pizza', 'tree'))\n",
    "print(word_vectors.similarity('pizza', 'yoga'))\n",
    "print(word_vectors.similarity('pizza', 'tomato'))\n",
    "print(word_vectors.similarity('pizza', 'sauce'))\n",
    "print(word_vectors.similarity('pizza', 'Texas'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "aR2dE67bbQpK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5314661"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.n_similarity(\"dog bites man\".split(), \"canine nips human\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf6NYTCcbQpK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxnPBIwDbQpK"
   },
   "source": [
    "Visualizing word vectors is straight-forward and can offer insights into what kind of contexts the training algorithm picked up. Because these word vectors have a dimension of 300, we need to reduce them down to two dimensions to plot them on a regular graph. This can be done through **Principal Components Analysis (PCA)**. Here, we're plotting the words we considered in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "80rVbPWdbQpK"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_pca_scatterplot(model, words):\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r', s=128)\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "\n",
    "display_pca_scatterplot(word_vectors, ['swim', 'swimming', 'cat', 'dog', 'feline', 'road', 'car', 'bus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "YqfpDplgbQpK"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FIXME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# We can also use t-SNE to visualize this to see if it is still true\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mFIXME\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FIXME' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "# We can also use t-SNE to visualize this to see if it is still true\n",
    "FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CErnAts8490j"
   },
   "source": [
    "## Conclustion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypR4pnsp4_83"
   },
   "source": [
    "In this NLP lab, we delved into text tokenization and vectorization, starting with bag-of-words (BOW) representation for converting text into numerical forms using scikit-learn, then progressing to TF-IDF for assessing term importance in documents relative to a corpus. We utilized TF-IDF for text search and semantic analysis via cosine similarity metrics. Implementing a basic search engine using TF-IDF showcased practical application. Additionally, we explored pre-trained word vectors like Google News vectors, demonstrating how third-party resources can enhance NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgmgCneb4_EJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
