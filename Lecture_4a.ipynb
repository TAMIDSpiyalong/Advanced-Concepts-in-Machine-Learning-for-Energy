{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAMIDSpiyalong/Introduction-to-Machine-Learning-for-Energy/blob/main/Lecture_4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Tokenziation and Vectorization"
      ],
      "metadata": {
        "id": "JDqRHsHmYH_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text tokenization is a crucial step in NLP that involves reformatting a piece of text into smaller units called “tokens.” These tokens serve as the building blocks for text vectorization, which converts text into numerical representations (vectors) that machine learning models can work with."
      ],
      "metadata": {
        "id": "QgoqZrZLYPlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives"
      ],
      "metadata": {
        "id": "htkT3qqsZCoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Use the bag of words method to process text and compare the difference between texts.\n",
        "2. Learn about TF-IDF (Term Frequency-Inverse Document Frequency) and use it to search texts.\n",
        "3. Use pretrained models to vectorize texts."
      ],
      "metadata": {
        "id": "JL1mg86kb4Ax"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t81VT9JboTzt"
      },
      "source": [
        "## Basic Bag-of-Words (BOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILvS020Zzm6F"
      },
      "source": [
        "We want to build a basic bag-of-words (BOW) representation of our corpus. You can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy. We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts:<br>\n",
        "https://scikit-learn.org/stable/index.html<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_EAof8njfHz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fwfWQDVyJpY"
      },
      "outputs": [],
      "source": [
        "# A corpus of sentences.\n",
        "corpus = [\n",
        "  \"Red Bull drops hint on F1 engine.\",\n",
        "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
        "  \"Hamilton eyes record eighth F1 title.\",\n",
        "  \"Aston Martin announces sponsor.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRhJPxbHwuj_"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAphZMVPBX9P"
      },
      "source": [
        "The *fit_transform* method does two things:\n",
        "1. It learns a vocabulary dictionary from the corpus.\n",
        "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term).<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5wi4_C7BAWv"
      },
      "outputs": [],
      "source": [
        "bow = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Bp1XNcF1FQ"
      },
      "source": [
        "We can take a look at the features and vocabulary dictionary. Notice the **CountVectorizer** took care of tokenization for us. It also removed punctuation and lower-cased everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlTKj8GPbQpD"
      },
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bywJ0XnGKPQ"
      },
      "source": [
        "Each scentence is represented in a row and each column corresponds to the word listed in the vocabulary. This is a very primitive way to carry information but it does its job to some level. Note the word sequence is not considered in such way of embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At6Gt4bsEx2D"
      },
      "outputs": [],
      "source": [
        "for i in bow:\n",
        "    print(i.toarray()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF0NVhdEUR1r"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leI1VuDVVP4W"
      },
      "source": [
        "Using the **spatial** package, which is a collection of spatial algorithms and data structures, we can measure how similar are these scentences to each other. To get the cosine *similarity*, we have to substract the distance from 1.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOQQ50IgXQfH"
      },
      "outputs": [],
      "source": [
        "# The cosine method expects array_like inputs, so we need to generate\n",
        "# arrays from our sparse matrix.\n",
        "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
        "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
        "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
        "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
        "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnC_i4oH2ARW"
      },
      "source": [
        "# TF-IDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TF-IDF (Term Frequency-Inverse Document Frequency) helps in assessing the importance of a term in a document relative to a corpus of documents. The TF score for a term $t$ in a document $d$ is calculated as the product of the term frequency\n",
        "\n",
        "$TF(t,d)=f_{t,d}$\n",
        "\n",
        "and the IDF\n",
        "\n",
        "$IDF(t,D) =ln \\frac{N+1}{n_t+1}+1$\n",
        "\n",
        "where $N$ is the total number of documents and $n_t$ is the numbner of document $t$ appears in. The final TF-IDF score is expressed as:\n",
        "\n",
        "$s_t=TF(t,d)*IDF(t,D)$\n",
        "\n",
        "This formulation is consistent with the scikit-learn package. There are many variations of this score, e.g., with nomalization or $log_{10}$."
      ],
      "metadata": {
        "id": "qrHI-1oMeKC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the previous case, the TF-IDF score of \"f1\" in document 1 is equal to $1*(ln(5/4)+1)$."
      ],
      "metadata": {
        "id": "I7oXVC8vxq05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# A corpus of sentences.\n",
        "corpus = [\n",
        "  \"Red Bull drops hint on F1 engine.\",\n",
        "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
        "  \"Hamilton eyes record eighth F1 title.\",\n",
        "  \"Aston Martin announces sponsor.\"\n",
        "]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(norm=None)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "68_HinJnhXTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF scores for each term in each document"
      ],
      "metadata": {
        "id": "SH4eYlDl1hPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "Z5HcMqAa1oJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])"
      ],
      "metadata": {
        "id": "zNvZpjOn1IF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between the first document and the others\n",
        "print(\"Cosine Similarity between first document and the others:\")\n",
        "for i, doc in enumerate(tfidf_matrix[1:]):\n",
        "    print(f\"Document {i+2}: {cosine_similarity(tfidf_matrix[0], tfidf_matrix[i+1])}\")\n"
      ],
      "metadata": {
        "id": "1hgQ5KNI07DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcTBtSx-XqZ"
      },
      "source": [
        "## A little search engine with TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYkq3i7_-qhQ"
      },
      "source": [
        "The TF-IDF features can also be used to look up some articles, which is how roughly search engine works. This time around, rather than using a short toy corpus, let's use a larger dataset. scikit-learn has a **datasets** module with utilties to load datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMwv39AfP7Ti"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYjxqxVBBINV"
      },
      "source": [
        "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.\n",
        "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9to6gQNCGiN"
      },
      "outputs": [],
      "source": [
        "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
        "                            remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6AgmbL0ES9I"
      },
      "outputs": [],
      "source": [
        "# Number of posts in our dataset.\n",
        "len(corpus.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAjM4uNDEXGf"
      },
      "outputs": [],
      "source": [
        "# View first two posts.\n",
        "print(corpus.data[:2][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH99M6cxCpsz"
      },
      "source": [
        "## Creating TF-IDF features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy is a library designed in Python and provides easy-to-use interfaces for various NLP tasks such as tokenization, named entity recognition (NER), part-of-speech (POS) tagging, dependency parsing, and more."
      ],
      "metadata": {
        "id": "dAGP5pqX3Tam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a sample sentence\n",
        "sentence = \"Red Bull drops hint on F1 engine.\"\n",
        "\n",
        "# Process the sentence using SpaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Tokenize the sentence and print each token along with its POS tag\n",
        "print(\"Tokenization and POS tagging:\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "id": "pvDbb-z35x1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpmBI0eF6FVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il-0gY9LEiNv"
      },
      "source": [
        "The usage pattern is similar, but we pass a customized tokenizer to the TFIDF vectorizor to make the dataset smaller and more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtnQX-wWDhGh"
      },
      "outputs": [],
      "source": [
        "# If we want to use spaCy's tokenizer, we need to create a callback.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# We don't need named-entity recognition nor dependency parsing for\n",
        "# this so these components are disabled. This will speed up the\n",
        "# pipeline.\n",
        "unwanted_pipes = [\"ner\", \"parser\"]\n",
        "\n",
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters, and return the lemma (which require POS tagging).\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_ for t in nlp(doc) if \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            t.is_alpha]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shj6BS0BN6FU"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Use the default settings of TfidfVectorizer.\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "features = vectorizer.fit_transform(corpus.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ9w4gh9sobB"
      },
      "outputs": [],
      "source": [
        "# The number of unique tokens.\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CxmKlPcNRLk"
      },
      "outputs": [],
      "source": [
        "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJwnU8PZNdHU"
      },
      "outputs": [],
      "source": [
        "# What the encoding of the first document looks like in sparse format.\n",
        "print(features[:5,:5].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7VTwYzONlt"
      },
      "source": [
        "As we mentioned, there are TF-IDF variations out there and scikit-learn, among other things, adds **smoothing** (adds a one to the numerator and denominator in the IDF component), and normalizes by default. These can be disabled if desired using the *smooth_idf* and *norm* parameters respectively. See here for more information:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKLM-IMOwbJ"
      },
      "source": [
        "## Querying the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8oTtCg0QB71"
      },
      "source": [
        "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
        "1. *Transform* our query using the same vocabulary from our *fit* step on our corpus.\n",
        "2. Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
        "3. Sort them in descending order by score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNjEUzqlP6Oy"
      },
      "outputs": [],
      "source": [
        "# Transform the query into a TF-IDF vector.\n",
        "query = [\"lunar orbit\"]\n",
        "query_tfidf = vectorizer.transform(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_tfidf.shape"
      ],
      "metadata": {
        "id": "4YwNba88DGvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEfdfkmpP8Tv"
      },
      "outputs": [],
      "source": [
        "# Calculate the cosine similarities between the query and each document.\n",
        "# We're calling flatten() here becaue cosine_similarity returns a list\n",
        "# of lists and we just want a single list.\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skuSFhLxXOMC"
      },
      "source": [
        "Now that we have our list of cosine similarities, we can use this utility function to return the indices of the top k documents with the highest cosine similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0PvqRDpUSYO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy's argsort() method returns a list of *indices* that\n",
        "# would sort an array:\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "#\n",
        "# The sort is ascending, but we want the largest k cosine_similarites\n",
        "# at the bottom of the sort. So we negate k, and get the last k\n",
        "# entries of the indices list in reverse order. There are faster\n",
        "# ways to do this using things like argpartition but this is\n",
        "# more succinct.\n",
        "def top_k(arr, k):\n",
        "  kth_largest = (k + 1) * -1\n",
        "  return np.argsort(arr)[:kth_largest:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFYpEldVUaAG"
      },
      "outputs": [],
      "source": [
        "# So for our query above, these are the top five documents.\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "print(top_related_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e86P3bQR1ZS"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at their respective cosine similarities.\n",
        "print(cosine_similarities[top_related_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzdyTptURiTQ"
      },
      "outputs": [],
      "source": [
        "# Top match.\n",
        "print(corpus.data[top_related_indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQwWXypfR8vh"
      },
      "outputs": [],
      "source": [
        "# Second-best match.\n",
        "print(corpus.data[top_related_indices[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-5aqUbGSM5J"
      },
      "outputs": [],
      "source": [
        "# Try a different query\n",
        "query = [\"American company\"]\n",
        "query_tfidf = vectorizer.transform(query)\n",
        "\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "\n",
        "print(top_related_indices)\n",
        "print(cosine_similarities[top_related_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHQtRQIcSbTj"
      },
      "outputs": [],
      "source": [
        "print(corpus.data[top_related_indices[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4v5wQ4JaBIh"
      },
      "source": [
        "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google.\n",
        "<br>\n",
        "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an **inverted index**. Search applications such as Elasticsearch do that under the hood.\n",
        "- You'd also want to evaluate the efficacy of your search using metrics like **precision** and **recall**.\n",
        "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
        "- Beyond word presence, intent and meaning are playing a larger role.\n",
        "<br>\n",
        "\n",
        "Information Retrieval is a huge, rich topic and beyond search, it's also key in tasks such as question-answering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3LXiETfGIY"
      },
      "source": [
        "# Using Pretrained, Third-Party Vectors\n",
        "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58V-1VqHbQpI"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import spacy\n",
        "import tensorflow as tf\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz2FCCq1fsjz"
      },
      "outputs": [],
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1BpfbHu4denceXiv8yfdY3EHgjKIcULku\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9roZ_SeybQpJ"
      },
      "outputs": [],
      "source": [
        "embedding_file = 'GoogleNews-vectors-negative300.bin.gz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQkCrzSEbQpJ"
      },
      "source": [
        "Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "<br><br>\n",
        "To save time and space, we'll limit ourselves to 200,000 word vectors for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVeb02mqbQpJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jpuGamMbQpJ"
      },
      "outputs": [],
      "source": [
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11hDkconbQpJ"
      },
      "outputs": [],
      "source": [
        "len(word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh1WY8agbQpJ"
      },
      "outputs": [],
      "source": [
        "pizza = word_vectors['pizza']\n",
        "print(f'Vector dimension: {pizza.shape}')\n",
        "\n",
        "# The embedding for the word 'pizza'.\n",
        "print(pizza)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFArLjMJbQpJ"
      },
      "outputs": [],
      "source": [
        "print(word_vectors.similarity('pizza', 'gorilla'))\n",
        "print(word_vectors.similarity('pizza', 'tree'))\n",
        "print(word_vectors.similarity('pizza', 'yoga'))\n",
        "print(word_vectors.similarity('pizza', 'tomato'))\n",
        "print(word_vectors.similarity('pizza', 'sauce'))\n",
        "print(word_vectors.similarity('pizza', 'cheese'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR2dE67bbQpK"
      },
      "outputs": [],
      "source": [
        "word_vectors.n_similarity(\"dog bites man\".split(), \"canine nips human\".split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf6NYTCcbQpK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxnPBIwDbQpK"
      },
      "source": [
        "Visualizing word vectors is straight-forward and can offer insights into what kind of contexts the training algorithm picked up. Because these word vectors have a dimension of 300, we need to reduce them down to two dimensions to plot them on a regular graph. This can be done through **Principal Components Analysis (PCA)**. Here, we're plotting the words we considered in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80rVbPWdbQpK"
      },
      "outputs": [],
      "source": [
        "def display_pca_scatterplot(model, words):\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r', s=128)\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)\n",
        "\n",
        "display_pca_scatterplot(word_vectors, ['swim', 'swimming', 'cat', 'dog', 'feline', 'road', 'car', 'bus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqfpDplgbQpK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclustion"
      ],
      "metadata": {
        "id": "CErnAts8490j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this NLP lab, we delved into text tokenization and vectorization, starting with bag-of-words (BOW) representation for converting text into numerical forms using scikit-learn, then progressing to TF-IDF for assessing term importance in documents relative to a corpus. We utilized TF-IDF for text search and semantic analysis via cosine similarity metrics. Implementing a basic search engine using TF-IDF showcased practical application. Additionally, we explored pre-trained word vectors like Google News vectors, demonstrating how third-party resources can enhance NLP tasks."
      ],
      "metadata": {
        "id": "ypR4pnsp4_83"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YgmgCneb4_EJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}